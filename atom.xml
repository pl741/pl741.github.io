<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>木木的博客</title>
  
  <subtitle>木木的博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://pl741.github.io/"/>
  <updated>2019-10-14T11:54:00.562Z</updated>
  <id>http://pl741.github.io/</id>
  
  <author>
    <name>木木</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《Modeling Vehicle Interactions via Modified LSTM Models for Trajectory Prediction》</title>
    <link href="http://pl741.github.io/2019/10/10/%E3%80%8AModeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction%E3%80%8B/"/>
    <id>http://pl741.github.io/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/</id>
    <published>2019-10-10T07:42:13.000Z</published>
    <updated>2019-10-14T11:54:00.562Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://xs.scihub.ltd/https://ieeexplore.ieee.org/abstract/document/8672889/" target="_blank" rel="noopener">《Modeling Vehicle Interactions via Modified LSTM Models for Trajectory Prediction》</a></p><h3 id="通过改进的LSTM模型对车辆相互作用进行轨迹预测建模"><a href="#通过改进的LSTM模型对车辆相互作用进行轨迹预测建模" class="headerlink" title=" 通过改进的LSTM模型对车辆相互作用进行轨迹预测建模 "></a><center> 通过改进的LSTM模型对车辆相互作用进行轨迹预测建模 </center></h3><h3 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h3><p>长短期记忆（<code>LSTM</code>）模型是最常用的车辆轨迹预测模型之一。在本文中，我们研究了现有的<code>LSTM</code>模型在密集交通中进行长期轨迹预测的两个问题。首先，现有的<code>LSTM</code>模型无法同时描述不同车辆之间的空间相互作用以及轨迹时间序列之间的时间关系。因此，现有的模型不能准确地估计车辆交互在密集交通中的影响。其次，基本的<code>LSTM</code>模型经常会遇到梯度消失的问题，因此很难长期训练。这两个问题有时会导致车辆轨迹预测中存在较大的预测误差。 在本文中，我们提出了一种基于时空<code>LSTM</code>的轨迹预测模型（<code>STLSTM</code>），其中涵盖了两个地方的修改。我们将空间交互嵌入到<code>LSTM</code>模型中，以隐式测量相邻车辆之间的交互。 我们还在两个连续<code>LSTM</code>层的输入和输出之间的引入了<code>shortcut connection</code>，以处理梯度消失问题。在<code>I-80</code>和<code>US-101</code>数据集上评估了提出的新模型。 结果表明，我们的新模型比一种最新模型（<code>maneuver-LSTM</code>（<code>M-LSTM</code>））具有更高的轨迹预测精度。</p><h3 id="INDEX-TERMS"><a href="#INDEX-TERMS" class="headerlink" title="INDEX TERMS"></a>INDEX TERMS</h3><p>轨迹预测     车辆交互作用   <code>Shortcut Connection</code>  <code>LSTM</code>（<code>long short-term memory</code>)</p><h3 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h3><p>自主车辆始终通过各种传感器来感知静态交通设施以及周围车辆和行人的运动，以预测其轨迹以用于将来的运动计划。车辆轨迹预测模型的输入是最近几秒钟内目标车辆的历史轨迹，输出是接下来几秒钟内的预测轨迹。</p><p>根据详细的预测过程，可以将先前工作中使用的预测模型大致分为两种类型：基于<code>Maneuver</code>的模型和端到端模型。</p><h4 id="A-MANEUVER-BASED-MODELS"><a href="#A-MANEUVER-BASED-MODELS" class="headerlink" title="A. MANEUVER-BASED MODELS"></a>A. MANEUVER-BASED MODELS</h4><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/1.png" title="Figure1. 两类轨迹预测模型:(a)基于Maneuver的模型，(b)端到端模型"><p>如图<code>Fig.1(a)</code>所示，基于<code>Maneuver</code>的轨迹预测模型包含两个连续步骤：机动识别步骤和预测步骤 [1] – [4]。机动识别步骤输出中间结果，该中间结果指示预定类型的目标车辆的操纵。例如，在直行的情况下，车辆的运动可以分为两种：直行和换道。</p><p>在机动识别步骤中，常规方法使用SVM<sup> [5]，[6]</sup> 或概率图形模型，例如HMM <sup>[3]，[5]，[7]</sup>，贝叶斯网络<sup>[4]</sup>，随机森林分类器<sup>[8]</sup>。最近的一些工作还使用RNN<sup> [9]，[10]</sup>来提高识别精度。在预测步骤中，常规方法使用原型轨迹方法<sup>[2]，[11]</sup>，基于簇的模型<sup>[3]</sup>，动力学模型<sup>[12]</sup>，多项式模型<sup>[13]-[15]</sup>，高斯过程<sup>[16]，[ 17]</sup>，RRT <sup>[18]</sup>，高斯混合模型<sup>[1]，[19]</sup>等。然而，但是，由于车辆的轨迹是非线性的<sup>[9]</sup>，并且运动模型的特征很复杂，因此许多先前的方法不能用于长期预测<sup>[4]，[20]</sup>。因此，最近的一些工作开始将RNN用于基于长期机动的轨迹预测<sup>[9]</sup>。</p><p>基于<code>Maneuver</code>的模型具有两个优点。首先，车辆的运动可以分解为几个相对容易识别的简洁的动作<sup>[2]</sup>。 其次，预定义的操纵类别与驾驶员的直觉一致。 因此，基于<code>Maneuver</code>的模型通常是可以解释的。</p><p>但是，基于机动的模型有两个缺点，可能导致较大的预测误差。首先，当场景或车辆运动变得复杂时，很难自然地将车辆的运动合理地分为几类。 例如，一些明显不同的轨迹可以被分类为同一操作，但是它们不能通过一个简单的轨迹模型来建模。其次，我们必须为训练过程手动标记轨迹的操作； 然而标签任务既费时又昂贵。 同时错误的标签也可能会增加训练模型的错误。</p><h4 id="B-END-TO-END-MODELS"><a href="#B-END-TO-END-MODELS" class="headerlink" title="B. END-TO-END MODELS"></a>B. END-TO-END MODELS</h4><p>如图<code>Fig.1(b)</code>所示，端到端模型试图跳过机动识别过程并直接执行轨迹预测。一些研究证明，即使没有操纵识别步骤，<code>LSTM</code>模型也能够为车辆或行人的复杂运动建模<sup>[21] – [23]</sup>。此外，使用端到端模型可以避免因不正确的机动划分而导致的错误。</p><p>但是，端到端<code>LSTM</code>模型仍然存在两个问题。首先，基本的<code>LSTM</code>模型不能同时对车辆和轨迹序列之间的空间相互作用进行建模。其次，在训练长期轨迹预测模型时，很难训练基本的<code>LSTM</code>模型。在对长时间序列进行建模时，此类<code>LSTM</code>模型在时间维度上等效于非常深的神经网络。因此，这种<code>LSTM</code>模型在反向传播中可能会遇到梯度消失的问题<sup>[24]</sup>。</p><p>为了解决上述两个问题，我们提出了一种新的模型<code>ST-LSTM</code>，该模型对基本<code>LSTM</code>模型的结构进行了两处结构修改，以进行轨迹预测。</p><p>第一个修改是，我们采用了<code>Structural-RNN（S-RNN）</code><sup>[25]</sup>的想法，并通过新的<code>LSTM</code>模型对所有车辆轨迹和车辆之间的相互作用进行建模。特别地，我们将时间关系和空间相互作用构造为不同的时间序列，并通过<code>LSTM</code>模型分别处理它们。与在时间关系和空间交互作用之间没有明确区分的那些模型（例如<code>S-RNN</code>）相比，新模型更适合交通场景。</p><p>第二个修改是在每个<code>LSTM</code>层的输入和输出之间引入<code>shortcut connections</code>，旨在将历史轨迹的先验信息直接传递到后续层。 这种结构可以减轻反向传播中的梯度消失<sup>[26]</sup>。</p><p>我们在<code>NGSIM I-80</code>和<code>US-101</code>数据集上训练和评估<code>ST-LSTM</code>。 所获得的预测结果比一种最新模型（即[9]中的<code>M-LSTM</code>）的结果具有更高的准确性，这证明了我们所做修改的有效性。</p><p>为了更好地展示我们的发现，本文的其余部分安排如下。 第二节介绍了我们研究的问题，声明了本文的符号并列出了<code>ST-LSTM</code>的预测步骤。 第三节详细介绍了<code>ST-LSTM</code>的结构。 第四节介绍了详细的训练过程，介绍了实验结果并讨论了<code>ST-LSTM</code>的有效性。 最后，第五节总结了论文。</p><h3 id="II-PROBLEM-PRESENTATION-AND-THE-MODEL-问题描述与模型"><a href="#II-PROBLEM-PRESENTATION-AND-THE-MODEL-问题描述与模型" class="headerlink" title="II. PROBLEM PRESENTATION AND THE MODEL 问题描述与模型"></a>II. PROBLEM PRESENTATION AND THE MODEL 问题描述与模型</h3><h4 id="A-THE-FRAMEWORK-OF-ST-LSTM"><a href="#A-THE-FRAMEWORK-OF-ST-LSTM" class="headerlink" title="A. THE FRAMEWORK OF ST-LSTM"></a>A. THE FRAMEWORK OF ST-LSTM</h4><h5 id="1-RESEARCH-SCENARIO-研究场景"><a href="#1-RESEARCH-SCENARIO-研究场景" class="headerlink" title="1) RESEARCH SCENARIO  研究场景"></a>1) RESEARCH SCENARIO  研究场景</h5><p>在本文中，我们研究了密集交通中的端到端长期轨迹预测。这里的“长期”是指该模型能够预测整个非平凡运动（除了直线运动以外的运动）的轨迹，同时保持较低的预测误差。这里的“密集交通”意味着每辆车都可以影响周围车辆的行驶轨迹，但道路并未完全阻塞。在这种情况下，由于周围车辆的影响，车辆的运动变得复杂。</p><p>我们将轨迹预测的目标载体表示为$V_{S}$。我们假设，当且仅当周围的车辆$V_{i}$靠近目标车辆时，它才能影响$V_{s}$的未来运动。如果$V_{i}$和$V_{s}$之间的纵向距离（以及车道）大于$80 m$，我们将忽略$V_{i}$对$V_{s}$的影响。我们仅注意在六个方向（左前，前，右前，右后，后，左，后）的六个最接近的周围车辆，分别表示为$V_{1}〜V_{6}$。参见<code>Fig.2</code>。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/2.png" title="Figure2. ST-LSTM的整体结构和计算过程"><h5 id="2-INPUT-AND-OUTPUT"><a href="#2-INPUT-AND-OUTPUT" class="headerlink" title="2) INPUT AND OUTPUT"></a>2) INPUT AND OUTPUT</h5><p>我们将$V_{s}$和$V_{1}〜V_{6}$的历史轨迹表示为$ \boldsymbol{x}^{t}_{s}$和$\boldsymbol{x}_{i}^{t}(i=1,2, \dots, 6)$。如图<code>Fig2</code>所示，我们的端到端模型的输入为$\boldsymbol{x}^{t}=\left[\boldsymbol{x}_{s}^{t}, \boldsymbol{x}_{1}^{t}, \boldsymbol{x}_{2}^{t}, \ldots, \boldsymbol{x}_{6}^{t}\right]$。输出$\boldsymbol{y}^{t}_{s}$是$V_{s}$的预测轨迹。</p><p>我们将车辆轨迹表示为位置位移序列$[\Delta X, \Delta Y]$。$X$和$Y$分别是横向和纵向的坐标。也就是说，输入和输出是：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}{\boldsymbol{x}_{s}^{t}=\left[\Delta X_{s}^{\left(t-t_{h}+\Delta t\right)}, \Delta Y_{s}^{\left(t-t_{h}+\Delta t\right)}, \ldots, \Delta X_{s}^{t}, \Delta Y_{s}^{t}\right]} \\ {\boldsymbol{x}_{i}^{t}=\left[\Delta X_{i}^{\left(t-t_{h}+\Delta t\right)}, \Delta Y_{i}^{\left(t-t_{h}+\Delta t\right)}, \ldots, \Delta X_{i}^{t}, \Delta Y_{i}^{t}\right]}\end{array}\right.            \tag{1}</script><script type="math/tex; mode=display">\boldsymbol{y}_{s}^{t}=\left[\Delta X_{s}^{(t+\Delta t)}, \Delta Y_{s}^{(t+\Delta t)}, \cdots, \Delta X_{s}^{\left(t+t_{p}\right)}, \Delta Y_{s}^{\left(t+t_{p}\right)}\right]  \tag{2}</script><p>其中$t$是当前时间。$t_{h}$定义了历史时间范围。 $t_{p}$定义了预测时间范围。 $\Delta t$定义预测步长。 $[\Delta X_{s}^{t}; \Delta Y_{s}^{t}]$和$[\Delta X_{i}^{t}; \Delta Y_{i}^{t}]$定义从时间$(t-\Delta t)$到$t$的$V_{s}$和$V_{i}$的位置位移。 预测时间数$n_{p}=t_{p}/\Delta t$。 历史时间数$n_{h}=t_{h}/\Delta t$。</p><h5 id="3-THE-STRUCTURE-OF-ST-LSTM"><a href="#3-THE-STRUCTURE-OF-ST-LSTM" class="headerlink" title="3) THE STRUCTURE OF ST-LSTM"></a>3) THE STRUCTURE OF ST-LSTM</h5><p>我们通过<code>LSTM</code>模型<sup>1</sup>对所有车辆轨迹和所有交互进行建模，并将它们集成到高级结构中。 如图<code>Fig2</code>所示，$V_{i}(V_{s})$轨迹的<code>LSTM</code>模型表示为$C_{i}(C_{s})$。 $V_{i}$和$V_{s}$之间相互作用的<code>LSTM</code>模型表示为$I_{i}$。 为简单起见，图中仅显示<code>LSTM</code>模型的一部分。</p><h4 id="B-A-THREE-STEP-TRAJECTORY-PREDICTION-三步轨迹预测"><a href="#B-A-THREE-STEP-TRAJECTORY-PREDICTION-三步轨迹预测" class="headerlink" title="B. A THREE-STEP TRAJECTORY PREDICTION 三步轨迹预测"></a>B. A THREE-STEP TRAJECTORY PREDICTION 三步轨迹预测</h4><p>如图<code>Fig2</code>所示，我们将整个轨迹预测过程分为三个步骤：</p><p><strong>Step 1:</strong> 通过$C_{i}（C_{s}）$初步预测$V_{i}（V_{s}）$的轨迹。</p><p>$C_{i}（C_{s}）$的输入是历史轨迹$\boldsymbol{x}^{t}_{i}（\boldsymbol{x}^{t}_{s}）$。输出表示为$\boldsymbol{h}^{t}_{i}（\boldsymbol{h}^{t}_{s}）$，是$V_{i}（V_{s}）$的初步预测轨迹：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}{\boldsymbol{h}_{i}^{t}=\left[\Delta X_{i}^{(t+\Delta t)}, \Delta Y_{i}^{(t+\Delta t)}, \ldots, \Delta X_{i}^{\left(t+t_{p}\right)}, \Delta Y_{i}^{\left(t+t_{p}\right)}\right]} \\ {\boldsymbol{h}_{s}^{t}=\left[\Delta X_{s}^{(t+\Delta t)}, \Delta Y_{s}^{(t+\Delta t)}, \ldots, \Delta X_{s}^{\left(t+t_{p}\right)}, \Delta Y_{s}^{\left(t+t_{p}\right)}\right]}\end{array}\right.  \tag{3}</script><p><strong>Step 2:</strong> 通过$I_{i}$评估空间相互作用，并输出$V_{s}$的轨迹校正序列。</p><p>$I_{i}$的输入是$\boldsymbol{h}^{t}_{s}$和$\boldsymbol{h}^{t}_{i}$。输出$\boldsymbol{h}^{t}_{i,s}$是$\boldsymbol{h}^{t}_{s}$的轨迹校正序列，它考虑了$V_{i}$和$V_{s}$之间的空间相互作用对$V_{s}$运动的影响。 $\boldsymbol{h}^{t}_{i, s}$，$\boldsymbol{h}^{t}_{s}$和$\boldsymbol{h}^{t}_{i}$的长度都等于$2n_{p}$。</p><p><strong>Step 3:</strong> 校正$V_{s}$的预测轨迹并输出最终的预测轨迹。</p><p>此步骤的输入为$\boldsymbol{h}^{t}_{s}$和所有$\boldsymbol{h}^{t}_{s}$，最终输出预测序列$y^{t}_{s}$由输入的加权和计算得出：</p><script type="math/tex; mode=display">\boldsymbol{y}_{s}^{t}=\boldsymbol{h}_{s}^{t}+\sum_{i} w_{i} \boldsymbol{h}_{i,s}^{t}=\boldsymbol{h}_{s}^{t}+\left(\boldsymbol{W}^{t}\right)^{T} \boldsymbol{H}^{t}      \tag{4}</script><p>其中$\boldsymbol{W}^{t}=\left[w_{1}, w_{2}, \cdots, w_{6}\right]^{T}$，$\boldsymbol{H}^{t}=\left[\boldsymbol{h}_{1, s}^{t}, \boldsymbol{h}_{2, s}^{t}, \cdots, \boldsymbol{h}_{6,s}^{t}\right]^{T}$</p><p>权重$w_{i}$反映了$I_{i}$对$V_{s}$未来轨迹的影响程度。我们可以从安全距离的角度估计$w_{i}$，因为每辆车都会主动保持与周围车辆的安全距离，从而产生空间相互作用。</p><p>在本文中，我们通过[27]，[28]中提出的安全间隙公式来测量安全距离。令$V_{l}（V_{f}）$为$V_{i}$和$V_{s}$之中的领先（跟随）车辆。两辆车之间的安全距离为：</p><script type="math/tex; mode=display">D_{i, s}=v_{f} \rho+\frac{\left(v_{f}\right)^{2}-\left(v_{l}\right)^{2}}{2 a_{b r a k e}}+L=\frac{\bar{v} \Delta v}{a_{b r a k e}}+\left(v_{f} \rho+L\right)  \tag{5}</script><p>其中$v_{l}（v_{f}）$是$V_{l}（V_{f}）$的纵向速度。 $L$是两辆车的平均长度。 $ρ$是驾驶员的平均响应时间。 $a_{brake}$是两辆车的制动减速度。平均速度$\bar{v}=\left(v_{f}+v_{l}\right) / 2$，相对速度$\Delta v=v_{f}-v_{l}$。</p><p>该安全距离设想了一个汽车跟随的情况，先行车辆通过$a_{brake}$制动直到完全停止，随后的车辆在响应时间$ρ$内保持均速，然后通过$a_{brake}$制动直到完全停止。该公式可被视为安全距离的下限，因为当两辆车的距离小于$D_{i,s}$时，随后的车可能无法通过制动避免碰撞。</p><p>我们假设$α = 1/a_{brake}$和$β=v_{2}ρ+ L$，并进一步简化$β$为常数。我们考虑以下一种权重的先验知识：</p><script type="math/tex; mode=display">w_{i} \propto \frac{D_{i, s}}{\Delta d_{i, s}}=\frac{\alpha \bar{v} \Delta v+\beta}{\Delta d_{i, s}} \tag{6}</script><p>其中$\Delta d_{i, s}$定义了$V_{i}$和$V_{s}$之间的纵向距离。 在训练过程中需要确定常数项$α$和$β$。 在执行<strong>Step 3</strong>之前，应将所有$w_{i}$标准化。</p><p>在公式（6）中，$\bar{v}$或$\Delta v$的增加和$\Delta d_{i, s}$的减少将导致$w_{i}$的增加。 这表明$V_{i}$将对$V_{s}$产生更大的影响，这与我们的直觉是一致的。</p><p>如果$V_{i}$不存在或$\Delta d_{i, s}$太大，我们可以直接设置$w_{i}=0$，这样$V_{i}$的缺乏将不会影响预测过程。 因此，ST-LSTM可以应用于具有任意数量的周围车辆的车辆，反映了ST-LSTM的灵活性。</p><h3 id="III-MODEL-DESIGN"><a href="#III-MODEL-DESIGN" class="headerlink" title="III. MODEL DESIGN"></a>III. MODEL DESIGN</h3><h4 id="A-THE-STRUCTURE-AND-FEATURES-OF-ST-LSTM"><a href="#A-THE-STRUCTURE-AND-FEATURES-OF-ST-LSTM" class="headerlink" title="A. THE STRUCTURE AND FEATURES OF ST-LSTM"></a>A. THE STRUCTURE AND FEATURES OF ST-LSTM</h4><p>为了使<code>LSTM</code>模型能够同时对车辆之间的空间相互作用和轨迹序列之间的时间关系进行建模，Jain等人。在[25]中提出了一种高级时空模型，称为S-RNN（S-RNN）。关键思想是将空间相互作用视为可以由<code>LSTM</code>模型处理的时间序列。</p><p>如<code>Fig2</code>所示，我们采用<code>S-RNN</code>的思想，并类似地构建时空模型。但是，<code>T-LSTM</code>和<code>S-RNN</code>之间有两个主要区别：</p><p>第一个区别是$C_{i}$和$I_{i}$（$\boldsymbol{h}^{t}_{i}$和$\boldsymbol{h}^{t}_{i, s}$）的输出定义。 在<code>S-RNN</code>中，未指定$C_{i}$和$I_{i}$的输出。 相反，<code>ST-LSTM</code>清楚地区分了$\boldsymbol{h}^{t}_{i}$（时间关系）和$\boldsymbol{h}^{t}_{i, s}$（空间相互作用），以突出它们在轨迹预测中的不同作用。 这是因为时间关系通常主导着整个驾驶行为，而不是空间相互作用。</p><p>第二个差异是<strong>Step 3</strong>中的权重$\boldsymbol{W}^{t}$。在<code>S-RNN</code>中，所有$I_{i}$的输出都通过在训练过程中直接学习的加权总和进行积分。 不同地，我们根据等式（4）-（6）中所示的相互作用影响程度的先验知识来评估$w_{i}$。 引入这些先验知识也将有助于加速学习。</p><p>根据比较，<code>ST-LSTM</code>比<code>S-RNN</code>更适合表征交通场景。</p><h4 id="B-THE-SHORTCUT-CONNECTIONS-FOR-LSTM-MODELS"><a href="#B-THE-SHORTCUT-CONNECTIONS-FOR-LSTM-MODELS" class="headerlink" title="B. THE SHORTCUT CONNECTIONS FOR LSTM MODELS"></a>B. THE SHORTCUT CONNECTIONS FOR LSTM MODELS</h4><p>由于基本的<code>LSTM</code>模型很难在长期轨迹预测中进行训练，因此我们对基本·LSTM·模型的结构进行了一些修改，如图3所示。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/3.png" title="Figure3. 带有shortcut connections的修改后的LSTM模型结构。"><p>受ResNet [26]的启发，我们引入了<code>shortcut connections</code>来解决梯度消失的问题。输入序列的信息可以直接传输到后续的层。因此，修改后的<code>LSTM</code>模型可以减轻反向传播时的梯度消失。换句话说，修改后的<code>LSTM</code>模型的学习对象变成了类似残差的序列，而不是轨迹序列。我们将证明此更改可以使<code>LSTM</code>在<code>IV-D</code>部分中更容易训练。</p><p>为了构造<code>shortcut connections</code>，输入和输出序列的长度应相等（$n=2n_{h}$）。 <code>LSTM</code>层的数量（<code>Fig3</code>中的$m$）和每层中的<code>LSTM</code>单元的数量（<code>Fig3</code>中的$n$）是修改后的<code>LSTM</code>模型的两个主要结构参数。</p><h4 id="C-NETWORK-PARAMETER-SELECTION"><a href="#C-NETWORK-PARAMETER-SELECTION" class="headerlink" title="C. NETWORK PARAMETER SELECTION"></a>C. NETWORK PARAMETER SELECTION</h4><p>现在，我们专注于每个<code>LSTM</code>（$C_{i}$和$I_{i}$）的详细网络结构。 具体来说，我们需要设置历史时间范围$t_{h}$（$n=2n_{h}=2t_{h}/\Delta t$）和隐藏的<code>LSTM</code>层数$N_{hidden}$（$m=N_{hidden}$）。</p><p>$t_{h}$和$N_{hidden}$的值可以直接影响修改后的<code>LSTM</code>模型的性能。 通常，当$t_{h}$太小时，它无法提供足够的有效历史信息来进行轨迹预测； 当$t_{h}$太大时，冗余的历史信息可能会降低预测的准确性。 当$N_{hidden}$太小时，<code>LSTM</code>模型可能无法对复杂的车辆轨迹进行建模。 当$N_{hidden}太大时，网络可能太深且收敛速度较慢。</p><p>所有$C_{i}$和$I_{i}$的详细网络参数选择如附录A所示。选择结果如<code>IV-B</code>节所示。</p><h3 id="IV-TRAINING-RESULTS-AND-DISCUSSIONS"><a href="#IV-TRAINING-RESULTS-AND-DISCUSSIONS" class="headerlink" title="IV. TRAINING RESULTS AND DISCUSSIONS"></a>IV. TRAINING RESULTS AND DISCUSSIONS</h3><h4 id="A-DATASET"><a href="#A-DATASET" class="headerlink" title="A. DATASET"></a>A. DATASET</h4><p>本文的研究场景是一条多车道的直路。在这种情况下，我们可以将车辆运动分为两种：直行和换道。</p><p>我们使用<code>NGSIM I-80</code>和<code>US-101</code>轨迹数据集来训练和评估ST-LSTM。</p><p><code>I-80</code>数据集于2005年4月13日在加利福尼亚州埃默里维尔的80号州际公路的东行方向上收集，其中包含下午4:00到下午4:15，下午5:00到下午5:15以及下午5:15到下午5:30。</p><p><code>US-101</code>数据集于2005年6月15日在加利福尼亚州洛杉矶的<code>US Highway 101</code>的南行方向上收集，其中包含从7:50 AM至8:05 AM，8:05 AM至8:20 AM，8:20 AM至8:35 AM。这两个数据集的实验场景都是交通密集的直行道路，其中包括大量的车道变换和跟随汽车的运动。因此，数据集非常适合我们在本文中研究的场景。</p><p>在<code>I-80</code>数据集中，在三个时间段内分别收集了2052、1836、1790个轨迹。这三个数据集分别包含1025、913、945个成功的车道变更。在US-101数据集中，在三个时间段内收集了2169、2017、1915个轨迹，分别包含1006、660、657个成功的车道变换。每个数据帧都包括车辆的位置，速度，偏航角，大小等。数据集的采样频率为10 Hz，因此我们在本文中设置$\Delta t=0.1s$。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/4.png" title="Table 1. I-80中不同类型轨迹的统计信息。"><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/5.png" title="Table 2. US-101中不同类型轨迹的统计数据。"><p>但是，如表1和表2所示，两个数据集严重不平衡。 因此，我们仅对<strong>left only</strong>和<strong>no lane change</strong>进行降采样，而对<strong>right only</strong>进行过采样，以确保不同类型轨迹的比例大致相等[29]。</p><p>为了便于学习车道变更的特性，我们使用车道变更来修剪每个轨迹，并且仅保留包含车道变更的部分数据。 我们还在不改变车道的情况下随机修整了轨迹，以保证不同类型轨迹的平衡。</p><p>我们从平衡数据集中随机抽取N条轨迹（表1和表2中所示）进行训练，剩下的轨迹进行测试。 我们将在附录B中讨论训练集大小<code>N</code>的选择。选择结果在<code>IV-B</code>节中显示。</p><h4 id="B-TRAINING-DETAILS"><a href="#B-TRAINING-DETAILS" class="headerlink" title="B. TRAINING DETAILS"></a>B. TRAINING DETAILS</h4><p>根据附录A中的参数选择，我们为所有Ci和Ii设置Nhidden D 3和th D 3。也就是说，所有的Ci和Ii都有三个隐藏层，每个层都有60个LSTM单元。根据附录B中训练集规模的选择，我们分别采用临界规模（具有580条轨迹）和较大规模（具有1350条轨迹）来训练ST-LSTM模型。</p><p>根据附录A中的参数选择，我们为所有$C_{i}$和$I_{i}$设置$N_{hidden} =3$和$t_{h} =3s$。也就是说，所有的$C_{i}$和$I_{i}$都有三个隐藏层，每个层都有60个<code>LSTM</code>单元。根据附录B中训练集规模的选择，我们分别采用临界规模（具有580条轨迹）和较大规模（具有1350条轨迹）来训练ST-LSTM模型。</p><p>由于大多数车辆具有相似的轨迹特性，因此<code>ST-LSTM</code>中的所有$C_{i}$都共享一个公共<code>LSTM</code>模型。相反，由于周围车辆在不同方向上对$V_{s}$的影响是不同的，因此每个$I_{i}$都是独立训练的，彼此之间不共享网络参数。</p><p>使用<code>Tensorflow</code>对<code>ST-LSTM</code>进行了逐步训练。我们首先对$C_{i}$进行预训练，然后根据$C_{i}$对$I_{i}$进行预训练。所有的预训练LSTM模型都以$0.001$的初始学习率进行了约$20k$次迭代的训练。最后，将所有预训练的LSTM模型进行组合并训练约$5k$次迭代，以训练等式（6）的常数项$α$和$β$。</p><p>在训练过程中，每个训练步骤的批次大小$N_{b}$设置为$50×50$，即每个批次含有有50条轨迹，并且每个轨迹序列的长度为$50$（即$5 s$）。 所有网络参数都是随机初始化的。 与其他参数不同，$α$和$β$具有特定的物理含义，但是我们测试了$α$和$β$的初始化值对整体训练结果的影响很小。 我们在训练过程中使用50％的随机失活。 优化器采用<code>ADAM</code>优化器。 训练过程的损失函数在预测位移序列$\boldsymbol{y}_{s}^{t}$和地面真实位移序列$\boldsymbol{y}^{t}$之间采用$MSE$（均方误差）。</p><script type="math/tex; mode=display">\boldsymbol{y}^{t}=\left[\Delta \hat{X}^{(t+\Delta t)}, \Delta \hat{Y}^{(t+\Delta t)}, \cdots, \Delta \hat{X}^{\left(t+t_{p}\right)}, \Delta \hat{Y}^{\left(t+t_{p}\right)}\right]  \tag{7}</script><p>每条轨迹的$MSE$损失函数为：</p><script type="math/tex; mode=display">\operatorname{loss}=\sum_{i=1}^{n_{p}}\left(\delta_{i}^{t}\right)^{T} \delta_{i}^{t}  \tag{8}</script><p>其中$\delta_{i}^{t}=\left[a\left(\Delta X^{(t+i \Delta t)}-\Delta \hat{X}^{(t+i \Delta t)}), \Delta Y^{(t+i \Delta t)}-\right. \Delta \hat{Y}^{(t+i \Delta t)}]\right]^{T} $。我们在本文中设置常数$a =10$来放大横向误差对损失函数的影响。</p><p>总体MSE损失函数为：</p><script type="math/tex; mode=display">\text {Loss}=\frac{1}{N_{b}} \sum \text {loss}  \tag{9}</script><p>其中$N_{b}$是训练数据集中的轨迹总数。</p><h4 id="C-TESTING-RESULTS"><a href="#C-TESTING-RESULTS" class="headerlink" title="C. TESTING RESULTS"></a>C. TESTING RESULTS</h4><p>我们比较以下模型之间的$RMS$预测误差：<code>Mabeuver-LSTM</code>（<code>M-LSTM</code>，在[9]中提出），由$580$条轨迹训练的<code>ST-LSTM</code>（表示为<code>ST-LSTM-580</code>）和由$1350$条轨迹训练的<code>ST-LSTM</code>（表示为<code>ST-LSTM-1350</code>）。根据我们的统计，车道变更过程的平均时间为$4$到$5s$，因此我们将预测范围设置为$t_{p} =6s$。由于$t_{p}&gt; t_{h}$，我们应该重复几次预测过程以执行更大范围的轨迹预测。</p><p>在此，根据预测轨迹和地面真实轨迹计算出$RMS$预测误差。因此，我们需要累加位移序列$\boldsymbol{y}^{t}_{s}$和$\boldsymbol{y}^{t}$以获得轨迹表示$z^{t}_{s}$和$z^{t}$：</p><script type="math/tex; mode=display">z_{s}^{t}=\left[X^{(t+\Delta t)}, Y^{(t+\Delta t)}, \cdots, X^{\left(t+t_{p}\right)}, Y^{\left(t+t_{p}\right)}\right]\tag{10}</script><script type="math/tex; mode=display">z^{t}=\left[\hat{X}^{(t+\Delta t)}, \hat{Y}^{(t+\Delta t)}, \cdots, \hat{X}^{\left(t+t_{p}\right)}, \hat{Y}^{\left(t+t_{p}\right)}\right]\tag{11}</script><script type="math/tex; mode=display">\left\{\begin{array}{l}{X^{(t+i \Delta t)}=\sum_{j=1}^{i} \Delta X^{(t+j \Delta t)}} \\ {Y^{(t+i \Delta t)}=\sum_{j=1}^{i} \Delta^{(t+j \Delta t)}} \\ {\hat{X}^{(t+i \Delta t)}=\sum_{j=1}^{i} \Delta \hat{X}^{(t+j \Delta t)}} \\ {\hat{Y}^{(t+i \Delta t)}=\sum_{j=1}^{i} \Delta \hat{Y}^{(t+j \Delta t)}}\end{array}\right.  \tag{12}</script><p>因此，在给定预测范围$P$的情况下，每个单个轨迹的$RMS$预测误差为：</p><script type="math/tex; mode=display">r m s^{P}=\left(\frac{1}{P} \sum_{i=1}^{P}\left(\sigma_{i}^{t}\right)^{T} \sigma_{i}^{t}\right)^{\frac{1}{2}}\tag{13}</script><script type="math/tex; mode=display">R M S^{P} =\frac{1}{N_{t}} \sum r m s^{P} \tag{14}</script><p>其中$\sigma_{i}^{t}=\left[X^{(t+i \Delta t)}-\hat{X}^{(t+i \Delta t)}, Y^{(t+i \Delta t)}-\hat{Y}^{(t+i \Delta t)}\right]^{T}$。$N_{t}$是测试数据集中的轨迹总数。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/6.png" title="Table 3.预测误差的RMS值。"><p>$RMS$值显示在表3中，该值说明<code>ST-LSTM-580</code>和<code>ST-LSTM-1350</code>均胜过<code>M-LSTM</code>。从<code>ST-LSTM-580</code>和<code>ST-LSTM-1350</code>之间的比较中，我们可以发现后者模型使用了较大的训练集，但预测准确率仅得到了一点改进。这种现象证明了“临界点”的有效性，它可以指导我们适当地减少训练量，从而损失一点预测精度，以换取训练速度的显着提高。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/7.png" title="Fig. 4 速度偏差的平均值和标准偏差的统计数据（I-80数据集上的ST-LSTM-1350）。"><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/8.png" title="Fig. 5 位置偏差的平均绝对偏差（I-80数据集上的ST-LSTM-1350）。"><p>为了分析轨迹预测误差的主要来源，我们计算了不同预测范围内速度偏差（表示为$VD_{i}$）的平均值和标准偏差，如Fig. 4所示。在Fig. 5中绘制了不同预测范围的位置偏差（表示为$PD_{i}$）的平均绝对偏差。$VD_{i}$和$PD_{i}$的定义为：</p><script type="math/tex; mode=display">V D_{i}=\frac{1}{\Delta t}\left[\Delta X^{(t+i \Delta t)}-\Delta \hat{X}^{(t+i \Delta t)}, \Delta Y^{(t+i \Delta t)}-\Delta \hat{Y}^{(t+i \Delta t)}\right]^{T}\tag{15}</script><script type="math/tex; mode=display">P D_{i}=\sigma_{t}^{i}\tag{16}</script><p>$Fig. 4$显示了不同预测水平下$VD_{i}$的分布大致相同。速度误差的累积极大的影响了总的长期预测误差。这就是我们使用位置误差的$RMS$评估<code>ST-LSTM</code>的原因。</p><p>当预测范围变得太大时，平均绝对偏差会增加，并且由于误差累积，预测会变得非常不稳定。从另一个角度看，历史轨迹不足以为大时间间隔的预测提供有效的信息。因此，本文选择的预测范围（$6s$）已经足够大。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/9.png" title="Fig. 6 一个典型情况下的几个快照的预测轨迹和地面真实轨迹的示意图：(a) T=0s, (b) T=0.5s, (c) T=1s, (d) T=1.5 s, (e) T=2s, (f) T=4s。"><p>为了更直观地显示轨迹预测结果，我们随机选择车道变化轨迹，并在整个车道变化过程中观察预测结果。如图$6$所示，我们在此时间间隔中拍摄了六个快照。图$6(a)(b)$说明了预测的轨迹在车道变更过程的最开始还没有呈现出车道变更的特征。图$6(c)-(e)$中的预测轨迹显示了车道变化的典型特征。图$6(f)$说明了预测的轨迹在车道变化结束时变回直线。</p><h4 id="D-THE-ROLE-OF-SHORTCUT-CONNECTIONS"><a href="#D-THE-ROLE-OF-SHORTCUT-CONNECTIONS" class="headerlink" title="D. THE ROLE OF SHORTCUT CONNECTIONS"></a>D. THE ROLE OF SHORTCUT CONNECTIONS</h4><p>在训练过程中，我们发现修改后的<code>LSTM</code>模型更容易训练，并且可以将$RMS$误差降低到较低的值。</p><p>在一个实验中，我们通过基本的LSTM模型训练了另一个$C_{s}$，并将其与$IV-C$节中的$C_{s}$进行了比较。 除了<code>shortcut connections</code>外，这两个模型的其他部分都相同，例如输入和输出，$N_{hidden}$，$t_{h}$，训练过程等。在训练过程中没有人工干预。 损失函数的值绘制在$Fig. 7$中，表明修改后的LSTM模型收敛速度快于基本LSTM模型。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/10.png" title="Fig. 7 训练过程中两个模型的损失变化。"><h3 id="V-CONCLUSION"><a href="#V-CONCLUSION" class="headerlink" title="V. CONCLUSION"></a>V. CONCLUSION</h3><p>在本文中，我们提出了一种用于密集交通的新颖的端到端长期轨迹预测模型。 为了解决密集交通中长期预测的两个问题，我们通过添加<code>shortcut connections</code>来修改经典<code>LSTM</code>模型，并通过修改后的<code>LSTM</code>模型对所有空间相互作用和时间关系进行建模。 最后，我们构建了一个时空轨迹预测模型。 在<code>NGSIM I-80​</code>和<code>US-101</code>上进行的实验证明，与最新模型（<code>M-LSTM</code>）相比，<code>ST-LSTM</code>可以获得更精确的轨迹预测。</p><p>但是，我们仅在<code>NGSIM I-80</code>和<code>US-101</code>上测试<code>ST-LSTM</code>，它们在单个简单场景中是相对较小的数据集。 在将来的工作中，我们将收集更多数据并在更复杂的情况下评估<code>ST-LSTM</code>。</p><h3 id="APPENDIX-A"><a href="#APPENDIX-A" class="headerlink" title="APPENDIX A"></a>APPENDIX A</h3><h5 id="NETWORK-PARAMETER-SELECTION"><a href="#NETWORK-PARAMETER-SELECTION" class="headerlink" title="NETWORK PARAMETER SELECTION"></a>NETWORK PARAMETER SELECTION</h5><p>我们通过$N_{\text {hidden}} \in\{2,3,4\}$和$t_{h} \in\{2s,3s,4s,5s\}$来训练$C_{i}$。在本实验中，我们在<code>I-80</code>数据集上随机选择一个轨迹数量$N =1200$的训练集。详细的训练过程与第<code>IV-B</code>节中所描述的相同。我们对训练集采用10倍交叉验证，以计算不同预测范围内的均方根误差（$RMS$)。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/11.png" title="Fig. 8 比较具有不同$N_{hidden}$和$t_{h}$的修改LSTM模型之间的RMS误差: (a) $N_{hidden} = 2$，(b) $N_{hidden} =3$, (c) $N_{hidden}= 4$。"><p>如图8所示，我们比较了这些模型在不同预测范围内的RMS误差。 我们选择最佳组合（$t_{h} =3s$和$N_{hidden} =3$）作为$C_{i}$的参数，因为具有该组参数的模型的RMS误差几乎总是最低的（$t_{p} =2s$时第二低）。 我们还测试了更深的网络和更大$t_{h}$的网络，但是这些模型的性能并不比所选模型更好。</p><p>由于模型结构的限制，$I_{i}$的$t_{h}$应该等于$C_{i}$的$t_{h}$。 我们重复前面的步骤，发现$N_{hidden} =3$对于$I_{i}$也是一个很好的值。 </p><p>同样，我们尝试使用不同的训练集大小和不同的数据集（即<code>US-101</code>），发现上述$t_{h}$和$N_{hidden}$值也相对适合这些训练集。 因此，我们在<code>ST-LSTM</code>中为所有$C_{i}$和$I_{i}=3$和$N_{hidden} =3$。</p><h3 id="APPENDIX-B"><a href="#APPENDIX-B" class="headerlink" title="APPENDIX B"></a>APPENDIX B</h3><h5 id="OVERFITTING-ANALYSIS-AND-TRAINING-SET-SCALE-SELECTION"><a href="#OVERFITTING-ANALYSIS-AND-TRAINING-SET-SCALE-SELECTION" class="headerlink" title="OVERFITTING ANALYSIS AND TRAINING SET SCALE SELECTION"></a>OVERFITTING ANALYSIS AND TRAINING SET SCALE SELECTION</h5><p>深度学习模型（包括ST-LSTM）需要大量的数据进行训练。训练数据不足会导致拟合过度（泛化能力低），但是过多的数据也无法无限地提高模型泛化能力。特别是，通常很难获得用于深度学习的大规模数据集，因此我们需要找到数据量的“临界值”，以在提高模型泛化能力和减少训练数据需求之间达成折衷。该临界点也可以视为有效训练集的最小大小。</p><p>在本文中，我们分别使用不同大小的训练集来训练<code>ST-LSTM</code>模型，并计算它们的训练和测试误差（具有不同的预测范围），以观察模型是否过度拟合。</p><p>我们以<code>US-101</code>数据集为例。根据<code>IV-A</code>节，平衡数据集总共包含$1919$条有效轨迹。我们分别随机采样$40、95、190、380、580、960、1350、1540$轨迹（约占总数据集的$2％，5％，10％，20％，30％，50％，70％，80％$）进行训练，并在图9中绘制训练和测试误差。网络参数遵循附录A的结果。详细的训练过程遵循<code>IV-B</code>节）。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/12.png" title="Fig. 9 .通过不同规模的训练集训练的ST-LSTM模型之间的训练和测试误差（不同预测范围）的比较。"><p>$Fig.9$说明了当训练集太小时，训练得到的<code>ST-LSTM</code>会产生严重的过拟合。随着轨迹数量的增加，过度拟合现象会减弱，而当训练集包含$580$多个轨迹时，这种过度拟合现象几乎可以消除。之后，尽管训练集数量大大增加，但是模型泛化能力仅略有改善。因此，我们认为$580$条轨迹是适合于我们的<code>ST-LSTM</code>模型的训练量。在其他数据集（例如<code>I-80</code>）上训练<code>ST-LSTM</code>模型时，我们构建了具有$580$条随机轨迹的训练集，并且还可以获得训练较好的模型。</p><p>上面的方法探讨了模型的“边际”性能，即对于特定模型，“临界点”代表我们至少应使用多少训练数据进行训练。我们认为，我们上面提出的方法值得推广。基于“临界点”，我们可以直接比较不同深度网络模型对训练数据的需求。当我们的训练数据规模有限时，我们可以基于该指标选择在此条件下具有更强泛化能力（并且不能过度拟合）的模型，并获得更好的性能。确定模型后，还可以根据该指标设置合理的训练集规模，避免训练数据冗余造成的计算资源浪费。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://xs.scihub.ltd/https://ieeexplore.ieee.org/abstract/document/8672889/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Modeling Vehicle I
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://pl741.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="LSTM" scheme="http://pl741.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>《Long Short Term Memory Networks for Anomaly Detection in Time Series》</title>
    <link href="http://pl741.github.io/2019/09/19/%E3%80%8ALong-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series%E3%80%8B/"/>
    <id>http://pl741.github.io/2019/09/19/《Long-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series》/</id>
    <published>2019-09-19T01:24:08.000Z</published>
    <updated>2019-09-19T11:52:34.929Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf" target="_blank" rel="noopener">Long Short Term Memory Networks for Anomaly Dection in Time Series</a></p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>&emsp;&emsp;长短期记忆（LSTM）网络已被证明对于学习包含未知长度的长期模式的序列特别有用，因为它们能够维持长期记忆。在这样的网络中堆叠循环隐藏层还能够学习更高级别的时间特征，以便通过更稀疏的表示来更快地学习。在本文中，我们使用堆叠式LSTM（Stacked LSTM）网络进行时间序列中的异常检测。网络在非异常数据上进行训练，并用作多个时间步长的预测器。预测误差被建模为多变量高斯分布，用于评估异常行为的可能性。此方法的有效性在四个数据集上得到证实：ECG（心电图），航天飞机，电力需求和多传感器引擎数据集。</p><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><p>&emsp;&emsp;传统的过程监控技术在时间窗<sup>[1]</sup>上使用统计测量值，如累积和（CUSUM）和指数加权移动平均值（EWMA），以检测基础分布的变化。该时间窗口的长度通常需要预先确定，同时结果很大程度上取决于该参数。LSTM神经网络<sup>[2]</sup>通过采用乘法门来克服递归神经网络（RNN）所遇到的梯度消失问题。<u>该乘法门能保证常量错误一定从“内存单元”的内部状态通过</u>。输入$(IG)$，输出$(OG)$和遗忘$(FG)$门防止记忆内容被不相关的输入和输出扰动（参见$Fig.1(a)$），从而允许长期记忆存储。由于能够在序列中学习长期相关性，LSTM网络避免了对预先指定时间窗口的需要，并且能够精确地建模复杂的多变量序列。在本文中，我们通过堆叠式LSTM网络对时间序列的正常行为进行建模，我们可以准确地检测出与正常行为的偏差，而无需任何预先指定的上下文窗口或预处理。</p><img src="/2019/09/19/《Long-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series》/1.png" title="$Fig.1:(a)$长短期记忆单元$(b)$堆叠结构"><p>&emsp;&emsp;已经表明，在网络中$Sigmoid$激活单元的堆叠循环隐藏层能更轻易地捕获时间序列的结构，并允许在不同的时间尺度处理时间序列<sup>[3]</sup>。使用分层时间处理技术进行异常检测的一个值得注意的例子是分层时间记忆（HTM）系统，它试图模仿新皮层中细胞，区域和水平的层次结构<sup>[4]</sup>。此外，像[5,6]这样的时间异常检测方法学习预测时间序列并使用预测误差来检测新颖性。然而，据我们所知，LSTM提供的记忆性功能尚未与循环分层处理层相结合，以用于预测时间序列并进行异常检测。</p><p>&emsp;&emsp;如[5]中所述，我们使用预测器建模正常行为，然后使用预测误差来识别异常行为。（这在现实世界的异常检测场景中特别有用，在这种场景中，正常行为的实例可能很多，但异常行为的实例很少见。）为了确保网络捕获序列的时间结构，我们预测未来的几个时间步骤。因此，序列中的每个点具有在过去的不同点处产生的多个对应的预测值，从而产生多个误差值。然后使用预测正常数据时的错误概率分布来获得在测试数据上正常行为的可能性。当控制变量（例如车辆加速器或制动器）也存在时，除了因变量之外，网络还用来预测控制变量。这迫使网络通过控制和相关传感器变量预测误差的联合分布来学习正常的使用模式：结果，当控制输入改变时已经捕获了明显的预测误差，并且不会有助于声明异常。</p><p>&emsp;&emsp;本文的其余部分安排如下：第2节描述了我们的方法。在第3节中，我们使用堆叠式LSTM方法（LSTM-AD）以及循环$Sigmoid$单元的堆叠RNN方法（RNN-AD），在四个真实世界数据集上呈现时间异常检测结果。第4节提供结论性意见。</p><h3 id="2-LSTM-AD：基于LSTM的异常检测"><a href="#2-LSTM-AD：基于LSTM的异常检测" class="headerlink" title="2. LSTM-AD：基于LSTM的异常检测"></a>2. LSTM-AD：基于LSTM的异常检测</h3><p>&emsp;&emsp;考虑一个时间序列$X=\left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(n)}\right\}$，时间序列中的每个点$\mathbf{x}^{(t)} \in R^{m}$是一个$m$维的向量$\left\{x_{1}^{(t)}, x_{2}^{(t)}, \ldots, x_{m}^{(t)}\right\}$，，这些元素对应于输入变量。预测模型学习去预测输入变量$d$ $(s.t. 1 \leq d \leq m)$的接下来$l$个值。正常序列$(s)$被分为四组：正常训练集$(s_{N})$，正常验证集-1$(v_{N1})$，正常验证集-2$(v_{N2})$和正常测试集$(t_{N})$。异常序列$(s)$被分为两组：异常验证集$(v_{A})$和异常测试集$(t_{A})$。我们首先使用堆叠式LSTM网络学习一个预测模型，然后计算我们检测到异常的预测误差分布：</p><p>&emsp;&emsp;<strong>基于堆叠式LSTM的预测模型：</strong>我们考虑以下LSTM网络架构：对于$m$个维度中的每一个维度，我们都会对输入层放置一个单元，输出层有$d×l$个单元，使其满足对每一个维度的$l$个预测输出都有一个单元对应。隐藏层中的LSTM单元通过循环连接完全连接。我们基于如下方式堆叠LSTM层：较低层的LSTM隐藏层中的每个单元通过前馈连接完全连接到其上方的较高层的LSTM隐藏层中的每个单元（参见$Fig. 1(b)$）。使用$s_{N}$中的序列来学习预测模型。集合$v_{N1}$用于在学习网络权重时的提前停止。</p><p>&emsp;&emsp;<strong>基于预测误差分布的异常检测：</strong>在预测长度为$l$的情况下，对于$l&lt;t \leq n-1$，$\mathbf{x}^{(t)} \in X$中每个$d$维被预测$l$次。我们计算点$\mathbf{x}^{(t)}$的误差向量$ \mathbf e^{(t)}$,$\mathbf{e}^{(t)}=\left[e_{11}^{(t)}, \ldots, e_{1 l}^{(t)}, \ldots, e_{d 1}^{(t)}, \ldots, e_{d l}^{(t)}\right]$，其中$e_{ij}^{(t)}$表示$x_{i}^{(t)}$和其在$t-j$时间上的预测值之间的差。</p><p>&emsp;&emsp;在$s_{N}$上训练的预测模型用于计算验证和测试序列中每个点的误差向量。对误差向量进行建模以拟合多元高斯分布$\mathcal{N}=\mathcal{N}(\mu, \mathbf{\Sigma})$。观察误差向量$\mathbf e^{(t)}$的似然性$p^{(t)}$由$\mathcal{N}$在$\mathbf e^{(t)}$处的值给出（类似于使用基于卡尔曼滤波器的动态预测模型<sup>[5]</sup>的新颖性检测所使用的基于归一化新息平方$(NIS)$）。来自$v_{N1}$的点的误差向量用于使用最大似然估计来估计参数$μ$和$Σ$。如果$p^{(t)}&lt;τ$，则观察值$\mathbf x^{(t)}$被分类为“异常”，否则观察值被分类为“正常”。集合$v_{N2}$和$v_{A}$用于通过最大化$F_{β}-socre$来学习$τ$（其中异常点属于正类，而正常点属于负类）。</p><h3 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h3><p>&emsp;&emsp;我们在四个真实世界数据集上呈现LSTM-AD的结果，这些数据集在检测它们中的异常时具有不同的难度级别。在使用第2节中描述的验证集选择最大化$F_{0.1}-score$的网络架构以及和$τ$后，我们在表1中展示了采用LSTM-AD和RNN-AD两种方法的精度，召回率，$F_{0.1}-score$和架构。</p><h4 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h4><p>&emsp;&emsp;<em>心电图（ECGs)</em>：qtdb / sel102心电图数据集包含对应于心室前收缩的单个短期异常$(Fig.2 (a))$。由于ECG数据集只有一个异常，我们不计算该数据集的阈值和相应的$F_{0.1}-score$；我们只使用正常的ECG子序列学习预测模型，并计算剩余序列的误差向量的可能性。</p><p>&emsp;&emsp;<em>航天飞机Marotta阀门时间序列</em>：该数据集具有短时间段模式和长时间段模式，持续100个时间步长。数据集中存在三个异常区域在$Fig.2 (b)$中被标记为$a_{1}, a_{2}, a_{3}$。区域$a_{3}$是更容易辨别的异常，而区域$a_{1}$和$a_{2}$对应于在该方法下不易辨别的更微妙的异常。</p><p>&emsp;&emsp;<em>电力需求数据集</em>：电力消耗的正常行为是在每周中有五个峰值对应于五个工作日和两个低谷对应于周末。该数据集属于长期模式具有跨越数百个的时间步长。此外，此数据集有噪声，因为峰值不会在每条的同一时间出现。</p><p>&emsp;&emsp;<em>多传感器引擎数据</em>：该数据集具有来自12个不同的传感器：其中一个传感器是发动机的“控制”部件，其余的测量依赖如温度，扭矩等变量。我们使用对应于三个独立故障的序列来训练异常检测模型，并在三个不同的独立故障集上测量$F_{β}-score$。我们选择“控制”传感器和其他一个因变量作为要预测的值。</p><img src="/2019/09/19/《Long-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series》/2.png" title="Fig.2: 样本序列（正常：绿色， 异常：蓝色）和相关似然$p$（红色）具有相同$S_{i}(i = 1,2,3)$的图具有相同的y轴"><h4 id="3-2-结果"><a href="#3-2-结果" class="headerlink" title="3.2 结果"></a>3.2 结果</h4><p>&emsp;&emsp;我们的实验结果的主要观察结果如下：</p><p>&emsp;&emsp;（i）在$Fig.2$ 中，对于所有数据集，异常区域中的似然值$p^{(t)}$显着低于正常区域。而且，$p^{(t)}$值在整个异常区域中不会保持低值。我们使用$β&lt;&lt; 1(0.1)$以便更加重视召回率的精确度：请注意，异常子序列中的所有点都具有“异常”标签，但实际上在这些中，也会存在许多“正常”行为点。因此，如果“异常”子序列中的很大一部分点被预测为异常就足够了。获得的$τ$值（$Fig.2. (a)- (f)$中的$p^{(t)}$图中的红色虚线）表明$F_{β}-score$（表1）是所考虑的数据集的合适度量。</p><p>&emsp;&emsp;（ii）对于所有数据集，发现阳性似然比（真阳性率/假阳性率）高（超过34.0）。高正似然比值表明在异常区域是异常的概率远高于在正常区域是异常的概率。</p><p>&emsp;&emsp;（iii）$Fig.2$ （f.1）和（f.2）显示了所选隐藏单元的激活，分别来自功率数据集的LSTM-L1（30个单元的低隐藏层）和LSTM-L2（20个单元的高隐藏层）各4个。在$Fig.2$ （f.2）中所示的最后激活序列中标记为$w_{1}$和$w_{2}$的子序列表示该隐藏单元激活在工作日期间高而在周末期间低。这些是由较高隐藏层学习的<em>high-level</em>特征的实例，其似乎以周为时间尺度操作。</p><p>&emsp;&emsp;（iv）如表1所示，对于没有任何长期时间依赖性的“ECG”和“引擎”数据集，LSTM-AD和RNN-AD的表现同样良好。另一方面，对于具有长期时间依赖性和短期依赖性的“航天飞机”和“电力需求”数据集，在$F_{0.1}-score$上LSTM-AD比RNN-AD分别显着提高了18％和30％。 </p><p>&emsp;&emsp;（v）“发动机”数据集故障前检测到的异常点的比例高于正常运行期间的异常点。这表明我们的方法可能对早期故障预测很有用。</p><img src="/2019/09/19/《Long-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series》/3.png" title="Table 1: RNN和LSTM结构的精度，召回和$F_{0.1}-score$ 注意:（30-20）分别表示第一和第二隐藏层中的30和20个单位。"><h3 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h3><p>&emsp;&emsp;我们已经证明了（i）堆叠式LSTM网络能够在没有模式持续时间的先验知识的情况下学习更高级别的时间模式，因此（ii）堆叠的LSTM网络可能是建模正常时间序列行为的可行技术，然后可以用于检测异常。 我们的LSTM-AD方法在四个真实世界数据集上产生了有希望的结果，这些数据集涉及建模短期和长期时间依赖性。 与RNN-AD相比，LSTM-AD给出了更好或类似的结果，表明与基于RNN的模型相比，基于LSTM的预测模型可能更稳健，特别是当我们事先不知道正常行为是否涉及长期依赖性时。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] M. Basseville and I. V. Nikiforov. Detection of abrupt changes: theory and application. Prentice Hall, 1993.<br>[2] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation,9(8):1735–1780, 1997.<br>[3] M. Hermans and B. Schrauwen. Training and analysing deep recurrent neural networks. Advances in Neural Information Processing Systems 26, pages 190–198, 2013.<br>[4] D. George. How the brain might work: A hierarchical and temporal model for learning and recognition. PhD Thesis, Stanford University, 2008.<br>[5] P. Hayton et al. Static and dynamic novelty detection methods for jet engine health monitoring. Philosophical Transactions of the Royal Society of London, 365(1851):493–514, 2007.<br>[6] J. Ma and S. Perkins. Online novelty detection on temporal sequences. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 613–618. ACM, 2003. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Long Short Term Memory Netwo
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://pl741.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="时间序列异常检测" scheme="http://pl741.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>《Detecting Spacecraf Anomalies Using LSTMs and Nonparametric Dynamic Thresholding》</title>
    <link href="http://pl741.github.io/2019/09/16/%E3%80%8ADetecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding%E3%80%8B/"/>
    <id>http://pl741.github.io/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/</id>
    <published>2019-09-16T10:00:30.000Z</published>
    <updated>2019-10-09T03:11:31.363Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1802.04431.pdf" target="_blank" rel="noopener">Detecting Spacecraf Anomalies Using LSTMs and Nonparametric Dynamic Thresholding</a></p><h3 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h3><p>异常检测&emsp;神经网络&emsp;RNN&emsp;LSTM&emsp;时间序列</p><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><h4 id="利用LSTMs进行遥测值预测"><a href="#利用LSTMs进行遥测值预测" class="headerlink" title="利用LSTMs进行遥测值预测"></a>利用LSTMs进行遥测值预测</h4><p>&emsp;&emsp;模型确定后，提供一种非参数、动态无监督的阈值方法来评估残差。</p><p>&emsp;&emsp;为每个单通道创建一个单独的模型，使用每个模型预测该通道的值。为每个通道单独建模还可以跟踪通道级别，实现航天器异常模式的细粒度检测。考虑时间序列$X=\left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(n)}\right\}$，其中时间序列中的每一步$\mathbf{x}^{(t)} \in R^{m}$为$m$维向量$\left\{x_{1}^{(t)}, x_{2}^{(t)}, \ldots, x_{m}^{(t)}\right\}$对应于输入向量，对于每个点$\mathbf{x}^{(t)}$，序列长度$l_s$决定输入模型进行预测的点的数量，预测长度$l_p$决定预测的步长，预测维度$d$的范围为$1 \leq d \leq m$。要预测单个通道的遥测值则$d=1$，同时使用$l_p=1$限制每个步骤$t$的预测数量，以减少运行时间。在每个步骤$t$为实际遥测值生成单个标量预测值$\hat{y}^{(t)}$。在本次实验中输入到LSTM中的$x^{(t)}$包括给定信道的先验遥测值和发送到航天器的编码命令信息。发出命令的模块和发送或接收命令的模块的组合是一个 one-hot 编码的模块，插入到每个步骤$t$中。</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/1.png" title="图1. 每个时间步长$t$预测所用输入矩阵的可视化表示。将当前预测误差与过去预测误差进行比较，以确定是否异常"><h4 id="动态误差阈值"><a href="#动态误差阈值" class="headerlink" title="动态误差阈值"></a>动态误差阈值</h4><p>&emsp;&emsp;本文提出一种方法，可以在不做对过去平滑误差分布做高斯假设的情况下有效地标识极值。每一步$t$产生一个预测值$\hat{y}^{(t)}$，预测误差$e^{(t)}=\left|y^{(t)}-\hat{y}^{(t)}\right|$，其中$y^{(t)}=x_{i}^{(t+1)}$，其中$i$对应于真实遥测值得维度，将每个误差$e^{(t)}$添加到一维误差$e$ 向量中，</p><script type="math/tex; mode=display">\mathbf{e}=\left[e^{(t-h)}, \ldots, e^{\left(t-l_{s}\right)}, \ldots, e^{(t-1)}, e^{(t)}\right]</script><p>其中$h$表示用于评估当前误差的历史误差值的数量，然后对误差集$\mathbf e$进行平滑以抑制LSTM预测中的尖锐误差值，这些尖锐误差会影响预测结果，即使在正常情况下，也会出现误差值的急剧峰值。本文使用指数加权平均（EWMA）来产生平滑误差$\mathbf{e}_{s}=\left[e_{s}^{(t-h)}, \ldots, e_{s}^{(t-l s)}, \ldots, e_{s}^{(t-1)}, e_{s}^{(t)}\right]$。为评估这些值是否为正常值，本文将为平滑预测误差设置一个阈值，将阈值以上的平滑预测误差值对应的值分类为异常。</p><p>&emsp;&emsp;<strong>阈值计算和异常评分</strong>：本文提出一种无监督的异常阈值计算方法，可以在低开销、不使用标记数据或误差统计假设的情况下实现高效的阈值计算。阈值$\epsilon$从以下集合中选出：</p><script type="math/tex; mode=display">\boldsymbol{\epsilon}=\mu\left(\mathbf{e}_{s}\right)+\mathbf{z} \sigma\left(\mathbf{e}_{s}\right)</script><p>其中$\epsilon$取决于：</p><script type="math/tex; mode=display">\epsilon=\arg \max (\boldsymbol{\epsilon})=\frac{\Delta \mu\left(\mathbf{e}_{s}\right) / \mu\left(\mathbf{e}_{s}\right)+\Delta \sigma\left(\mathbf{e}_{s}\right) / \sigma\left(\mathbf{e}_{s}\right)}{\left|\mathbf{e}_{a}\right|+\left|\mathbf{E}_{s e q}\right|^{2}}</script><p>其中：</p><script type="math/tex; mode=display">\begin{array}{l}{\Delta \mu\left(\mathbf{e}_{s}\right)=\mu\left(\mathbf{e}_{s}\right)-\mu\left(\left\{e_{s} \in \mathbf{e}_{s} | e_{s}<\epsilon\right\}\right)} \\ {\Delta \sigma\left(\mathbf{e}_{s}\right)=\sigma\left(\mathbf{e}_{s}\right)-\sigma\left(\left\{e_{s} \in \mathbf{e}_{s} | e_{s}<\epsilon\right\}\right)} \\ {\mathbf{e}_{a}=\left\{e_{s} \in \mathbf{e}_{s} | e_{s}>\epsilon\right\}} \\ {\mathbf{E}_{s e q}=\text { continuous sequences of } e_{a} \in \mathbf{e}_{a}}\end{array}</script><p>使用$z \in \mathbf{z}$来确定$\epsilon$的评估值，其中$\mathbf z$是一个有序正值集，表示标准差大于$\mu\left(\mathbf{e}_{s}\right)$的数量。$\mathbf z$的值取决于上下文，但根据实验结果，$2 - 10$之间的范围可以很好的工作。$z &lt;2$的值通常会导致过多的假阳性。一旦确定了$\arg \max (\boldsymbol{\epsilon})$，每个得到的平滑错误序列$\mathbf{e}_{s e q} \in \mathbf{E}_{s e q}$都会得到一个异常分数$s$，用来表示异常的严重程度：</p><script type="math/tex; mode=display">s^{(i)}=\frac{\max \left(\mathbf{e}_{s e q}^{(i)}\right)-\arg \max (\boldsymbol{\epsilon})}{\mu\left(\mathbf{e}_{s}\right)+\sigma\left(\mathbf{e}_{s}\right)}</script><p>也就是说，如果找到一个阈值，去掉超过它的所有值，平滑误差$\mathbf e_{s}$的均值和标准差都会下降最大的百分比。该函数还惩罚具有最大异常值$\left(\left|\mathbf{e}_{a}\right|\right)$和序列$\left(\left|\mathbf{E}_{s e q}\right|\right)$以防止过度贪心行为。然后根据每个异常误差序列到所选阈值的距离，给出平滑误差最大值的归一化分数。</p><h4 id="减少误报"><a href="#减少误报" class="headerlink" title="减少误报"></a>减少误报</h4><p>&emsp;&emsp;<strong>修剪异常</strong>：基于预测的异常检测方法的精度很大程度上取决于用于设置阈值和判定当前预测误差的历史数据量$(h)$。为了减少误报、限制内存和计算成本，我们引入了一个剪枝过程，创建一个新集合$\mathbf{e}_{m a x}$，包含按照降序排序的所有$\mathbf{e}_{s e q}$的$\max \left(\mathbf{e}_{s e q}\right)$。同时在$\mathbf e_{max}$的末尾添加一个非异常$\max \left(\left\{e_{s} \in \mathbf{e}_{s} \in \mathbf{E}_{s e q} | e_{s} \in \mathbf{e}_{a}\right\}\right)$的最大平滑误差。之后以增量的方式逐步执行序列，计算每一步的减少百分比$d^{(i)}=\left(e_{\max }^{(i-1)}-e_{\max }^{(i)}\right) / e_{\max }^{(i-1)}$，其中$i \in\left\{1,2, \ldots,\left(\left|\mathbf{E}_{s e q}\right|+1\right)\right\}$。如果在某个步骤$i$中，$d^{(i)}$超过了最小百分比降幅$p$，则所有$e_{m a x}^{(j)} \in \mathbf{e}_{m a x} | j&lt;i$及其对应的异常序列均为异常。如果$d^{(i)}$没有满足最小减少量$p$，对于所有后续平滑的误差序列$d^{(i)}, d^{(i+1)}, \ldots, d^{\left(i+\left|\mathbf{E}_{s e q}\right|+1\right)}$都将被重新分类为正常误差。这种剪枝有助于确保异常序列不是流中常规噪声的结果，并且可通过阈值处理来初始识别异常序列。将评估仅限于少数潜在异常序列中的最大误差比没有阈值处理所需的大量值 - 值比较要有效得多。</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/2.png" title="图2. 展示异常修剪过程的例子"><p>&emsp;&emsp;在这种情况下，$\mathbf{e}_{m a x}=[0.01396,0.01072,0.00994]$，最小下降百分比 $p=0.1$。从异常$2$到异常$1$的下降幅度为 $d^{(1)}=0.23&gt;p$，该序列保留为异常分类。从异常$1$到下一个最高平滑误差$\left(e_{s}=0.0099 \right)$的下降幅度为 $d^{(2)}=.07&lt;p$，因此这个序列被重新分类为正常序列。</p><p>&emsp;&emsp;<strong>学习历史数据</strong>：一旦收集到少量异常历史纪录或标记数据，就可以使用这种学习策略来抑制假阳性。基于相似度$s$的异常通常不会在同一频道内频繁重复出现的假设，可以设置最小分数$s_{min}$，以便在$\boldsymbol{s}&lt;\boldsymbol{s}_{\min }$时，将未来的异常重新分类为正常。最低分数只适用于系统产生异常率超过某一比率的数据通道，并为所有这些通道单独设置$s_{min}$。可以使用通道的先验异常得分来设置适当的$s_{min}$，具体取决于精确度和召回率之间的期望平衡。此外，如果异常检测系统有一种机制，用户可以通过该机制为异常提供标签，那么这些标签还可以用于为给定流设置$s_{min}$。例如，如果一个流或通道有多个合并的假阳性异常，那么$s_{min}$可以设置在这些假阳性异常分数的上界附近。</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/3.png" title="图3 包含上下文异常的遥测流命令信息编码"><p>&emsp;&emsp;这种异常不太可能使用基于限制或距离的方法进行识别。使用已编码的命令信息和信道的先前遥测值生成下一个时间步骤的预测，并产生误差。在这个例子中，一步预测和实际遥测值非常接近，如顶部时间序列所示。利用非参数阈值化方法设置误差阈值，得到标记异常区域内的两个预测异常序列，一个为假阳性，一个为真阳性。假阳性表明需要对序列进行修剪，如果该序列相对接近阈值以下的值，则将该序列重新分类为正常序列（参见图2）。</p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>&emsp;&emsp;我们将异常分为两类：点异常和上下文异常，以区分可能由适当设置的警报或忽略时间信息的基于距离的方法（点异常）识别的异常和需要更复杂的方法（如LSTMs或分层时间记忆（HTM）方法）来检测（上下文异常）的异常。这个特征是从前面提到的三个类别中改编而来的——点异常、上下文异常、集合异常。由于上下文异常和集合异常都需要时间上下文，并且比较难以检测，因此它们都被合并到上下文类别中。</p><p>&emsp;&emsp;<strong>设置：</strong>对于主要发生在时间$t_{a}$包含一个或多个异常序列的每个唯一数据流，我们评估从$t_{s}=t_{a}-3 d$到$t_{f}=t_{a}+2 d$ 时间帧之间的所有遥测值， 其中$d$表示天。使用从$t_{s_{\text {train}}}=t_{s}-2 d$到$t_{f_{\text {train}}}=t_{s}$的值和命令数据为每个唯一流训练模型。如果在这些时间范围内没有足够的数据，则增加额外的天数。5天的异常周期被选择用来平衡两个目标：精度和计算成本。预测异常区域略微扩大，以便将扩展后重叠或邻近的异常区域合并为一个区域，来解释多个异常区域代表一个时间的情况。根据系统识别出的最后一组预测异常序列，对每个标记的遥测异常序列$x_{a} \in \mathbf{x}_{a}$按照下面规则进行评估：</p><p>&emsp;&emsp;（1）真阳性：</p><script type="math/tex; mode=display">\left|e_{a}^{(t)} \in e_{s e q} \in \mathbf{e}_{s e q} : x_{i}^{(t)} \in x_{a}\right|>0</script><p>&emsp;&emsp;对于任意的$x_{a} \in \mathbf{x}_{a}$，换句话说，预测异常序列的任何一部分都属于任何真实标记序列，则结果为真阳性。即使许多预测序列的一部分属于标记序列也仅纪录一个真阳性。</p><p>&emsp;&emsp;（2）如果没有预测序列与阳性标记序列重叠，标记为假阴性。</p><p>&emsp;&emsp;（3）所有没有与标记异常区域重叠的预测序列，标记为假阳性。</p><p>&emsp;&emsp;为简单起见，我们不会根据检测到异常的早期程度或误报与标记区域之间的距离进行评分调整。</p><p>&emsp;&emsp;<strong>批处理：</strong>遥测数据被聚合到一分钟窗口中，按照SMAP和当前系统实现的下行计划，以70分钟/组进行评估。使用$h=2100$计算每70分钟批处理的值，其中$h$是用于计算错误阈值和评估当前批处理的先前值的数量。该系统还非常适合以实时流式方式处理值。</p><p>&emsp;&emsp;<strong>模型参数和评估：</strong>在实验中所有模型使用相同的架构和参数：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/4.png"><p>&emsp;&emsp;每个模型都是只含有两层隐藏层和80个（/层）单元的浅层模型。我们发现这种结构能够提供足够的能力来很好的预测单个通道，增加额外的能力在增加模型大小和训练时间的同时对提供预测效益没有任何用处。所有通道不一定都需要这么大的容量，未来的改进可能包括根据信道复杂度自动选择合适的模型容量。类似地，序列长度$l_{s}=250$提供了性能和训练时间之间的平衡。SMAP和MSL在输入维度上的差异是由于每个任务都有不同的命令模块集。在模型训练过程中，早期停止被用来防止过度拟合，并不是所有的模型都经过了完整35次迭代的训练。预测生成后，采用3.2节的方法计算平滑误差的异常阈值，其中$\mathbf{z}=\{2.5,3.0,3.5, \ldots, 10.0\}$，并且后续异常值之间的最小百分比只差$p=0.13$。参数$p$是控制精确度和召回率的重要标准，当标签可用时可推断出一个适当的值。在我们的设置中，在$0.05&lt;p&lt;0.20$达到了合理的结果。</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/5.png" title="图4 参数方法和本文提出的方法（标记为“非参数”）在不同参数设置下的总体精度和召回结果的比较的图。"><p>&emsp;&emsp;<strong>参数阈值比较：</strong>使用原始的LSTM进行误差预测，我们还使用参数误差评估方法生成异常，该方法与Numenta异常基准的最精确模型相协调。这种方法递增地处理原始误差-在每个步骤$t$，将历史误差的窗口$W$建模为正态分布，并且在每个步骤$t$更新均值$μ_{w}$和方差$σ_{W}^{2}$。我们设置$W$的长度为$l_{w}=h=2100$，并对两种方法使用相同的预测误差集。然后计算预测误差长度$l_{short}$的短期平均值$μ_{s}$，其平滑效果与第3.2节中的EWMA平滑相似。然后使用尾概率$Q$来定义异常$L$的可能性:</p><script type="math/tex; mode=display">L=1-Q\left(\frac{\mu_{s}-\mu_{W}}{\sigma_{W}^{2}}\right)</script><p>如果$L \geq 1-\epsilon_{n o r m}$的值为分类为异常。在下一节中，使用$l_{short}=10$和$\epsilon_{n o r m}=\{0.01,0.0001\}$生成的结果与3.2节中的方法进行比较。</p><h3 id="复现实验"><a href="#复现实验" class="headerlink" title="复现实验"></a>复现实验</h3><p>&emsp;&emsp;GitHub地址：<a href="https://github.com/khundman/telemanom" target="_blank" rel="noopener">https://github.com/khundman/telemanom</a></p><p>&emsp;&emsp;<strong>遥测数据：包含两个分支</strong>  </p><p>&emsp;&emsp;&emsp;1.  master：包含标记异常的数据并用于复现KDD论文中的实验<br>&emsp;&emsp;&emsp;2.  no-labels：未标记的数据（一组时间序列流）</p><p>&emsp;&emsp;<strong>开始</strong>：</p><p>&emsp;&emsp;&emsp;克隆仓库——<code>git clone https://github.com/khundman/telemanom.git &amp;&amp; cd telemanom</code></p><p>&emsp;&emsp;&emsp;Curl和Unzip数据——<code>curl -O https://s3-us-west-2.amazonaws.com/telemanom/data.zip &amp;&amp; unzip data.zip &amp;&amp; rm data.zip</code></p><p>&emsp;&emsp;&emsp;使用Python 3.6+安装依赖项——<code>pip install -r requirements.txt</code></p><p>&emsp;&emsp;&emsp;在<code>config.yaml</code>文件中配置系统/模型参数：<br>&emsp;&emsp;&emsp;&emsp;&emsp;<code>train:</code> 如果是<code>True</code>，将为每个输入流训练一个新模型；如果是 <code>False</code> ，将加载现有的训练模型并用于生成预测。<br>&emsp;&emsp;&emsp;&emsp;&emsp;<code>predict:</code> 如果是<code>Ture</code>，使用模型生成新的预测；如果是<code>False</code>，请在评估中使用现有的已保存预测（用于调整错误阈值并跳过先前的处理步骤）<br>&emsp;&emsp;&emsp;&emsp;&emsp;<code>l_s:</code> 250 确定在每个时间步$t$上输入到模型的先前时间步数（用于生成预测）</p><p>&emsp;&emsp;&emsp;开始实验——<code>python run.py</code></p><p>&emsp;&emsp;&emsp;利用<code>Jupyter notebook</code>来进行结果评估并比较不同参数的运行结果在<code>results/</code>下提供。启动notebook——<code>jupyter notebook results/result-viewer.ipynb</code><br>&emsp;&emsp;&emsp;Plotly用于生成交互式内联图，例如：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/6.png"><p>&emsp;&emsp;<strong>数据</strong>：</p><p>&emsp;&emsp;&emsp;&emsp;<strong>使用你自己的数据</strong></p><p>&emsp;&emsp;&emsp;首先划分数据集为训练数据集和测试数据集并分别放在<code>data/train/</code>和<code>data/test/</code>文件夹下。对于训练集和测试集为每个通道或流生成形状为<code>(n_timesteps, n_inputs)</code>的一个<code>.npy</code>文件。每个文件名应该具有唯一的名称或ID。在测试数据中预测的遥测值必须是输入中的第一个特征。</p><p>&emsp;&emsp;&emsp;例如，<code>T-1</code>通道应该具有被命名为<code>T-1.npy</code>的训练集/测试集，形状类似于<code>(4900, 61)</code>和<code>(3925, 61)</code>，其中输入维度大小是<code>(61)</code>。实际遥测值应沿第一维<code>(4900, 1)</code>和<code>(3925, 1)</code>。</p><p>&emsp;&emsp;&emsp;&emsp;<strong>原始数据</strong></p><p>&emsp;&emsp;&emsp;可供下载的原始数据代表了来自SMAP卫星和MSL卫星真实的航天器遥测数据和异常。所有数据都已在时间上匿名化，并且所有遥测值都根据测试集中的最小值/最大值在<code>(-1,1)</code>之间预先缩放。通道ID也被匿名化，但是第一个字母表示出通道的类型（<code>P</code>=功率，<code>R</code>=辐射）。模型输入数据还包括有关特定航天器模块在给定时间窗口内发送或接收的命令的<code>one-hot</code>编码信息。模型输入数据还包括有关特定航天器模块在给定时间窗口内发送或接收的命令的一键编码信息。数据中不包含与命令的时间或特性有关的识别信息。 例如：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/3.png"><p>&emsp;&emsp;&emsp;此数据还包括使用<code>config.yaml</code>中的默认设置生成的预分割测试和训练数据，预训练模型，预测和平滑错误。当熟悉仓库时，可通过运行<code>result-viewer.ipynb</code>来可视化结果。数据对于隔离系统的各个部分也很有用。例如，如果你希望在不训练新模型的情况下看到阈值参数更改的影响，可以在<code>config.yaml</code>中将<code>Train</code>和<code>Predict</code>设置为<code>False</code>，以使用从之前模型生成的预测。</p><p>&emsp;&emsp;<strong>异常标签和元数据</strong>：</p><p>&emsp;&emsp;&emsp;异常标签和元数据可在<code>labeled_anomalies.csv</code>中使用，包括：</p><p>&emsp;&emsp;&emsp; <code>channel_id</code>: 匿名的频道ID———首字母代表频道的性质（<code>P</code>=功率，<code>R</code>=辐射）。<br>&emsp;&emsp;&emsp; <code>spacecraft</code>: 产生遥测流的航天器<br>&emsp;&emsp;&emsp; <code>anomaly_sequences</code>: 流中真实异常开始和结束的索引<br>&emsp;&emsp;&emsp; <code>class</code>: 异常的类别<br>&emsp;&emsp;&emsp; <code>num values</code>: 每个流中的遥测值数量</p><p>&emsp;&emsp;&emsp;要提供自己的标签，请使用<code>labeled_anomalies.csv</code>文件作为模板。  唯一需要的字段/列是<code>channel_id</code>和<code>anomaly_sequences</code>。 <code>anomaly_sequences</code>是列表中的一列，其中包含通道的测试数据集中异常区域的开始和结束索引。</p><p>&emsp;&emsp;<strong>数据集和性能统计</strong>：</p><p>&emsp;&emsp;&emsp;&emsp;数据：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/7.png"><p>&emsp;&emsp;&emsp;&emsp;性能统计：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/8.png"><p>&emsp;&emsp;<strong>处理</strong>：</p><p>&emsp;&emsp;每次启动系统时，唯一的日期时间ID（例如2018-05-17_16.28.00）将用于创建以下内容:</p><p>&emsp;&emsp;<code>results</code>文件：（在<code>results /</code>中），该文件扩展了<code>labeled_anomalies.csv</code>包括已识别的异常序列和相关信息。</p><p>&emsp;&emsp;<code>data subdirectory</code>: 包含用于每个通道的已创建模型，预测和平滑错误的数据文件。 还创建了一个名为<code>params.log</code>的文件，其中包含参数设置和处理期间的日志记录输出。</p><p>&emsp;&emsp;如前所述，jupyter notebook的<code>results / result-viewer.ipynb</code>可用于可视化每个流的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.04431.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Detecting Spacecraf Anomalies Using LSTMs and Nonparametric
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://pl741.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="时间序列异常检测" scheme="http://pl741.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>使用链接</title>
    <link href="http://pl741.github.io/2019/09/16/%E9%93%BE%E6%8E%A5/"/>
    <id>http://pl741.github.io/2019/09/16/链接/</id>
    <published>2019-09-16T07:38:06.000Z</published>
    <updated>2019-09-16T10:09:59.426Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.jianshu.com/p/4eaddcbe4d12" target="_blank" rel="noopener">搭建个人博客</a></p><p><a href="http://blog.rexking6.top/2017/03/30/hexo%E4%B8%BB%E9%A2%98%E5%92%8C%E6%B7%BB%E5%8A%A0%E8%AF%84%E8%AE%BA%E3%80%81%E6%89%93%E8%B5%8F%E3%80%81%E6%90%9C%E7%B4%A2%E3%80%81%E9%98%85%E8%AF%BB%E9%87%8F%E7%AD%89%E5%8A%9F%E8%83%BD/" target="_blank" rel="noopener">博客个性化设置</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/4eaddcbe4d12&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;搭建个人博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.rexking6.top/2017/
      
    
    </summary>
    
    
    
      <category term="配置" scheme="http://pl741.github.io/tags/%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>jupyter主题设置</title>
    <link href="http://pl741.github.io/2019/09/16/jupyter_%E4%B8%BB%E9%A2%98%E8%AE%BE%E7%BD%AE/"/>
    <id>http://pl741.github.io/2019/09/16/jupyter_主题设置/</id>
    <published>2019-09-16T05:38:06.000Z</published>
    <updated>2019-09-16T09:57:34.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="jupyter-主题设置"><a href="#jupyter-主题设置" class="headerlink" title="jupyter 主题设置"></a>jupyter 主题设置</h2><p>安装主题：pip install jupyterthemes</p><p>如果之前安装过可以更新一下：pip install —upgrade jupyterthemes</p><p>设置主题：蓝色主题——jt -t onedork -fs 95 -altp -tfs 11 -nfs 115 -cellw 88% -T</p><p>​                    黑色主题——jt -t monokai -f fira -fs 13 -cellw 90% -ofs 11 -dfs 11 -T -N</p><p>​                    部分参数：-f(字体)  -fs(字体大小) -cellw(占屏比或宽度)  </p><p>​                                      -ofs(输出段的字号)  -T(显示工具栏)  -N(显示自己主机名)</p><h4 id="Conda环境自由切换："><a href="#Conda环境自由切换：" class="headerlink" title="Conda环境自由切换："></a>Conda环境自由切换：</h4><p>​        看一下是否已经把 Anaconda 中创建的所有定制环境作为核心添加在了 Jupyter Notebook 中。这样我们就能简单地利用 Kernel 按钮切换环境。换核的时候不需要重启 notebook。</p><p>​        假设你的 Anaconda 环中有两个自定义的环境 my_NLP 和 gym。按照下面的步骤将这些添加到你的 Jupyter Notebook 中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda activate my_NLP</span><br><span class="line"># Install the IPython Kernel </span><br><span class="line">pip install ipykernel</span><br><span class="line"># Link your environment with Jupyter </span><br><span class="line"># Repeat steps for the other environment, gym</span><br><span class="line">python -m ipykernel install --user --name=my_NLP</span><br><span class="line">pip install ipykernel </span><br><span class="line">python -m ipykernel install --user --name=gym</span><br></pre></td></tr></table></figure><p>​        现在打开你的 Jupyter Notebook，找到 kernel 按钮下的 Change Kernel 选项，接下来就是见证奇迹的时刻：所有的核都被列举出来了，你可以通过简单地点击来激活一个服务核。</p><h4 id="其他功能："><a href="#其他功能：" class="headerlink" title="其他功能："></a>其他功能：</h4><p>​        安装 nbextensions for Jupyter Notebooks</p><p>​        安装 nbextensions 是很容易的，简单地遵循下面的步骤就行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Stop and exit your Jupyter Notebook server </span><br><span class="line"># Make sure you are in the base environment</span><br><span class="line">conda activate base</span><br><span class="line"># Install the nbextensions </span><br><span class="line">pip install jupyter_contrib_nbextensions</span><br><span class="line"># Install the necessary JS and CSS files </span><br><span class="line">jupyter contrib nbextension install --system</span><br></pre></td></tr></table></figure><p>​        启动 Jupyter notebook 服务，你可以在起始页看到第四个叫做 Nbextensions 的选项。点击这个选项，然后就可以看到极妙的功能集，这些都是你一直希望在 Jupyter Notebooks 中拥有的。</p><p>其中一些的简单介绍：</p><p>​          Table of Contents(2)：单击生成整个笔记本的目录，不同的 section 都有对应的超链接。</p><p>​         Scratchpad：在我看来绝对是最好的扩展了。这是一个你可以在里面做代码实验的独立空间，不会干扰笔记本中的其他部分。</p><p>​        Codefolding ：代码折叠，这个不需要做过多的解释。</p><p>​        Hide Input All：隐藏所有的代码单元，同时保持所有的输出和 markdown 单元可见。如果你要向非技术人员解释你的结果，那么这就会是一个很有用的功能。</p><p>​        Variable Inspector：将你从调试的忧伤中拯救出来，这与 Spyder IDE 中的变量检查窗口有些类似。</p><p>​        Spellchecker：对 markdown 单元中的内容进行拼写检查。</p><p>​        Zenmode：移除掉屏幕中杂乱无关的内容，以便你能够聚焦于重要的东西上，例如代码。</p><p>​        Snippets Menu：从 list comprehension 到 pandas 以及它们之间的所有常用代码片段的一个很酷的集合。这是最好的部分？你可以修改窗口的小部件来添加你自己的定制片段。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;jupyter-主题设置&quot;&gt;&lt;a href=&quot;#jupyter-主题设置&quot; class=&quot;headerlink&quot; title=&quot;jupyter 主题设置&quot;&gt;&lt;/a&gt;jupyter 主题设置&lt;/h2&gt;&lt;p&gt;安装主题：pip install jupyterthemes
      
    
    </summary>
    
    
    
      <category term="配置" scheme="http://pl741.github.io/tags/%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>机器学习评价标准</title>
    <link href="http://pl741.github.io/2019/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86/"/>
    <id>http://pl741.github.io/2019/09/16/机器学习评价标准/</id>
    <published>2019-09-16T05:38:06.000Z</published>
    <updated>2019-09-16T07:24:19.337Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本内容"><a href="#基本内容" class="headerlink" title="基本内容"></a>基本内容</h1><p><strong>True真&emsp; False假&emsp;Positive正&emsp;Negative负</strong></p><p>&emsp;&emsp;异常检测中，P和N一般是针对预测来说的，Positive正类指的是你更关心的那一类！即“异常”，P指预测为正类，即预测为异常。T和F针对预测与真实情况的比较， True指正确匹配，F指错误匹配。</p><div class="table-container"><table><thead><tr><th></th><th style="text-align:center">实际正例</th><th style="text-align:center">实际负例</th><th style="text-align:center"></th></tr></thead><tbody><tr><td>预测正例P</td><td style="text-align:center">TP</td><td style="text-align:center">FP</td><td style="text-align:center">所有预测为正的个数TP+FP</td></tr><tr><td>预测负例N</td><td style="text-align:center">FN</td><td style="text-align:center">TN</td><td style="text-align:center">所有预测为负的个数FN+TN</td></tr><tr><td></td><td style="text-align:center">所有实际正例的个数TP+FN</td><td style="text-align:center">所有实际负例的个数FP+TN</td></tr></tbody></table></div><img src="/2019/09/16/机器学习评价标准/72.png"><ul><li><p>TPR：真正类率，代表预测是异常实际也是异常的样本数，占实际总异常数的比例——值越大 性能越好</p></li><li><p>FPR：假正类率，代表预测是异常但实际是正常的样本数，占实际正常总数的比例——值越小 性能越好</p></li><li><p>R：召回率，意义同TPR——值越大 性能越好</p></li><li><p>P：精确率Precision，代表预测是异常实际也是异常的样本数，占预测是异常的总数的比例——值越大 性能越好</p></li><li><p>F：P和R的加权调和平均，常用的是F1值——值越大 性能越好</p></li><li><p>A：正确率Accuracy，与精确率的区别是，不仅考虑异常类也考虑正常类，即所有匹配样本数，占所有样本的比例——值越大 性能越好</p></li></ul><p>另外还有两个，分别为：虚警率和漏警率</p><ul><li>虚警率（<strong>False alarm</strong>）表示负类样本被分为正类样本在所有负类样本中的比例</li></ul><img src="/2019/09/16/机器学习评价标准/71.png"><ul><li>漏警率表示（漏警率表示（Missing alarm）表示正类样本被分为负类样本在所有正类样本中的比例Missing alarm）表示正类样本被分为负类样本在所有正类样本中的比例</li></ul><img src="/2019/09/16/机器学习评价标准/70.png"><h1 id="Tensorflow实现"><a href="#Tensorflow实现" class="headerlink" title="Tensorflow实现"></a>Tensorflow实现</h1><p><strong>&emsp;损失值：</strong><br>    tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)<br><strong>&emsp;参数解析：</strong><br>&emsp; logits:神经网络最后一层的输出，如果有batch，大小为[batch_size, n_classes]<br>&emsp; labels:实际标签，大小同上</p><p><strong>&emsp;执行过程：</strong><br>&emsp;&emsp;先对网络最后一层的输出做一个softmax，通常是求取输出属于某一类的概率，对于单样本而言，输出就是一个num_classes大小的向量。</p><img src="/2019/09/16/机器学习评价标准/73.png"><p>&emsp;然后将softmax的输出向量与样本的实际标签做一个交叉熵。</p><img src="/2019/09/16/机器学习评价标准/74.png"><p>&emsp;其中 y’ 指代实际的标签中第i个的值（用mnist数据举例，如果是3，那么标签是[0，0，0，1，0，0，0，0，0，0]，除了第4个值为1，其他全为0）; y就是softmax的输出向量[Y1，Y2,Y3…]中，第i个元素的值</p><p>&emsp;&emsp;显而易见，预测越准确，结果的值越小（别忘了前面还有负号），最后求一个平均，得到我们想要的loss</p><p>&emsp;&emsp;注意！！！这个函数的返回值并不是一个数，而是一个向量，如果要求交叉熵，我们要再做一步tf.reduce_sum操作,就是对向量里面所有元素求和，最后才得到H(y)，如果求loss，则要做一步tf.reduce_mean操作，对向量求均值！</p><pre><code>tf.reduce_mean(input_tensor, axis=None,keep_dims=False,name=None,               reduction_indices=None)</code></pre><h4 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h4><ul><li>&emsp; 第一个参数input_tensor： 输入的待降维的tensor;</li><li>&emsp; 第二个参数axis： 指定的轴，如果不指定，则计算所有元素的均值;</li><li>&emsp; 第三个参数keep_dims：是否降维度，设置为True，输出的结果保持输入tensor的形状，设置为False，输出结果会降低维度;</li><li>&emsp; 第四个参数name： 操作的名称;</li><li>&emsp; 第五个参数 reduction_indices：在以前版本中用来指定轴，已弃用;</li></ul><p>&emsp;predict是预测结果，也就是神经网络的输出，real是真实的标签，sess就是tensorflow当前的会话，feed_dict是需要喂的数据。</p><p>&emsp;tf.equal(A, B)是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，反正返回False，返回的值的矩阵维度和A是一样的。</p><p>&emsp;tf.cast()函数的作用是执行 tensorflow 中张量数据类型转换，<br>    tf.cast(x, dtype, name=None)</p><h4 id="参数解析-1"><a href="#参数解析-1" class="headerlink" title="参数解析"></a>参数解析</h4><ul><li>&emsp;第一个参数 x:   待转换的数据（张量）</li><li>&emsp;第二个参数 dtype： 目标数据类型</li><li>&emsp;第三个参数 name： 可选参数，定义操作的名称</li></ul><p>&emsp;tf.loical_and()将数值变成逻辑值<br>    tf.logical_and(x, y, name=None)</p><p>&emsp;tf.argmax()返回值是是数值最大值的索引位置，如果最大值位置相同，则分类正确，反之则分类错误<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">predictions = tf.argmax(predict, <span class="number">1</span>)</span><br><span class="line">actuals = tf.argmax(real, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将上述获得的变量设置成元素为0或者为1的矩阵</span></span><br><span class="line">ones_like_actuals = tf.ones_like(actuals)</span><br><span class="line">zeros_like_actuals = tf.zeros_like(actuals)</span><br><span class="line">ones_like_predictions = tf.ones_like(predictions)</span><br><span class="line">zeros_like_predictions = tf.zeros_like(predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照前面的计算公式编写如下计算代码</span></span><br><span class="line"></span><br><span class="line">tp_op = tf.reduce_sum(tf.cast(tf.logical_and(</span><br><span class="line">            tf.equal(actuals, ones_like_actuals),</span><br><span class="line">            tf.equal(predictions, ones_like_predictions)),<span class="string">"float"</span>))</span><br><span class="line"> </span><br><span class="line">tn_op = tf.reduce_sum(tf.cast(tf.logical_and(</span><br><span class="line">            tf.equal(actuals, zeros_like_actuals),</span><br><span class="line">            tf.equal(predictions, zeros_like_predictions)),<span class="string">"float"</span>))</span><br><span class="line"> </span><br><span class="line">fp_op = tf.reduce_sum(tf.cast(tf.logical_and(</span><br><span class="line">            tf.equal(actuals, zeros_like_actuals),</span><br><span class="line">            tf.equal(predictions, ones_like_predictions)),<span class="string">"float"</span>))</span><br><span class="line"> </span><br><span class="line">fn_op = tf.reduce_sum(tf.cast(tf.logical_and(</span><br><span class="line">            tf.equal(actuals, ones_like_actuals),</span><br><span class="line">            tf.equal(predictions, zeros_like_predictions)),<span class="string">"float"</span>))</span><br><span class="line">            </span><br><span class="line">tp, tn, fp, fn = session.run([tp_op, tn_op, fp_op, fn_op], feed_dict)</span><br><span class="line">tpr = float(tp)/(float(tp) + float(fn))</span><br><span class="line">fpr = float(fp)/(float(fp) + float(tn))</span><br><span class="line">fnr = float(fn)/(float(tp) + float(fn))</span><br><span class="line">accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))</span><br><span class="line">recall = tpr</span><br><span class="line">precision = float(tp)/(float(tp) + float(fp))</span><br><span class="line">f1_score = (<span class="number">2</span> * (precision * recall)) / (precision + recall)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本内容&quot;&gt;&lt;a href=&quot;#基本内容&quot; class=&quot;headerlink&quot; title=&quot;基本内容&quot;&gt;&lt;/a&gt;基本内容&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;True真&amp;emsp; False假&amp;emsp;Positive正&amp;emsp;Negative负&lt;/stro
      
    
    </summary>
    
    
    
      <category term="机器学习" scheme="http://pl741.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
