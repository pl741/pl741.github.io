<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>木木的博客</title>
  
  <subtitle>木木的博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://pl741.github.io/"/>
  <updated>2019-11-14T13:17:13.964Z</updated>
  <id>http://pl741.github.io/</id>
  
  <author>
    <name>木木</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>剑指offer-Python</title>
    <link href="http://pl741.github.io/2019/11/08/%E5%89%91%E6%8C%87offer-Python/"/>
    <id>http://pl741.github.io/2019/11/08/剑指offer-Python/</id>
    <published>2019-11-08T03:51:07.000Z</published>
    <updated>2019-11-14T13:17:13.964Z</updated>
    
    <content type="html"><![CDATA[<p>参考：所有动图来自牛客网（ <a href="https://www.nowcoder.com/discuss/198840?type=1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/198840?type=1</a> ）</p><h4 id="3-1-数组中重复的数字"><a href="#3-1-数组中重复的数字" class="headerlink" title="3.1  数组中重复的数字"></a>3.1  数组中重复的数字</h4><p>长度为n的数组，所有数字都在0~n-1的范围内，找出数组中任意重复的数字。</p><p>三种方法：</p><ol><li><p>首先将所有数字排序，然后从头到尾扫描排序后的数组。时间复杂度为$O(nlogn)$</p></li><li><p>利用哈希表，从头到尾扫描数组，每一个数字利用$O(1)$的时间检查数字是否在哈希表中，如果没有则加入，如果存在表明此数字重复。时间复杂度$O(n)$，空间复杂度$O(n)$。</p></li><li><p>d</p><img src="/2019/11/08/剑指offer-Python/1.gif"></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="3-2-不修改数组找出重复的数字"><a href="#3-2-不修改数组找出重复的数字" class="headerlink" title="3.2 不修改数组找出重复的数字"></a>3.2 不修改数组找出重复的数字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考：所有动图来自牛客网（ &lt;a href=&quot;https://www.nowcoder.com/discuss/198840?type=1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.nowcoder.com/discuss/19
      
    
    </summary>
    
    
      <category term="Python" scheme="http://pl741.github.io/categories/Python/"/>
    
    
      <category term="Python—剑指offer算法" scheme="http://pl741.github.io/tags/Python%E2%80%94%E5%89%91%E6%8C%87offer%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Python面试题</title>
    <link href="http://pl741.github.io/2019/11/08/Python%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <id>http://pl741.github.io/2019/11/08/Python面试题/</id>
    <published>2019-11-08T03:36:01.000Z</published>
    <updated>2019-11-12T02:43:39.028Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h5 id="1-列举Python中基本数据类型"><a href="#1-列举Python中基本数据类型" class="headerlink" title="1. 列举Python中基本数据类型"></a>1. 列举Python中基本数据类型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">六个标准的数据类型：字符串String、数字Digit、列表List、元组Tuple、集合Sets、字典Dictionary。</span><br></pre></td></tr></table></figure><h5 id="2-区别可变数据类型和不可变数据类型"><a href="#2-区别可变数据类型和不可变数据类型" class="headerlink" title="2. 区别可变数据类型和不可变数据类型"></a>2. 区别可变数据类型和不可变数据类型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">根据对象内存地址来说：</span><br><span class="line">  1. 可变数据类型：在内存地址不变的情况下，值可改变（列表和字典是可变类型，但是字典中的 key 值必须是不可变类型）</span><br><span class="line">  2. 不可变数据类型：内存改变，值也跟着改变。（数字，字符串，布尔类型，都是不可变类型）可以通过 id() 方法进行内存地址的检测。</span><br></pre></td></tr></table></figure><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><h5 id="1-将”hello-world”转换为首字母大写”Hello-World”"><a href="#1-将”hello-world”转换为首字母大写”Hello-World”" class="headerlink" title="1. 将”hello world”转换为首字母大写”Hello World”"></a>1. 将”hello world”转换为首字母大写”Hello World”</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一：</span></span><br><span class="line">    <span class="comment"># title() 方法返回"标题化"的字符串，所有单词的首个字母转化为大写，其余字母均为小写</span></span><br><span class="line">    <span class="string">"hello world"</span>.title()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 方法二：</span></span><br><span class="line">    <span class="comment"># 分析：capitalize()将字符串的第一个字母变成大写,其他字母变小写，题目要求将两个单词的首字母都大写：</span></span><br><span class="line">    arr = <span class="string">"hello world"</span>.split(<span class="string">" "</span>)</span><br><span class="line">    new_str = <span class="string">f"<span class="subst">&#123;arr[<span class="number">0</span>].capitalize()&#125;</span> <span class="subst">&#123;arr[<span class="number">1</span>].capitalize()&#125;</span>"</span></span><br><span class="line">    print(new_str)</span><br><span class="line">    <span class="comment"># f-string，亦称为格式化字符串常量（formatted string literals），是Python3.6新引入的一种字符串格式化方法</span></span><br><span class="line">    <span class="comment"># f-string在形式上是以 f 或 F 修饰符引领的字符串（f'xxx' 或 F'xxx'），以大括号 &#123;&#125; 标明被替换的字段；</span></span><br><span class="line">    <span class="comment"># f-string在本质上并不是字符串常量，而是一个在运行时运算求值的表达式,</span></span><br><span class="line">    <span class="comment"># f-string可以解析任意类型的变量（包括字典，元组，列表，集合，数字，函数）</span></span><br></pre></td></tr></table></figure><h5 id="2-检测字符串中只含有数字"><a href="#2-检测字符串中只含有数字" class="headerlink" title="2. 检测字符串中只含有数字"></a>2. 检测字符串中<font color="red">只含</font>有数字</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过isdigit方法：</span></span><br><span class="line">s1 = <span class="string">"12332"</span>.isdigit()  <span class="comment"># Ture</span></span><br><span class="line">    s1 = <span class="string">"12334a"</span>.isdigit()   <span class="comment"># False</span></span><br></pre></td></tr></table></figure><h5 id="3-字符串进行反转"><a href="#3-字符串进行反转" class="headerlink" title="3. 字符串进行反转"></a>3. 字符串进行反转</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一：</span></span><br><span class="line"><span class="comment"># 使用字符串切片</span></span><br><span class="line">    s1 = <span class="string">"ilovechina"</span>[: : <span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 方法二：</span></span><br><span class="line"><span class="comment"># reversed(seq)返回一个反转的迭代器， seq表示要转换的序列，可以是 tuple, string, list 或 range。</span></span><br><span class="line">    s2 = <span class="string">''</span>.join(reversed(<span class="string">"ilovechina"</span>))</span><br></pre></td></tr></table></figure><h5 id="4-字符串格式化方式"><a href="#4-字符串格式化方式" class="headerlink" title="4. 字符串格式化方式"></a>4. 字符串格式化方式</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># “旧式”字符串解析（%操作符）</span></span><br><span class="line">    print(<span class="string">"I'm %s. I'm %d year old"</span> % (<span class="string">'Vamei'</span>, <span class="number">99</span>)</span><br><span class="line">    <span class="comment"># %s 格式说明符代表了类型为字符串；%x 把一个类型为 Int 的值转成字符串使它代表十六进制数；</span></span><br><span class="line">    print(<span class="string">"I'm %(name)s. I'm %(age)d year old"</span> % &#123;<span class="string">'name'</span>:<span class="string">'Vamei'</span>, <span class="string">'age'</span>:<span class="number">99</span>&#125;)  </span><br><span class="line">    <span class="comment"># 对格式符进行了命名。命名使用()括起来。每个命名对应词典的一个key。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 format() 来做简单的基于位置的字符串格式化</span></span><br><span class="line">    print(<span class="string">'my name is &#123;1&#125; ,age &#123;0&#125;'</span>.format(<span class="number">10</span>,<span class="string">'jack'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串插值/f-Strings(Python 3.6+)</span></span><br><span class="line">    a = <span class="number">5</span></span><br><span class="line">    b = <span class="number">10</span></span><br><span class="line">    <span class="string">f'Five plus ten is <span class="subst">&#123;a + b&#125;</span> and not <span class="subst">&#123;<span class="number">2</span> * (a + b)&#125;</span>.'</span>  <span class="comment"># 'Five plus ten is 15 and not 30.'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串模板（Python标准库）</span></span><br><span class="line">    <span class="keyword">from</span> string <span class="keyword">import</span> Template</span><br><span class="line">    name = <span class="string">"Bob"</span>, errno = <span class="number">50159747054</span>     </span><br><span class="line">    Template(<span class="string">'Hey, $name!'</span>).substitute(name=name)  </span><br><span class="line">    <span class="comment"># 手动转换这个int错误号码为十六进制字符串</span></span><br><span class="line">    Template(<span class="string">'Hey $name, there is a $error error!'</span>).substitute(name=name, error=hex(errno))</span><br></pre></td></tr></table></figure><h5 id="5-字符串开头和末尾都有空格，比如“-adabdw-”，写一个函数把字符串前后空格都去掉"><a href="#5-字符串开头和末尾都有空格，比如“-adabdw-”，写一个函数把字符串前后空格都去掉" class="headerlink" title="5. 字符串开头和末尾都有空格，比如“ adabdw ”，写一个函数把字符串前后空格都去掉"></a>5. 字符串开头和末尾都有空格，比如“ adabdw ”，写一个函数把字符串前后空格都去掉</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># strip可以把它封装到函数中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strip_function</span><span class="params">(s1)</span>：</span></span><br><span class="line"><span class="function">    <span class="title">return</span> <span class="title">s1</span>.<span class="title">strip</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><h5 id="6-获取字符串”123456“最后的两个字符"><a href="#6-获取字符串”123456“最后的两个字符" class="headerlink" title="6. 获取字符串”123456“最后的两个字符"></a>6. 获取字符串”123456“最后的两个字符</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切片使用的考察，最后两个即开始索引是-2</span></span><br><span class="line">a = <span class="string">'123456'</span></span><br><span class="line">print(a[<span class="number">-2</span>: ])</span><br></pre></td></tr></table></figure><h5 id="7-将一个GBK字符串S转换成UTF-8编码格式"><a href="#7-将一个GBK字符串S转换成UTF-8编码格式" class="headerlink" title="7. 将一个GBK字符串S转换成UTF-8编码格式"></a>7. 将一个GBK字符串S转换成UTF-8编码格式</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">"S"</span>.encode(<span class="string">'gbk'</span>).decode(<span class="string">'utf-8'</span>, <span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># str.decode(encoding='UTF-8',errors='strict')</span></span><br><span class="line"><span class="comment"># errors -- 设置不同错误的处理方案。默认为 'strict',意为编码错误引起一个UnicodeError。 其他可能得值有 'ignore', 'replace', 'xmlcharrefreplace', 'backslashreplace' 以及通过 codecs.register_error() 注册的任何值。</span></span><br></pre></td></tr></table></figure><h5 id="8-1-怎样将字符串转换为小写。-2-单引号、双引号、三引号的区别？"><a href="#8-1-怎样将字符串转换为小写。-2-单引号、双引号、三引号的区别？" class="headerlink" title="8.  (1) 怎样将字符串转换为小写。 (2) 单引号、双引号、三引号的区别？"></a>8.  (1) 怎样将字符串转换为小写。 (2) 单引号、双引号、三引号的区别？</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.  使用字符串的 lower() 方法。 </span><br><span class="line">2.  单独使用单引号和双引号没什么区别，但是如果引号里面还需要使用引号的时候，就需要这两个配合使用了，然后说三引号，同样的三引号也分为三单引号和三双引号，两个都可以声名长的字符串时候使用，如果使用 docstring(  文档字符串 ) 就需要使用三双引号。</span><br></pre></td></tr></table></figure><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><h5 id="1-已知-AList-1-2-3-1-2-，对-AList-列表元素去重，写出具体过程。"><a href="#1-已知-AList-1-2-3-1-2-，对-AList-列表元素去重，写出具体过程。" class="headerlink" title="1. 已知 AList = [1,2,3,1,2]，对 AList 列表元素去重，写出具体过程。"></a>1. 已知 AList = [1,2,3,1,2]，对 AList 列表元素去重，写出具体过程。</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AList = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">list(set(AList))</span><br></pre></td></tr></table></figure><h5 id="2-实现”1-2-3”-变成-“1”-”2”-”3”"><a href="#2-实现”1-2-3”-变成-“1”-”2”-”3”" class="headerlink" title="2. 实现”1,2,3” 变成 [“1”,”2”,”3”]"></a>2. 实现”1,2,3” 变成 [“1”,”2”,”3”]</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">"1, 2, 3"</span></span><br><span class="line">print(a.split(<span class="string">","</span>))</span><br></pre></td></tr></table></figure><h5 id="3-给定两个-list，A-和-B，找出相同元素和不同元素"><a href="#3-给定两个-list，A-和-B，找出相同元素和不同元素" class="headerlink" title="3.  给定两个 list，A 和 B，找出相同元素和不同元素"></a>3.  给定两个 list，A 和 B，找出相同元素和不同元素</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A、B 中相同元素：print(set(A)&amp;set(B)) </span><br><span class="line">A、B 中不同元素：print(set(A)^set(B))</span><br></pre></td></tr></table></figure><h5 id="4-1-2-3-4-5-6-一行代码展开该列表，得出-1-2-3-4-5-6"><a href="#4-1-2-3-4-5-6-一行代码展开该列表，得出-1-2-3-4-5-6" class="headerlink" title="4. [[1,2],[3,4],[5,6]] 一行代码展开该列表，得出 [1,2,3,4,5,6]"></a>4. [[1,2],[3,4],[5,6]] 一行代码展开该列表，得出 [1,2,3,4,5,6]</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Input = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line"><span class="comment"># 列表推导式</span></span><br><span class="line">output = [j <span class="keyword">for</span> i <span class="keyword">in</span> Input <span class="keyword">for</span> j <span class="keyword">in</span> i]  </span><br><span class="line"><span class="comment"># itertools类库</span></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">output = list(itertools.chain(*Input))  </span><br><span class="line"><span class="comment"># sum函数合并</span></span><br><span class="line">output = sum(Input, [])   </span><br><span class="line"><span class="comment"># reduce() 函数会对参数序列中元素进行累积。函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。</span></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">output = reduce(list.__add__, Input)</span><br><span class="line"><span class="comment"># 若为numpy对象，即array或者mat，flatten()默认是按行的方向降维，可以先将列表转换成数组，降维后再tolist()</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">output = np.array(Input).flatten().tolist()</span><br></pre></td></tr></table></figure><h5 id="5-合并列表-1-5-7-9-和-2-2-6-8"><a href="#5-合并列表-1-5-7-9-和-2-2-6-8" class="headerlink" title="5. 合并列表 [1,5,7,9] 和 [2,2,6,8]"></a>5. 合并列表 [1,5,7,9] 和 [2,2,6,8]</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>], b = [<span class="number">2</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">8</span>]</span><br><span class="line"><span class="comment"># extend方法</span></span><br><span class="line">a.extend(b)</span><br><span class="line"><span class="comment"># 直接相加</span></span><br><span class="line">c = a + b</span><br></pre></td></tr></table></figure><h5 id="6-打乱一个列表的元素"><a href="#6-打乱一个列表的元素" class="headerlink" title="6. 打乱一个列表的元素"></a>6. 打乱一个列表的元素</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">random.shuffle(a)</span><br></pre></td></tr></table></figure><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><h5 id="1-字典操作中-del-和-pop-有什么区别"><a href="#1-字典操作中-del-和-pop-有什么区别" class="headerlink" title="1.字典操作中 del 和 pop 有什么区别"></a>1.字典操作中 del 和 pop 有什么区别</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2-按照字典的内的年龄排序"><a href="#2-按照字典的内的年龄排序" class="headerlink" title="2.按照字典的内的年龄排序"></a>2.按照字典的内的年龄排序</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d1 = [ &#123;<span class="string">'name'</span>：<span class="string">'alice'</span>, <span class="string">'age'</span>：<span class="number">38</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'name'</span>：<span class="string">'bob'</span>, <span class="string">'age'</span>：<span class="number">18</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'name'</span>：<span class="string">'Carl'</span>, <span class="string">'age'</span>：<span class="number">28</span>&#125; ]</span><br></pre></td></tr></table></figure><h5 id="3-合并两个字典"><a href="#3-合并两个字典" class="headerlink" title="3.合并两个字典"></a>3.合并两个字典</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = &#123;<span class="string">"A"</span>：<span class="number">1</span>,<span class="string">"B"</span>：<span class="number">2</span>&#125;</span><br><span class="line">b = &#123;<span class="string">"C"</span>：<span class="number">3</span>,<span class="string">"D"</span>：<span class="number">4</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="4-使用生成式的方式生成一个字典"><a href="#4-使用生成式的方式生成一个字典" class="headerlink" title="4.使用生成式的方式生成一个字典"></a>4.使用生成式的方式生成一个字典</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="5-把元组-“a”-”b”-和元组-1-2-，变为字典-“a”：1-”b”：2"><a href="#5-把元组-“a”-”b”-和元组-1-2-，变为字典-“a”：1-”b”：2" class="headerlink" title="5. 把元组 (“a”,”b”) 和元组 (1,2)，变为字典 {“a”：1,”b”：2}"></a>5. 把元组 (“a”,”b”) 和元组 (1,2)，变为字典 {“a”：1,”b”：2}</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h5 id="1-（1）s-”info：xiaoZhang-33-shandong”，用正则切分字符串输出-‘info’-‘xiaoZhang’-‘33’-‘shandong’-。（2）a-“你好-中国-“，去除多余空格只留一个空格。"><a href="#1-（1）s-”info：xiaoZhang-33-shandong”，用正则切分字符串输出-‘info’-‘xiaoZhang’-‘33’-‘shandong’-。（2）a-“你好-中国-“，去除多余空格只留一个空格。" class="headerlink" title="1. （1）s=”info：xiaoZhang 33 shandong”，用正则切分字符串输出[‘info’, ‘xiaoZhang’, ‘33’, ‘shandong’]。（2）a = “你好 中国 “，去除多余空格只留一个空格。"></a>1. （1）s=”info：xiaoZhang 33 shandong”，用正则切分字符串输出[‘info’, ‘xiaoZhang’, ‘33’, ‘shandong’]。（2）a = “你好 中国 “，去除多余空格只留一个空格。</h5><ol><li><p>根据冒号或者空格切分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">s = <span class="string">"info：xiaoZhang 33 shandong"</span></span><br><span class="line">res = re.split(<span class="string">r": | "</span>, s)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">"你好       中国"</span></span><br><span class="line">print(<span class="string">" "</span>.join(s.split()))</span><br></pre></td></tr></table></figure></li></ol><h2 id="Git面试题"><a href="#Git面试题" class="headerlink" title="Git面试题"></a>Git面试题</h2><h5 id="1-说说你知道得git命令"><a href="#1-说说你知道得git命令" class="headerlink" title="1. 说说你知道得git命令"></a>1. 说说你知道得git命令</h5><p> git init：该命令将创建一个名为 .git 的子目录，这个子目录含有你初始化的 Git 仓库中所有的必须文件,这些文件是 Git 仓库的骨干<br> git clone url：将服务器代码下载到本地<br> git pull：将服务器的代码拉到本地进行同步，如果本地有修改会产生冲突。<br> git push：提交本地修改的代码到服务器<br> git checkout -b branch：创建并切换分支<br> git status：查看修改状态<br> git add 文件名：提交到暂存区<br> git commit -m “提交内容”：输入提交的注释内容<br> git log：查看提交的日志情况 </p><h5 id="2-git如何查看某次提交修改的内容"><a href="#2-git如何查看某次提交修改的内容" class="headerlink" title="2. git如何查看某次提交修改的内容"></a>2. git如何查看某次提交修改的内容</h5><p> 首先可以 git log 显示历史的提交列表 之后用 git show 便可以显示某次提交的修改内容 同样 git show filename 可以显示某次提交的某个内容的修改信息。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据类型&quot;&gt;&lt;a href=&quot;#数据类型&quot; class=&quot;headerlink&quot; title=&quot;数据类型&quot;&gt;&lt;/a&gt;数据类型&lt;/h2&gt;&lt;h5 id=&quot;1-列举Python中基本数据类型&quot;&gt;&lt;a href=&quot;#1-列举Python中基本数据类型&quot; class=&quot;he
      
    
    </summary>
    
    
      <category term="Python" scheme="http://pl741.github.io/categories/Python/"/>
    
    
      <category term="Python面试题" scheme="http://pl741.github.io/tags/Python%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle比赛</title>
    <link href="http://pl741.github.io/2019/11/04/Kaggle%E6%AF%94%E8%B5%9B/"/>
    <id>http://pl741.github.io/2019/11/04/Kaggle比赛/</id>
    <published>2019-11-04T02:52:20.000Z</published>
    <updated>2019-11-14T12:57:48.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、GBDT回归算法"><a href="#一、GBDT回归算法" class="headerlink" title="一、GBDT回归算法"></a>一、GBDT回归算法</h2><p>参考：</p><p> <a href="https://www.cnblogs.com/peizhe123/p/5086128.html?clicktime=1572921140&amp;enterid=1572921140" target="_blank" rel="noopener">https://www.cnblogs.com/peizhe123/p/5086128.html?clicktime=1572921140&amp;enterid=1572921140</a> </p><p> <a href="https://mp.weixin.qq.com/s/adMMz8u29OnvqelgZuXWjg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/adMMz8u29OnvqelgZuXWjg</a> </p><p> <a href="https://mp.weixin.qq.com/s/M2PwsrAnI1S9SxSB1guHdg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/M2PwsrAnI1S9SxSB1guHdg</a> </p><p>GBDT(Gradient Boosting Decision Tree) 梯度提升决策树树，是一种迭代的决策树算法，由多棵决策树组成，所有树的结果累加成最终答案，即基函数的线性组合。主要的优点：1. 在深度学习没有广泛被应用时，比赛的效果很不错；2. 即可以做分类同时也可以做回归，同时还可处理排序任务； 3. 可以筛选特征。</p><p>Boosting、Bagging和Stacking是集成学习(Ensemble Learning)的三种主要方法。Boosting是一族可将弱学习器提升为强学习器的算法，不同于Bagging、Stacking方法，Boosting训练过程为串联方式，弱学习器的训练是有顺序的，每个弱学习器都会在前一个学习器的基础上进行学习，最终综合所有学习器的预测值产生最终的预测结果。 </p><h3 id="1-数学推导"><a href="#1-数学推导" class="headerlink" title="1. 数学推导"></a>1. 数学推导</h3><p>当基学习器采用决策树时，梯度提升算法就具体到了梯度提升决策树。GBDT算法可看成是$M$棵树组成的加法模型，对应公式：</p><script type="math/tex; mode=display">F(x, w)=\sum_{m=0}^M \alpha_{m}h_{m}(x, w_{m})=\sum_{m=0}^M f_{m}(x, w_{m})</script><p>其中，$x$为输入样本，$w$为模型参数，$h$为分类回归树，$\alpha$为每棵树的权重。</p><h5 id="1-1-GBDT算法的实现过程"><a href="#1-1-GBDT算法的实现过程" class="headerlink" title="1.1 GBDT算法的实现过程"></a>1.1 GBDT算法的实现过程</h5><p>给定训练数据集：$T=\{(x_{1}, y_{i}), (x_{2}, y_{2}), \ldots, (x_{N}, y_{N})\}$，其中$x_{i} \in \chi \subseteq R^{n}$，$\chi$为输入空间，$y_{i} \in Y \subseteq R$， $Y$为输出空间，损失函数$L(y, f(x))$，找出最终的回归树$F_{M}$。</p><ol><li>初始化第一个弱学习器$F_{0}(x)$：<script type="math/tex; mode=display">F_{0}(x)=\mathop {argmin}_{c} \sum_{i=1}^N L(y_{i}, c)</script></li><li><p>建立$M$棵分类回归树：$m=1, 2, \ldots, M$</p><p>a. 对于$i=1, 2, \ldots, N$，计算第$m$棵树对应的响应值（损失函数的负梯度，即伪残差）</p><script type="math/tex; mode=display">r_{m, i}=-\left[\frac{\partial L(y_{i}, F(x_{i}))}{\partial F(x)}\right]_{F(x)=F_{m-1}(x)}</script><p>b. 对于$i=1, 2, \ldots, N$，利用CART回归树（一般选择）拟合数据$(x_{i}, r_{m, i})$，得到第$m$棵回归树，对应的叶子节点区域为$R_{m,j}$，其中$j=1, 2, \ldots, J_{m}$，同时$J_{m}$是第$m$棵回归树叶子节点的个数。</p><p>c. 对于$J_{m}$个叶子节点区域$j=1, 2, \ldots, J_{m}$，计算最佳拟合值：</p><script type="math/tex; mode=display">c_{m, j}=\mathop {argmin}_{c} \sum_{x_{i} \in R_{m, j}} L(y_{i}, F_{m-1}(x_{i}) + c)</script><p>d. 更新强学习器$F_{m}(x)$:</p><script type="math/tex; mode=display">F_{m}(x)=F_{m-1}(x) + \sum_{j=1}^{J_{m}} c_{m, j}I(x \in R_{m, j})</script></li><li><p>强学习器$F_{M}(x)$：</p><script type="math/tex; mode=display">F_{M}(x)=F_{0}(x)+\sum_{m=1}^M \sum_{j=1}^{J_{m}} c_{m, j}I(x \in R_{m, j})</script></li></ol><h3 id="2-GBDT基本介绍"><a href="#2-GBDT基本介绍" class="headerlink" title="2. GBDT基本介绍"></a>2. GBDT基本介绍</h3><p>GBDT = Regression Decistion Tree (DT)  + Gradient Boosting (GB) + Shrinkage</p><h5 id="2-1-DT：回归树"><a href="#2-1-DT：回归树" class="headerlink" title="2.1 DT：回归树"></a>2.1 DT：回归树</h5><p>决策树分为两类，回归树和分类树，GBDT核心是可以累加所有树的结果作为最终结果，因此GBDT树属于回归树 。回归树中最好分割点的衡量标准不再是最大熵（分类树的衡量标准），而是最小均方差。</p><h5 id="2-2-GB：梯度迭代"><a href="#2-2-GB：梯度迭代" class="headerlink" title="2.2 GB：梯度迭代"></a>2.2 GB：梯度迭代</h5><p>Gradient Boosting，Boosting迭代，通过迭代多棵树来共同决策，GBDT中每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。</p><p>Boosting最大的优势在于，每一步的残差计算其实增大了分错instance的权重，使得后面的树能越来越专注于分错的instance。</p><h5 id="2-3-Shrinkage：缩减"><a href="#2-3-Shrinkage：缩减" class="headerlink" title="2.3 Shrinkage：缩减"></a>2.3 Shrinkage：缩减</h5><p>Shrinkage思想认为，每次走一小步逐渐逼近结果，要比每次一大步很快的比逼近结果更容易避免过拟合。即它不完全信任每一颗残差树，累加的时候只累加一小部分。</p><p>没用Shrinkage：其中$y_{i}$表示第$i$棵树上$y$的预测值，$y_{1-i}$表示前$i$棵树上$y$的综合预测值，</p><p>$y_{i + 1} = 残差(y_{1} \sim y_{i})$，其中：$残差(y_{1} \sim y_{i})=y_{真实值}-y_{1 \sim i}$，$y_{1  \sim  i}=SUM(y_{1}, \ldots, y_{i})$</p><p>Shrinkage中：$y_{1 \sim i}=y(1 \sim i-1)+step * y_{i}$</p><p>即Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分$（step*残差）$逐步逼近目标，step一般都比较小，如$0.01\sim 0.001$（注意该step非gradient的step） 。本质上，Shrinkage为每棵树设置一个$weight$，累加时要乘以这个$weight$， <font color="red">但和Gradient没有关系</font> 。Shrinkage实践证明可以减少过拟合。</p><h5 id="2-4-GBDT适用范围"><a href="#2-4-GBDT适用范围" class="headerlink" title="2.4 GBDT适用范围"></a>2.4 GBDT适用范围</h5><p>适用所有回归问题（线性/非线性），也可用于二分类问题（设定阈值，大于阈值为正例，反之为负例），排序</p><h5 id="2-5-关于RankNet"><a href="#2-5-关于RankNet" class="headerlink" title="2.5 关于RankNet"></a>2.5 关于RankNet</h5><p>搜索引擎排序，RankNet基本是用来定义cost function的，</p><h3 id="3-GBDT在Sklearn中的实现"><a href="#3-GBDT在Sklearn中的实现" class="headerlink" title="3. GBDT在Sklearn中的实现"></a>3. GBDT在Sklearn中的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line">gbdt = GradientBoostingRegressor(loss=<span class="string">'ls'</span>, learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">5</span>, subsample=<span class="number">1</span></span><br><span class="line">                                 , min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, max_depth=<span class="number">3</span></span><br><span class="line">                                 , init=<span class="literal">None</span>, random_state=<span class="literal">None</span>, max_features=<span class="literal">None</span></span><br><span class="line">                                 , alpha=<span class="number">0.9</span>, verbose=<span class="number">0</span>, max_leaf_nodes=<span class="literal">None</span></span><br><span class="line">                                 , warm_start=<span class="literal">False</span></span><br><span class="line">                                 )</span><br></pre></td></tr></table></figure><h3 id="4-GBDT回归任务中常见的损失函数"><a href="#4-GBDT回归任务中常见的损失函数" class="headerlink" title="4. GBDT回归任务中常见的损失函数"></a>4. GBDT回归任务中常见的损失函数</h3><p>对于GBDT回归模型，sklearn中实现了四种损失函数，有均方差’ls’, 绝对损失’lad’, Huber损失’huber’和分位数损失’quantile’。默认是均方差’ls’。一般来说，如果数据的噪音点不多，用默认的均方差’ls’比较好。如果是噪音点较多，则推荐用抗噪音的损失函数’huber’。而如果我们需要对训练集进行分段预测的时候，则采用’quantile’。 </p><ol><li><p><strong>均方差</strong>：</p><script type="math/tex; mode=display">L(y, f(x)) = (y-f(x))^2</script><p>对应的负梯度误差：$y_{i}-f(x_{i})$</p></li><li><p><strong>绝对损失</strong>：</p><script type="math/tex; mode=display">L(y, f(x))=|y-f(x)|</script><p>对应负梯度误差：$sign(y_{i}-f(x_{i}))$</p></li><li><p><strong>Huber损失</strong>: 均方差和绝对损失的折衷， 对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。 </p><script type="math/tex; mode=display">L(y, f(x))=\begin{cases} \frac{1}{2} (y-f(x))^2, & |y - f(x)|\le \delta \\ \delta (|y-f(x)|- \frac {\delta}{2}), & |y - f(x)|\gt \delta \end{cases}</script><p>对应的负梯度误差为：</p><script type="math/tex; mode=display">r(y_{i}, f(x_{i}))=\begin{cases} y_{i}-f(x_{i}), & |y_{i} - f(x_{i})|\le \delta \\ \delta * sign(y_{i}-f(x_{i})), & |y_{i} - f(x_{i})|\gt \delta \end{cases}</script></li><li><p><strong>分位数损失</strong>： 对应的是分位数回归的损失函数 </p><script type="math/tex; mode=display">L(y, f(x))=\sum_{y \geq f(x)} \theta|y-f(x)|+\sum_{y<f(x)}(1-\theta)|y-f(x)|</script><p>其中 $ \theta $为分位数，需要指定，对应的负梯度误差为：</p><script type="math/tex; mode=display">r\left(y_{i}, f\left(x_{i}\right)\right)=\left\{\begin{array}{rl}{\theta} & {y_{i} \geq f\left(x_{i}\right)} \\ {\theta-1} & {y_{i}<f\left(x_{i}\right)}\end{array}\right.</script></li></ol><h3 id="5-GBDT的正则化：防止过拟合"><a href="#5-GBDT的正则化：防止过拟合" class="headerlink" title="5. GBDT的正则化：防止过拟合"></a>5. GBDT的正则化：防止过拟合</h3><ol><li><p><strong>Shrinkage</strong>： 在每次对残差估计进行迭代时，不直接加上当前步所拟合的残差，而是乘以一个系数$\alpha$ 学习率， 对梯度提升的步长进行调整，可以影响设置的回归树个数 。</p><p> 加上了正则化项 ：</p><script type="math/tex; mode=display">F_{m}(x)=F_{m-1}(x)+h_{m}(x)  \Rightarrow F_{m}(x)=F_{m-1}(x)+\alpha h_{m}(x)</script><p>$\aloha$的取值范围为$(0, 1]$，较小的$\alpha$ 意味着需要更多的弱学习器的迭代次数 。通常用学习率和迭代最大次数一起来决定算法的拟合效果。  经验上，推荐小一点的<code>learning_rate​</code>会对测试误差(<code>test error</code>)更好。在实际调参中推荐将<code>learning_rate</code>设置为一个小的常数（<code>e.g. learning_rate &lt;= 0.1</code>），并通过<code>early stopping</code>机制来选<code>n_estimators</code>（弱学习器个数）。 </p></li><li><p><strong>Subsample</strong> ： 通过子采样比例（subsample），取值为 (0,1] 。 这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但会增加样本拟合的偏差，因此取值不能太低。推荐在 [0.5, 0.8]之间。 </p><p>使用了子采样的GBDT也称作随机梯度提升树 (Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做Boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。 </p></li><li><p><strong>对于弱学习器即CART回归树进行正则化剪枝</strong> </p></li><li><p><strong>Early</strong> <strong>Stopping</strong> ： 具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，控制迭代的轮数（树的个数）。在sklearn的GBDT中可以设置参数n_iter_no_change实现early stopping。 </p></li><li><p><strong>Dropout</strong> ： Dropout是deep learning里很常用的正则化技巧 ， 具体的做法是：每次新加一棵树，这棵树要拟合的并不是之前全部树ensemble后的残差，而是随机抽取的一些树ensemble；同时新加的树结果要规范化一下。 可参考： AISTATS2015有篇文章《DART: Dropouts meet Multiple Additive Regression Trees》 </p></li></ol><h3 id="6-有关GBDT常见面试问题"><a href="#6-有关GBDT常见面试问题" class="headerlink" title="6. 有关GBDT常见面试问题"></a>6. 有关GBDT常见面试问题</h3><h5 id="6-1-GBDT与AdaBoost的区别与联系？"><a href="#6-1-GBDT与AdaBoost的区别与联系？" class="headerlink" title="6.1 GBDT与AdaBoost的区别与联系？"></a>6.1 GBDT与AdaBoost的区别与联系？</h5><p>AdaBoost和GBDT都是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过调整错分数据点的权重来改进模型，GBDT是通过计算负梯度来改进模型。因此，相比AdaBoost, GBDT可以使用更多种类的目标函数，而当目标函数是均方误差时，计算损失函数的负梯度值在当前模型的值即为残差。 </p><h5 id="6-2-GBDT与随机森林（Random-Forest，RF）的区别与联系？"><a href="#6-2-GBDT与随机森林（Random-Forest，RF）的区别与联系？" class="headerlink" title="6.2 GBDT与随机森林（Random Forest，RF）的区别与联系？"></a>6.2 GBDT与随机森林（Random Forest，RF）的区别与联系？</h5><p><strong>相同点：</strong>都是由多棵树组成，最终的结果都是由多棵树一起决定。</p><p><strong>不同点：</strong></p><ol><li>集成的方式：随机森林属于Bagging思想，而GBDT是Boosting思想。</li><li>偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差。</li><li>训练样本方式：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本。</li><li>并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)。</li><li>最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合。</li><li>数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感。</li><li>泛化能力：RF不易过拟合，而GBDT容易过拟合。</li></ol><h5 id="6-3-我们知道残差-真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？"><a href="#6-3-我们知道残差-真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？" class="headerlink" title="6.3 我们知道残差=真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？"></a>6.3 我们知道残差=真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？</h5><ol><li><p>在GBDT中，无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损失函数是均方损失时，负梯度刚好是残差，残差只是特例。</p></li><li><p>GBDT的求解过程就是梯度下降在函数空间中的优化过程。在函数空间中优化，每次得到增量函数，这个函数就是GBDT中一个个决策树，负梯度会拟合这个函数。要得到最终的GBDT模型，只需要把初始值或者初始的函数加上每次的增量即可。参考： <a href="https://mp.weixin.qq.com/s?__biz=MzI5NDMzMjY1MA==&amp;mid=2247485110&amp;idx=1&amp;sn=86bcdb38f51fc5f82236b35c349ada4b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">梯度提升（Gradient Boosting）算法，地址：</a><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDMzMjY1MA==&amp;mid=2247485110&amp;idx=1&amp;sn=86bcdb38f51fc5f82236b35c349ada4b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Ods1PHhYyjkRA8bS16OfCg</a>  </p></li></ol><h5 id="6-4-GBDT训练过程"><a href="#6-4-GBDT训练过程" class="headerlink" title="6.4 GBDT训练过程"></a>6.4 GBDT训练过程</h5><img src="/2019/11/04/Kaggle比赛/1.png"><p> GBDT通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度。</p><h5 id="6-5-GBDT如何选择特征？"><a href="#6-5-GBDT如何选择特征？" class="headerlink" title="6.5 GBDT如何选择特征？"></a>6.5 GBDT如何选择特征？</h5><p>GBDT选择特征的细节其实是想问CART Tree生成的过程。这里有一个前提，GBDT的弱分类器默认选择的是CART TREE。其实也可以选择其他弱分类器的，选择的前提是低方差和高偏差。框架服从boosting 框架即可。参考： 李航第五章中CART TREE 算法  </p><h5 id="6-6-GBDT如何构建特征"><a href="#6-6-GBDT如何构建特征" class="headerlink" title="6.6 GBDT如何构建特征 ?"></a>6.6 GBDT如何构建特征 ?</h5><p>其实说GBDT能够构建特征并非很准确，GBDT本身是不能产生特征的，但是可以利用GBDT去产生特征的组合。在CTR预估中，工业界一般会采用<a href="http://www.cnblogs.com/ModifyRong/p/7739955.html#3825035" target="_blank" rel="noopener">逻辑回归</a>去进行处理，逻辑回归本身是适合处理线性可分的数据，如果想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。</p><p>Facebook 在2014年发表的一篇论文利用GBDT去产生有效的特征组合，以便用于逻辑回归的训练，提升模型最终的效果。</p><img src="/2019/11/04/Kaggle比赛/2.png"><p>如图所示，我们 使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为 0.</p><p>于是对于该样本，我们可以得到一个向量[0,1,0,1,0] 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。</p><h5 id="6-7-GBDT-如何用于分类-？"><a href="#6-7-GBDT-如何用于分类-？" class="headerlink" title="6.7 GBDT 如何用于分类 ？"></a>6.7 GBDT 如何用于分类 ？</h5><p>首先明确一点，GBDT无论用于分类还是回归一直都是使用的CART回归树。不会因为任务是分类任务就选用分类树，这里面的核心是因为GBDT每轮的训练是在上一轮的训练的残差基础之上进行训练的。这里的残差就是当前模型的负梯度值 。这个要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的。残差相减是有意义的。</p><p>如果选用的弱分类器是分类树，类别相减是没有意义的。上一轮输出的是样本 x 属于 A类，本一轮训练输出的是样本 x 属于 B类。 A 和 B 很多时候甚至都没有比较的意义，A 类- B类是没有意义的。</p><ol><li><p>在训练的时候，是针对样本$x$每个可能的类都训练一个分类回归树。举例说明，目前样本有三类，也就是$K = 3$。样本$x$属于第二类。那么针对该样本$x$类结果，可以用一个 三维向量$ [0,1,0] $来表示。$0$表示样本不属于该类，$1$表示样本属于该类。</p></li><li><p>针对样本有三类的情况，实质上是在每轮的训练的时候是同时训练三颗树。第一颗树针对样本$x$的第一类，输入为$(x,0)$。第二颗树输入针对样本$x$的第二类，输入为$(x,1)$。第三颗树针对样本$x$的第三类，输入为$(x，0)$。</p></li><li><p>每颗树的训练过程其实就是CATR TREE 的生成过程。参照之前的生成树的程序即可以就解出三颗树，以及三颗树对$x$类别的预测值$f_{1}(x),f_{2}(x),f_{3}(x)$。那么在此类训练中，仿照多分类的逻辑回归 ，使用softmax来产生概率，则属于类别 1 的概率：</p><script type="math/tex; mode=display">p_{1}=\exp \left(f_{1}(x)\right) / \sum_{k=1}^{3} \exp \left(f_{k}(x)\right)</script><p>针对类别1求出残差$y_{11}(x)=0−p_{1}(x)$;类别2求出残差$y_{22}(x)=1−p_{2}(x)$;类别3求出残差$y_{33}(x)=0−p_{3}(x)$。</p></li><li><p>开始第二轮训练，针对第一类输入为$(x,y_{11}(x))$, 针对第二类输入为$(x,y_{22}(x))$, 针对 第三类输入为 $(x,y_{33}(x))$，继续训练出三颗树。一直迭代M轮。每轮构建3颗树。 </p></li><li><p>当$K =3$。三个式子：</p><script type="math/tex; mode=display">F_{1 M}(x)=\sum_{m=1}^{M} \hat{C_{1 m}} I\left(x \epsilon R_{1 m}\right)</script><script type="math/tex; mode=display">F_{2 M}(x)=\sum_{m=1}^{M} \hat{C_{2 m}} I\left(x \epsilon R_{2 m}\right)</script><script type="math/tex; mode=display">F_{3 M}(x)=\sum_{m=1}^{M} \hat{C_{3 m}} I\left(x \epsilon R_{3 m}\right)</script></li><li><p>训练完毕以后，新来一个样本$x_{1}$ ，需要预测该样本的类别的时候，可以有这三个式子产生三个值，$f_{1}(x),f_{2}(x),f_{3}(x)$。样本属于 某个类别$c$的概率为 :</p><script type="math/tex; mode=display">p_{c}=\exp \left(f_{c}(x)\right) / \sum_{k=1}^{3} \exp \left(f_{k}(x)\right)</script></li></ol><h5 id="6-8-比较LR和GBDT，说说什么情景下GBDT不如LR"><a href="#6-8-比较LR和GBDT，说说什么情景下GBDT不如LR" class="headerlink" title="6.8 比较LR和GBDT，说说什么情景下GBDT不如LR"></a>6.8 比较LR和GBDT，说说什么情景下GBDT不如LR</h5><p>先说说LR和GBDT的区别：</p><ul><li>LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程</li><li>GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；</li></ul><p>当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下：</p><p>先看一个例子：</p><blockquote><p>假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只有10个正样本1，而这些样本的特征 $f_{1}$的值为全为1，而其余9990条样本的$f_{1}$特征都为0(在高维稀疏的情况下这种情况很常见)。</p><p>都知道在这种情况下，树模型很容易优化出一个使用$f_{1}$特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征$f_{1}$只是刚好偶然间跟$y$拟合到了这个规律，这也是我们常说的过拟合。</p></blockquote><p>那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：$y = W_{1}<em>f_{1} + W_{i}</em>f_{i}+….$，其中 $W_{1}$特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？</p><p>仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 $W_{1}$一旦过大，惩罚就会很大，进一步压缩 $W_{1}$的值，使其不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。</p><p>这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：<strong>带正则化的线性模型比较不容易对稀疏特征过拟合。</strong></p><ol><li><h2 id="二、XGBoost"><a href="#二、XGBoost" class="headerlink" title="二、XGBoost"></a>二、XGBoost</h2></li></ol><p>参考：</p><p> <a href="https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ" target="_blank" rel="noopener">终于有人把XGBoost 和 LightGBM 讲明白了，项目中最主流的集成算法！</a></p><p><a href="https://mp.weixin.qq.com/s/wLE9yb7MtE208IVLFlZNkw" target="_blank" rel="noopener">XGBoost超详细推导，终于有人讲明白了！</a></p><p>XGBoost是大规模并行boosting tree的工具，  Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是<font color="yellow">目标函数</font>的定义 。 目前已有的 GBDT 工具基本都是基于预排序的方法（pre-sorted）的决策树算法(如 xgboost)。 </p><h3 id="1-数学推导-1"><a href="#1-数学推导-1" class="headerlink" title="1 数学推导"></a>1 数学推导</h3><img src="/2019/11/04/Kaggle比赛/6.png"><h5 id="1-1-目标函数"><a href="#1-1-目标函数" class="headerlink" title="1.1 目标函数"></a>1.1 目标函数</h5><p>  XGBoost 是由 $k$ 个基模型组成的一个加法运算式，其中$f_{k}$为第$k$个基模型，$\hat{y}_{i}$为第$i$i个样本的预测值：</p><script type="math/tex; mode=display">\hat{y}_{i}=\sum_{t=1}^{k} f_{t}\left(x_{i}\right)</script><p>损失函数可由预测值$\hat{y}_{i}$与真实值$y_{i}$进行表示，其中$n$为样本数量：</p><script type="math/tex; mode=display">L=\sum_{i=1}^n l(y_{i}, \hat{y_{i}})</script><p>常见的损失函数：</p><ol><li>平方损失函数：$l(y_{i}, \hat{y}_{i}) = (y_{i}-\hat{y}_{i})^2$</li><li>逻辑回归损失函数：$l(y_{i}, \hat{y}_{i})=y_{i}ln(1+e^{-\hat{y}_{i}})+(1-y_{i})ln(1+e^{\hat{y}_{i}})$</li></ol><font color="yellow"> 模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，方差小需要简单的模型 </font> 目标函数由模型的损失函数 $L$ 与抑制模型复杂度的正则项 $ \Omega $ 组成 ：$$O b j=\sum_{i=1}^{n} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{t=1}^{k} \Omega\left(f_{t}\right)$$由于Boosting模型是前向加法， 以第 $t$ 步的模型为例，模型对第 $i$个样本 $x_{i}$的预测为： $$\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)$$其中 $\hat{y}_{i}^{t-1}$ 由第 $t-1$ 步的模型（第 $t-1$ 棵树）给出的预测值，$f_{t}(x_{i})$是新加入模型的预测值，此时的目标函数可写成：$$\begin{aligned} O b j^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+ \Omega(f_{t}) +\sum_{i=1}^{t-1} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+ \Omega(f_{t}) + const \end{aligned}$$将正则化项进行拆分，由于前 $t-1$ 棵树的结构已经确定，因此，前 $t-1$ 棵树的复杂度之和可以用一个常量 $const$ 表示。 注意到上式中只有一个变量，即第 $t$ 棵树模型的$f_{t}(x_{i})$，最优化目标函数，就相当于求解  $f_{t}(x_{i})$。##### 1.2 泰勒公式将一个在 $x = x0$ 处具有 $n$ 阶导数的函数 $f(x)$ 利用关于 $(x-x_{0})$ 的 $n$ 次多项式来逼近。 若函数 $f(x)$ 在包含 $x_{0}$ 的某个闭区间 $[a, b]$ 上具有 $n$ 阶导数，且在开区间 $(a,b)$ 上具有 $n+1$ 阶导数，则对闭区间  $[a, b]$ 上任意一点 $x$ 有：$$f(x)=\sum_{i=0}^{n} \frac{f^{(i)}\left(x_{0}\right)}{i !}\left(x-x_{0}\right)^{i}+R_{n}(x)$$其中的多项式称为函数在 $x_{0}$处的泰勒展开式，$R_{n}(x)$ 是泰勒公式的余项且是 $(x-x_{0})^n$ 的高阶无穷小。函数 $f(x+ \Delta x)$ 在点 $x$ 处进行泰勒公式的二阶展开，形式如下： $$f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}$$将损失函数 $l$ 视为上式的 $f(x)$ ，将 $\hat{y}_{i}^{t-1}$ 视为 $x$，将 $f_{t}(x_{i})$ 视为 $\Delta x$，定义损失函数 $l$ 关于 $\hat{y}_{i}^{t-1}$ 的一阶偏导数和二阶偏导数：$$g_{i}= \frac {\partial l(y_{i}, \hat{y}^{t-1})}{\partial {\hat{y}^{t-1}}}\\h_{i}=\frac {\partial^{2} l(y_{i}, \hat{y}^{t-1})}{\partial {\hat{y}^{t-1}}}$$损失函数可以表示成如下形式：$$l \left (y_{i}, \hat{y}_{i}^{t-1}+f_{t}(x_{i}) \right)=l(y_{i}, \hat{y}_{i}^{t-1})+g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})$$由此可以得到目标函数的近似表达式：$$Obj^{(t)} \simeq \sum_{i=1}^{n}\left[ l(y_{i}, \hat{y}_{i}^{t-1})+g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})\right]+\Omega (f_{t})+const$$由于在第 $t$ 步时 $\hat{y}_{i}^{t-1}$ 已知，所以 $l(y_{i}, \hat{y}_{i}^{t-1})$ 和$const$是一个常量，因此优化目标可写成：$$Obj^{(t)} \simeq \sum_{i=1}^{n}\left[g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})\right]+\Omega (f_{t})$$ 只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 $\hat{y}_{i}^{t-1}$ 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 $f(x)$ ，最后根据加法模型得到一个整体模型。 ##### 1.3 基于决策树的目标函数XGBoost的基模型不仅支持决策树，还支持线性模型。以下主要介绍基于决策树的目标函数。定义一棵树：1. 叶子结点的权重向量 $\omega$2. 实例  -->  叶子结点的映射关系 $q$决策树的表达式：$$f_{t}(x)=w_{q(x)},     w \in \boldsymbol R^{T},q:\boldsymbol R^{d} \to{1, 2, \cdots ,T}$$其中 $x$ 为某一样本, $w$ 是长度为 $T$ 的一维向量，$T$ 代表树的叶子结点的个数，代表树 $q$ 各叶子结点的权重，$q$ 代表了一棵树的结构，将输入 $x_{i}\in \boldsymbol R^{d}$ 映射到某叶子结点， $q(x)$ 代表了该样本在哪个叶子结点上，$w_{q}$ 表示叶子结点取值 $\omega$，$\omega_{q(x)}$ 代表了每个样本的取值 $w$。决策树的复杂度可由叶子数 $T$ 组成，叶子结点越少模型越简单，此外叶子结点也不应该含有过高的权重 $w$ （类比 LR 的每个变量的权重），由此可定义一棵树的复杂度 $\Omega$（即目标函数的正则化项）：$$\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}$$即决策树模型的复杂度由叶子结点数量 + 叶子结点权重向量的$L2$范数共同决定。 将属于第 $j$ 个叶子结点的所有样本 $x_{i}$ , 划入到一个叶子结点样本集中，数学表示如下： $$I_{j}=\left\{i | q\left(x_{i}\right)=j\right\}$$目标函数可写成如下式子：$$\begin{aligned} O b j^{(t)} & \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\ &=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\ &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T \end{aligned} 将训练样本按叶子结点进行分组$$ 第二步到第三步：第二步是遍历所有的样本后求每个样本的损失函数，但样本最终会落在叶子节点上，所以我们也可以遍历叶子节点，然后获取叶子节点上的样本集合，最后再求损失函数。即之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了 $\sum_{i \in I_{j}} g_{i}$ 和 $\sum_{i \in I_{j}} h_{i}$ 这两项， $w_{j}$为第 $j$ 个叶子节点取值。 进一步简化表达式，定义：$$G_{j} = \sum_{i \in I_{j}} g_{i}, \ H_{j} = \sum_{i \in I_{j}} h_{i}\\$$- $G_{j}$ ：叶子结点 $j$ 所包含样本的一阶偏导数累加之和，是一个常量；- $H_{j}$ ：叶子结点 $j$ 所包含样本的二阶偏导数累加之和，是一个常量；最终的目标函数：$$O b j^{(t)}=\sum_{j=1}^{T}\left[G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2}\right]+\gamma T$$此时表达式中只有第 $t$ 棵树的叶子结点权重$w_{j}$是未知量，一元二次函数变量一阶导为$0$的点为最值点（$y=ax^{2}+bx+c$，如果$a>0$，为最小值，若$a<0$为最大值）， 0 分析一下目标函数$obj$，可以发现，各个叶子结点的目标子式是相互独立的，也就是说，当每个叶子结点的子式都达到最值点时，整个目标函数式$obj$才达到最值点。 将目标函数对 $w_{j}$ 求一阶导，并令其等于 $0$ ，则可以求得叶子结点 $j$ 对应的权值： $$ w_{j}^{*}="-\frac{G_{j}}{H_{j}+\lambda}" 目标函数可以化简为： o b j="-\frac{1}{2}" \sum_{j="1}^{T}" \frac{g_{j}^{2}}{h_{j}+\lambda}+\gamma t ##### 1.4 最优切分点划分算法 在决策树的生长过程中，一个非常关键的问题是如何找到叶子节点的最优切分点，xgboost 支持两种分裂节点的方法——贪心算法和近似算法。 ###### 1.4.1 贪心算法 全局扫描法 从深度为 的树开始： 1. 对树中的每个叶子结点尝试进行分裂； 2. 每次分裂后，原来的一个叶子结点继续分裂为左右两个子叶子结点，原叶子结点中的样本集将根据该结点的判断规则分散到左右两个叶子结点中； 3. 新分裂一个结点后，我们需要检测这次分裂是否会给损失函数带来增益 假设在某一节点完成特征分裂，则分裂前的目标函数可以写为： j_{l+r}="-\frac{1}{2}" \frac{\left(g_{l}+g_{r}\right)^{2}}{h_{l}+h_{r}+\lambda}+\gamma 分裂后的目标函数为： j_{l}+o j_{r}="-\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}\right]+2" \gamma 目标函数来说，每个特征分裂后的收益（ 该特征收益也可作为特征重要性输出的重要依据 ）： \begin{aligned} \text {gain} &="O" j_{l+r}-\left(o j_{r}\right) \\ \frac{\left(g_{l}+g_{r}\right)^{2}}{h_{l}+h_{r}+\lambda}+\gamma\right]-\left[-\frac{1}{2}\left(\frac{g_{l}^{2}}{h_{l}+\lambda}+\frac{g_{r}^{2}}{h_{r}+\lambda}\right)+2 \gamma\right] \end{aligned} 增益$gain>0$，即分裂为两个叶子节点后，目标函数下降了，会考虑此次分裂的结果。但是，在一个结点分裂时，可能有很多个分裂点，每个分裂点都会产生一个增益，如何才能寻找到最优的分裂点呢？ 寻找最佳分割点的大致步骤如下： 1. 对每个叶节点枚举所有的可用特征（遍历每个结点的所有特征 ）；2. 针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集4. 回到第 1 步，递归执行到满足特定条件为止###### 1.4.2 近似算法当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。 基于此，XGBoost提出了一系列加快寻找最佳分裂点的方案： - **特征预排序+缓存：**XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构， 并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储， 后面的迭代中会重复地使用这个结构，使计算量大大减小。  - 特征预排序：每一个块结构包括一个或多个已经排序好的特征；缺失特征值将不进行排序；每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；    <img src="/2019/11/04/Kaggle比赛/5.png">  - 缓存访问优化算法：块结构的设计可以减少节点分裂时的计算量，但特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续，这样会造成缓存命中率低，从而影响到算法的效率。为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。此外适当调整块大小，也可以有助于缓存优化。- **"核外"快计算：**当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行。  此外，XGBoost 还用了两种方法来降低硬盘读写的开销：  - 块压缩：对 Block 进行按列压缩，并在读取时进行解压；  - 块拆分：将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。- **分位点近似法：**对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。- **并行查找：**由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。在提出候选切分点时有两种策略：- Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；- Local：每次分裂前将重新提出候选切分点。直观上来看，Local 策略需要更多的计算步骤，而 Global 策略因为节点没有划分所以需要更多的候选点。事实上， XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值 $h_{i}$ 作为样本的权重进行划分：1. 按照样本个数进行分位，三分位为例：   <img src="/2019/11/04/Kaggle比赛/3.png">2.  以二阶导数值 $h_{i}$ 作为样本的权重进行划分:   <img src="/2019/11/04/Kaggle比赛/4.png">为什么要用 $h_{i}$ 进行样本加权：模型的目标函数为： $$Obj^{(t)} \simeq \sum_{i=1}^{n}\left[g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})\right]+\Omega (f_{t})$$ 稍作整理，便可以看出 $h_{i}$ 有对 loss 加权的作用：$$\begin{aligned} O b j^{(t)} & \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right) \color{red} + \frac{1}{2} \frac{g_{i}^{2}}{h_{i}}\right]+\Omega\left(f_{t}\right)+const \\ &=\sum_{i=1}^{n} \color{red}\frac{1}{2} h_{i}\left[f_{t}\left(x_{i}\right)-\left(-\frac{g_{i}}{h_{i}}\right)\right]^{2}+\Omega\left(f_{t}\right)+const \end{aligned}$$ 其中 $h_{i}$ 与 $const$ 皆为常数。可以看到 $h_{i}$ 就是平方损失函数中样本的权重。 <font color="red">[不是很懂]</font><h5 id="1-5-树停止生长"><a href="#1-5-树停止生长" class="headerlink" title="1.5 树停止生长"></a>1.5 树停止生长</h5><ol><li>当新引入的一次分裂所带来的增益 $Gain&lt;0$ 时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 </li><li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数 max_depth。 </li><li><p>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数：最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施。 </p><h3 id="2-稀疏感知算法"><a href="#2-稀疏感知算法" class="headerlink" title="2. 稀疏感知算法"></a>2. 稀疏感知算法</h3></li></ol><p>XGBoost 在决策树应对数据缺失时给出的分裂策略，XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。</p><p>在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而特征值缺失的样本无需遍历只需直接分配到左右节点，故算法所需遍历的样本量减少。</p><h3 id="3-算法优缺点"><a href="#3-算法优缺点" class="headerlink" title="3. 算法优缺点"></a>3. 算法优缺点</h3><h5 id="3-1-优点"><a href="#3-1-优点" class="headerlink" title="3.1 优点"></a>3.1 优点</h5><ol><li>精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost  对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</li><li>灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li><li>正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</li><li>Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li><li>列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</li><li>缺失值处理：XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；</li><li>可以并行化操作：块结构可以很好的支持并行计算。</li></ol><h5 id="3-2-缺点"><a href="#3-2-缺点" class="headerlink" title="3.2 缺点"></a>3.2 缺点</h5><ol><li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li><li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。</li></ol><h3 id="4-高频面试题"><a href="#4-高频面试题" class="headerlink" title="4. 高频面试题"></a>4. 高频面试题</h3><h5 id="4-1-XGB与GBDT、随机森林等模型相比，有什么优缺点"><a href="#4-1-XGB与GBDT、随机森林等模型相比，有什么优缺点" class="headerlink" title="4. 1 XGB与GBDT、随机森林等模型相比，有什么优缺点?"></a>4. 1 XGB与GBDT、随机森林等模型相比，有什么优缺点?</h5><p>   与GBDT模型相比：</p><ul><li><strong>基分类器</strong>：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。</li><li><strong>导数信息</strong>：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。</li><li><strong>正则项</strong>：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。</li><li><strong>列抽样</strong>：XGBoost支持列采样，与随机森林类似，用于防止过拟合。</li><li><strong>缺失值处理</strong>：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。</li><li><strong>并行化</strong>：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</li></ul><h5 id="4-2-XGB为什么可以并行训练？"><a href="#4-2-XGB为什么可以并行训练？" class="headerlink" title="4.2 XGB为什么可以并行训练？"></a>4.2 XGB为什么可以并行训练？</h5><ul><li>XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。</li><li>XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。</li></ul><h5 id="4-3-XGB用二阶泰勒展开的优势在哪？"><a href="#4-3-XGB用二阶泰勒展开的优势在哪？" class="headerlink" title="4.3 XGB用二阶泰勒展开的优势在哪？"></a>4.3 XGB用二阶泰勒展开的优势在哪？</h5><ul><li><strong>精准性</strong>：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数</li><li><strong>可扩展性</strong>：损失函数支持自定义，只需要新的损失函数二阶可导。</li></ul><h5 id="4-4-XGB为了防止过拟合，进行了哪些设计？"><a href="#4-4-XGB为了防止过拟合，进行了哪些设计？" class="headerlink" title="4.4 XGB为了防止过拟合，进行了哪些设计？"></a>4.4 XGB为了防止过拟合，进行了哪些设计？</h5><p>XGBoost在设计时，为了防止过拟合做了很多优化，具体如下：</p><ul><li><strong>目标函数添加正则项</strong>：叶子节点个数+叶子节点权重的L2正则化</li><li><strong>列抽样</strong>：训练的时候只用一部分特征（不考虑剩余的block块即可）</li><li><strong>子采样</strong>：每轮计算可以不使用全部样本，使算法更加保守</li><li><strong>shrinkage</strong>: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间</li></ul><h5 id="4-5-XGB如何处理缺失值？"><a href="#4-5-XGB如何处理缺失值？" class="headerlink" title="4.5 XGB如何处理缺失值？"></a>4.5 XGB如何处理缺失值？</h5><p>XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：</p><ul><li>在特征$k$上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。</li><li>在逻辑实现上，为了保证完备性，会将该特征值 missing 的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。</li><li>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。</li></ul><h5 id="4-6-为什么XGBoost相比某些模型对缺失值不敏感"><a href="#4-6-为什么XGBoost相比某些模型对缺失值不敏感" class="headerlink" title="4.6 为什么XGBoost相比某些模型对缺失值不敏感"></a>4.6 为什么XGBoost相比某些模型对缺失值不敏感</h5><p>对存在缺失值的特征，一般的解决方法是：</p><ul><li>离散型变量：用出现次数最多的特征值填充；</li><li>连续型变量：用中位数或均值填充；</li></ul><p>一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。而树模型对缺失值的敏感度低，大部分时候可以在数据缺失时使用。原因就是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。</p><p>XGBoost对缺失数据有特定的处理方法，因此，对于有缺失值的数据在经过缺失处理后：</p><ul><li>当数据量很小时，优先用朴素贝叶斯</li><li>数据量适中或者较大，用树模型，优先XGBoost</li><li>数据量较大，也可以用神经网络</li><li>避免使用距离度量相关的模型，如KNN和SVM</li></ul><h5 id="4-7-XGB如何分裂一个结点？如何选择特征？"><a href="#4-7-XGB如何分裂一个结点？如何选择特征？" class="headerlink" title="4.7 XGB如何分裂一个结点？如何选择特征？"></a>4.7 XGB如何分裂一个结点？如何选择特征？</h5><p>XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。</p><p>如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。</p><h5 id="4-8-XGB中一颗树停止生长的条件有哪些？"><a href="#4-8-XGB中一颗树停止生长的条件有哪些？" class="headerlink" title="4.8 XGB中一颗树停止生长的条件有哪些？"></a>4.8 XGB中一颗树停止生长的条件有哪些？</h5><ul><li>当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。</li><li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。</li><li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。</li></ul><h5 id="4-9-XGB叶子结点的权重有什么含义？如何计算？"><a href="#4-9-XGB叶子结点的权重有什么含义？如何计算？" class="headerlink" title="4.9 XGB叶子结点的权重有什么含义？如何计算？"></a>4.9 XGB叶子结点的权重有什么含义？如何计算？</h5><p>XGBoost目标函数最终推导形式如下：</p><script type="math/tex; mode=display">O b j^{(t)}=\sum_{j=1}^{T}\left[G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2}\right]+\gamma T</script><p>利用一元二次函数求最值的知识，当目标函数达到最小值$Obj^{<em>}$时，每个叶子结点的权重为$w_{j}^{</em>}$。</p><p>具体公式如下：</p><script type="math/tex; mode=display">w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda} \quad, \quad O b j=-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T</script><h5 id="4-10-训练一个XGB模型，经历了哪些过程？调参步骤是什么？"><a href="#4-10-训练一个XGB模型，经历了哪些过程？调参步骤是什么？" class="headerlink" title="4.10 训练一个XGB模型，经历了哪些过程？调参步骤是什么？"></a>4.10 训练一个XGB模型，经历了哪些过程？调参步骤是什么？</h5><p>首先需要初始化一些基本变量，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- max_depth = 5</span><br><span class="line">- min_child_weight = 1</span><br><span class="line">- gamma = 0</span><br><span class="line">- subsample, colsample_bytree = 0.8</span><br><span class="line">- scale_pos_weight = 1</span><br></pre></td></tr></table></figure><p><strong>(1) 确定learning rate和estimator的数量</strong></p><p>&emsp;&emsp;learning rate可以先用0.1，用cv来寻找最优的estimators</p><p><strong>(2) max_depth和 min_child_weight</strong></p><p>&emsp;&emsp;调整这两个参数是因为，这两个参数对输出结果的影响很大。首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。</p><p>&emsp;&emsp;max_depth，每棵子树的最大深度，check from range(3,10,2)。</p><p>&emsp;&emsp;min_child_weight，子节点的权重阈值，check from range(1,6,2)。</p><p>&emsp;&emsp;如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。</p><p><strong>(3) gamma</strong></p><p>&emsp;&emsp;也称作最小划分损失<code>min_split_loss</code>，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。</p><ul><li>如果大于该阈值，则该叶子节点值得继续划分</li><li>如果小于该阈值，则该叶子节点不值得继续划分</li></ul><p><strong>(4) subsample, colsample_bytree</strong></p><p>&emsp;&emsp;subsample是对训练的采样比例</p><p>&emsp;&emsp;colsample_bytree是对特征的采样比例</p><p>&emsp;&emsp;both check from 0.6 to 0.9</p><p><strong>(5) 正则化参数</strong></p><p>&emsp;&emsp;$alpha$ 是L1正则化系数，try $1e-5, 1e-2, 0.1, 1, 100$</p><p>&emsp;&emsp;$lambda$ 是L2正则化系数</p><p><strong>(6) 降低学习率</strong></p><p>&emsp;&emsp;降低学习率的同时增加树的数量，通常最后设置学习率为$0.01~0.1$</p><h5 id="4-11-XGB如何给特征评分？"><a href="#4-11-XGB如何给特征评分？" class="headerlink" title="4.11 XGB如何给特征评分？"></a>4.11 XGB如何给特征评分？</h5><p> 采用三种方法来评判XGBoost模型中特征的重要程度： </p><ul><li><strong>weight</strong> ：该特征在所有树中被用作分割样本的特征的总次数。</li><li><strong>gain</strong> ：该特征在其出现过的所有树中产生的平均增益。</li><li><strong>cover</strong> ：该特征在其出现过的所有树中的平均覆盖范围。</li></ul><blockquote><p>注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。</p></blockquote><h5 id="4-12-XGBoost如何处理不平衡数据"><a href="#4-12-XGBoost如何处理不平衡数据" class="headerlink" title="4.12 XGBoost如何处理不平衡数据"></a>4.12 XGBoost如何处理不平衡数据</h5><p>对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决：</p><p>第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置<em>scale_pos_weight</em>来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，<em>scale_pos_weight</em>可以取10；</p><p>第二种，如果你在意概率(预测得分的合理性)，不能重新平衡数据集(会破坏数据的真实分布)，应该设置<em>max_delta_step</em>为一个有限数字来帮助收敛（基模型为LR时有效）。</p><p>那么，源码到底是怎么利用<strong>scale_pos_weight</strong>来平衡样本的呢，是调节权重还是过采样呢？请看源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (info.labels[i] == <span class="number">1.0</span>f)  w *= param_.scale_pos_weight</span><br></pre></td></tr></table></figure><p>可以看出，应该是增大了少数样本的权重。</p><p>除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。</p><h5 id="4-13-XGBoost中如何对树进行剪枝"><a href="#4-13-XGBoost中如何对树进行剪枝" class="headerlink" title="4.13 XGBoost中如何对树进行剪枝"></a>4.13 XGBoost中如何对树进行剪枝</h5><ul><li>在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。</li><li>在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。</li><li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。</li><li>XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。</li></ul><h2 id="三、LightGBM"><a href="#三、LightGBM" class="headerlink" title="三、LightGBM"></a>三、LightGBM</h2><p>参考：</p><p> <a href="https://mp.weixin.qq.com/s/fDwpJxSwWXWjV0y0mtKxAQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/fDwpJxSwWXWjV0y0mtKxAQ</a> </p><p> <a href="https://mp.weixin.qq.com/s/441iM0snbfHznewbktUsvQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/441iM0snbfHznewbktUsvQ</a> </p><p> <a href="https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ</a> </p><p>LightGBM  （Light Gradient Boosting Machine） 由微软提出，用于解决GBDT在海量数据中遇到的问题。 其相对 XGBoost 具有训练速度快、内存占用低的特点。 GBDT在工业界通常被用于点击预测，搜索排序等任务。LightGBM是实现GBDT算法的框架，支持高效的并行训练。</p><h3 id="1-LightGBM在相比XGBoost哪些地方进行了优化？"><a href="#1-LightGBM在相比XGBoost哪些地方进行了优化？" class="headerlink" title="1. LightGBM在相比XGBoost哪些地方进行了优化？"></a>1. LightGBM在相比XGBoost哪些地方进行了优化？</h3><ol><li>单边梯度抽样算法；</li><li>直方图算法；</li><li>互斥特征捆绑算法；</li><li>基于最大深度的 Leaf-wise 的垂直生长算法；</li><li>类别特征最优分割；</li><li>特征并行和数据并行；</li><li>缓存优化。</li></ol><h3 id="2-数学原理"><a href="#2-数学原理" class="headerlink" title="2.  数学原理"></a>2.  数学原理</h3><h5 id="2-1-单边梯度抽样算法"><a href="#2-1-单边梯度抽样算法" class="headerlink" title="2.1 单边梯度抽样算法"></a>2.1 单边梯度抽样算法</h5><p>GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法（Gradient-based One-Side Sampling, GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算中只需关注梯度高的样本，极大的减少了计算量。</p><p>GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样，为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。 GOSS 事先基于梯度的绝对值对样本进行排序（无需保存排序后结果），然后拿到前 $a \%$ 的梯度大的样本，和剩下样本的 $b\%$，在计算增益时，通过乘上 $\frac{1-a}{b}$来放大梯度小的样本的权重。一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。 </p><h5 id="2-2-直方图算法"><a href="#2-2-直方图算法" class="headerlink" title="2.2 直方图算法"></a>2.2 直方图算法</h5><h6 id="2-2-1-直方图算法"><a href="#2-2-1-直方图算法" class="headerlink" title="2.2.1 直方图算法"></a>2.2.1 直方图算法</h6><p>直方图算法的基本思想是将连续的特征离散化为 $k$ 个离散特征 （其实又是分桶的思想，而这些桶称为bin，比如[0,0.1)→0, [0.1,0.3)→1） ，同时构造一个宽度为 $k$ 的直方图用于统计信息（含有 $k$ 个 bin）。利用直方图算法无需遍历数据，只需要遍历 $k$ 个 bin 即可找到最佳分裂点。</p><p>我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）：</p><ul><li>内存占用更小：XGBoost 需要用 32 位的浮点数去存储特征值，并用 32 位的整形去存储索引，而 LightGBM 只需要用 8 位去存储直方图，相当于减少了 1/8；</li><li>计算代价更小：计算特征分裂增益时，XGBoost 需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k 次，直接将时间复杂度从 $O(data  <em> feature) $降低到 $O(k  </em> feature)$  ， $data &gt;&gt; k$ 。</li></ul><p>虽然将特征离散化后无法找到精确的分割点，可能会对模型的精度产生一定的影响，但较粗的分割也起到了正则化的效果，一定程度上降低了模型的方差。</p><h6 id="2-2-2-直方图加速"><a href="#2-2-2-直方图加速" class="headerlink" title="2.2.2 直方图加速"></a>2.2.2 直方图加速</h6><p>在构建叶节点的直方图时，还可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。在实际操作过程中，还可以先计算直方图小的叶子节点，然后利用直方图作差来获得直方图大的叶子节点。 </p><img src="/2019/11/04/Kaggle比赛/7.png"><h6 id="2-2-3-稀疏特征优化"><a href="#2-2-3-稀疏特征优化" class="headerlink" title="2.2.3 稀疏特征优化"></a>2.2.3 稀疏特征优化</h6><p> XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。 </p><h5 id="2-3-带深度限制的-Leaf-wise-算法"><a href="#2-3-带深度限制的-Leaf-wise-算法" class="headerlink" title="2.3 带深度限制的 Leaf-wise 算法"></a><strong>2.3 带深度限制的 Leaf-wise 算法</strong></h5><p>在建树的过程中有两种策略：</p><ul><li><em>Level-wise</em>：基于层进行生长，直到达到停止条件；</li><li><em>Leaf-wise</em>：每次分裂增益最大的叶子节点，直到达到停止条件。</li></ul><img src="/2019/11/04/Kaggle比赛/8.png"><p>XGBoost 采用 <em>Level-wise</em> 的增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂，降低了计算量；LightGBM 采用 <em>Leaf-wise</em> 的增长策略减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂。</p><h5 id="2-4-类别特征最优分割"><a href="#2-4-类别特征最优分割" class="headerlink" title="2.4 类别特征最优分割"></a>2.4 类别特征最优分割</h5><p>大部分的机器学习算法都不能直接支持类别特征，一般都会对类别特征进行编码，然后再输入到模型中。常见的处理类别特征的方法为 one-hot 编码，但对于决策树来说并不推荐使用 one-hot 编码：</p><ol><li>会产生样本切分不平衡问题，切分增益会非常小。如，国籍切分后，会产生是否中国，是否美国等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0。这种划分的增益非常小：较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零；</li><li><p>影响决策树学习：决策树依赖的是数据的统计信息，而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上统计信息不准确的，学习效果变差。本质是因为独热码编码之后的特征的表达能力较差的，特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败，最终该特征得到的重要性会比实际值低。</p><p>LightGBM 原生支持类别特征，采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设有某维特征有 $k$ 个类别，则有 $2^{(k-1)} - 1$ 中可能，时间复杂度为 $O(2^k)$ ，LightGBM 实现了 $O(klog_2k)$ 的时间复杂度。 </p></li></ol><img src="/2019/11/04/Kaggle比赛/9.png"><p>上图为左边为基于 one-hot 编码进行分裂，后图为 LightGBM 基于 many-vs-many 进行分裂，在给定深度情况下，后者能学出更好的模型。其基本思想在于每次分组时都会根据训练目标对类别特征进行分类，根据其累积值 $\frac{\sum gradient }{\sum hessian}$ 对直方图进行排序，然后在排序的直方图上找到最佳分割。此外，LightGBM 还加了约束条件正则化，防止过拟合。</p><h3 id="3-XGBoost和LightGBM的区别"><a href="#3-XGBoost和LightGBM的区别" class="headerlink" title="3.  XGBoost和LightGBM的区别"></a>3.  XGBoost和LightGBM的区别</h3><ol><li><p>树生长策略：XGB采用<code>level-wise</code>的分裂策略，LGB采用<code>leaf-wise</code>的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。<em>Leaf-wise</em>是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。</p></li><li><p>分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下：</p><ul><li><p>减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。</p></li><li><p>计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为$𝑂(𝑓𝑒𝑎𝑡𝑢𝑟𝑒<em>𝑑𝑎𝑡𝑎)$。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为$𝑂(𝑓𝑒𝑎𝑡𝑢𝑟𝑒</em>b𝑖𝑛𝑠)$。</p></li><li><p>LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算</p><blockquote><p>但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？</p><p>xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。</p></blockquote></li></ul></li><li><p>支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。</p></li><li><p>缓存命中率：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</p></li><li><p>LightGBM 与 XGboost 的并行策略不同：</p><ul><li><strong>特征并行</strong> ：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。</li><li><strong>数据并行</strong> ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。</li><li><strong>投票并行（LGB）</strong>：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。</li></ul></li></ol><h2 id="四、BigQuery-Geotab-Intersection-Congestion"><a href="#四、BigQuery-Geotab-Intersection-Congestion" class="headerlink" title="四、BigQuery-Geotab Intersection Congestion"></a>四、BigQuery-Geotab Intersection Congestion</h2><p>BigQuery-Geotab交叉路口拥堵：预测主要城市十字路口的等待时间</p><h3 id="1-基本介绍"><a href="#1-基本介绍" class="headerlink" title="1. 基本介绍"></a>1. 基本介绍</h3><h4 id="1-1-概要"><a href="#1-1-概要" class="headerlink" title="1.1 概要"></a>1.1 概要</h4><p>该比赛的数据集包括总计停止的车辆信息和交叉路口等待时间。任务是根据美国四个主要城市（亚特兰大，波士顿，芝加哥和费城）的交叉路口的停车距离和等待时间的总量来预测交通拥堵。</p><p>这项比赛是与BigQuery合作举办的，BigQuery是一个用于操纵，联接和查询大规模表格数据集的数据仓库。 BigQuery还提供BigQuery ML，这是用户创建和运行机器学习模型以通过SQL查询界面生成预测的简便方法。</p><p>Kaggle最近在我们的内核笔记本环境中发布了BigQuery集成，该入门内核为您提供了使用BQ和BQML的绝佳起点。建议使用精明的数据，机智和直觉来查找并加入其他外部数据集，以提高模型的预测能力。</p><h4 id="1-2-评估"><a href="#1-2-评估" class="headerlink" title="1.2 评估"></a>1.2 评估</h4><p>根据均方根误差对提交的内容评分。RMSE定义为：</p><script type="math/tex; mode=display">\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}</script><p>其中$\hat{y}$是预测值，$y$是原始值。</p><p>提交文件：</p><p>对于测试集中的每一行，必须按照“数据”选项卡上的描述预测六个目标结果的值，每个目标结果都在提交文件中的单独一行中。 该文件应包含标头，并具有以下格式：</p><div class="table-container"><table><thead><tr><th>ID</th><th>TARGET</th></tr></thead><tbody><tr><td>0_1</td><td>0</td></tr><tr><td>0_2</td><td>0</td></tr><tr><td>0_3</td><td>0</td></tr></tbody></table></div><h4 id="1-3-资源"><a href="#1-3-资源" class="headerlink" title="1.3 资源"></a>1.3 资源</h4><p>通过内核使用 BigQuery 和 BigQuery ML：除了可以在 Kaggle 上下载之外，该比赛的数据集还托管在一个私有的 BigQuery 数据集中。 您可以按照“数据”页面上 BigQuery-Dataset-Access.md 文档中的说明进行访问。在“设置”面板中，可以在 Kaggle 内核/笔记本上启用 BigQuery。</p><p>入门资源：本入门教程向您展示如何开始在 BigQuery ML上运行查询。</p><h4 id="1-4-数据描述"><a href="#1-4-数据描述" class="headerlink" title="1.4 数据描述"></a>1.4 数据描述</h4><p>数据包括来自商用车辆（例如半卡车）的汇总行程记录指标。数据已按照交叉路口，月份，一天中的小时，通过交叉路口的方向以及一天是否在周末进行了分组。</p><p>对于测试集中的每个分组，需要针对两个不同度量标准的三个不同分位数进行预测，以涵盖该组车辆驶过交叉路口需要多长时间。具体来说，总时间的20％，50％和80％在交叉路口停止，并且交叉路口与车辆在等待时停止的第一位置之间的距离。您可以将目标视为总结每个交叉路口的等待时间和停车距离的分布。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">RowId                int64</span><br><span class="line">IntersectionId       int64</span><br><span class="line">Latitude           float64</span><br><span class="line">Longitude          float64</span><br><span class="line">EntryStreetName     object</span><br><span class="line">ExitStreetName      object</span><br><span class="line">EntryHeading        object</span><br><span class="line">ExitHeading         object</span><br><span class="line">Hour                 int64</span><br><span class="line">Weekend              int64</span><br><span class="line">Month                int64</span><br><span class="line">Path                object</span><br><span class="line">City                object</span><br></pre></td></tr></table></figure><p>这六个预测中的每个预测都在提交文件中的新行中进行。 读取提交的TargetId字段，例如1_1，第一个数字是RowId，第二个数字是度量标准ID。 可以使用submission_metric_map.json解压缩提交指标ID代码。</p><p>训练集包括一个可选的附加输出指标（TimeFromFirstStop），以防它对构建模型有用。 它仅从测试集中排除，以限制必须进行的预测的次数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">RowId                        int64</span><br><span class="line">IntersectionId               int64</span><br><span class="line">Latitude                   float64</span><br><span class="line">Longitude                  float64</span><br><span class="line">EntryStreetName             object</span><br><span class="line">ExitStreetName              object</span><br><span class="line">EntryHeading                object</span><br><span class="line">ExitHeading                 object</span><br><span class="line">Hour                         int64</span><br><span class="line">Weekend                      int64</span><br><span class="line">Month                        int64</span><br><span class="line">Path                        object</span><br><span class="line">TotalTimeStopped_p20         int64</span><br><span class="line">TotalTimeStopped_p40         int64</span><br><span class="line">TotalTimeStopped_p50         int64</span><br><span class="line">TotalTimeStopped_p60         int64</span><br><span class="line">TotalTimeStopped_p80         int64</span><br><span class="line">TimeFromFirstStop_p20        int64</span><br><span class="line">TimeFromFirstStop_p40        int64</span><br><span class="line">TimeFromFirstStop_p50        int64</span><br><span class="line">TimeFromFirstStop_p60        int64</span><br><span class="line">TimeFromFirstStop_p80        int64</span><br><span class="line">DistanceToFirstStop_p20    float64</span><br><span class="line">DistanceToFirstStop_p40    float64</span><br><span class="line">DistanceToFirstStop_p50    float64</span><br><span class="line">DistanceToFirstStop_p60    float64</span><br><span class="line">DistanceToFirstStop_p80    float64</span><br><span class="line">City                        object</span><br></pre></td></tr></table></figure><h3 id="2-实现"><a href="#2-实现" class="headerlink" title="2. 实现"></a>2. 实现</h3><h4 id="2-1-导入库"><a href="#2-1-导入库" class="headerlink" title="2.1 导入库"></a>2.1 导入库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1. scipy：建立在Numpy基础上，用于数值运算，如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵。scipy.stats中包含很多常见的分布</span><br><span class="line">        rvs产生服从制定分布的随机数</span><br><span class="line">        pdf概率密度函数</span><br><span class="line">        cdf累计分布函数</span><br><span class="line">        sf残存函数(1-cdf)</span><br><span class="line">        ppf分位点函数（CDF的你函数）</span><br><span class="line">        isf逆残存函数</span><br><span class="line">        fit对随机取样进行你和，最大似然估计找到的概率密度函数系数</span><br><span class="line">2. Seaborn：对matplotlib的extend，是一个数据可视化库，提供更高级的API封装，在应用中更加的方便灵活。</span><br><span class="line">3. plotly：一款基于D3.js框架的Python库，非常著名且强大的开源数据可视化框架，通过构建基于浏览器显示的web形式的可交互图表来展示信息，可创建多达数十种精美的图表和地图。</span><br><span class="line">4. skleran: sklearn.metrics.mean_squared_error计算均方误差回归误差</span><br><span class="line">sklearn.preprocessing.LabelEncoder可以将标签分配一个0—n_classes-1之间的编码</span><br><span class="line">5. functools：为了高阶函数（该高阶函数的定义为作用于或返回其它函数的函数）而设置的。一般来说，任何可调用的对象在该模块中都可被当做函数而处理。functools.partial 通过包装手法，允许我们 &quot;重新定义&quot; 函数签名，用一些默认参数包装一个可调用对象,返回结果是可调用对象，并且可以像原始对象一样对待。</span><br><span class="line">6. hyperopt: 帮助快速进行机器学习模型参数调试。通常情况下有两种类型的参数调试方法，网格搜索（grid search）和随机搜索（random search）。网格搜索速度慢但是适用于需要随整个参数空间进行搜索的情况；随机搜索速度很快但是容易遗漏一些重要信息。Hyheropt四个重要的因素：指定需要最小化的函数，搜索的空间，采样的数据集(trails database)（可选），搜索的算法（可选）。</span><br></pre></td></tr></table></figure><h4 id="2-2-部分函数解读"><a href="#2-2-部分函数解读" class="headerlink" title="2.2 部分函数解读"></a>2.2 部分函数解读</h4><h5 id="pd-DataFrame"><a href="#pd-DataFrame" class="headerlink" title="pd.DataFrame()"></a>pd.DataFrame()</h5><p>在pandas里，DataFrame是最经常用的数据结构，这里总结生成和添加数据的方法：</p><ol><li><p>把其他格式的数据整理到DataFrame中；</p><p>① 字典类型读取到DataFrame: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">test_dict = &#123;<span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],<span class="string">'name'</span>:[<span class="string">'Alice'</span>,<span class="string">'Bob'</span>,<span class="string">'Cindy'</span>],<span class="string">'math'</span>:[<span class="number">90</span>,<span class="number">89</span>,<span class="number">99</span>],<span class="string">'english'</span>:[<span class="number">89</span>,<span class="number">94</span>,<span class="number">80</span>]&#125;</span><br><span class="line"><span class="comment"># 直接使用`pd.DataFrame(data=test_dict)`即可,括号中的`data=`写不写都可以 。</span></span><br><span class="line">test_dict_df = pd.DataFrame(test_dict) <span class="comment">#[1].直接写入参数test_dict</span></span><br><span class="line">test_dict_df = pd.DataFrame(data=test_dict) <span class="comment">#[2].字典型赋值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用from_dict方法</span></span><br><span class="line">test_dict_df = pd.DataFrame.from_dict(test_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：如果你的dict变量很小，例如&#123;'id':1,'name':'Alice'&#125;,想直接写到括号里：</span></span><br><span class="line"><span class="comment"># test_dict_df = pd.DataFrame(&#123;'id':1,'name':'Alice'&#125;) 会报错ValueError: If using all scalar values, you must pass an index</span></span><br><span class="line">test_dict_df = pd.DataFrame(&#123;<span class="string">'id'</span>:<span class="number">1</span>,<span class="string">'name'</span>:<span class="string">'Alice'</span>&#125;,pd.Index(range(<span class="number">1</span>)))</span><br><span class="line">pd.DataFrame(&#123;<span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>],<span class="string">'name'</span>:[<span class="string">'Alice'</span>,<span class="string">'Bob'</span>]&#125;,pd.Index(range(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只需要选择dict中部分的键当做DataFrame的列，可以使用columns参数</span></span><br><span class="line">test_dict_df = pd.DataFrame(data=test_dict,columns=[<span class="string">'id'</span>,<span class="string">'name'</span>])</span><br></pre></td></tr></table></figure><p>② csv文件构建DataFrame</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最常用的应该就是pd.read_csv('filename.csv')了，用sep指定数据的分割方式，默认的是','</span></span><br><span class="line">df = pd.read_csv(<span class="string">'./xxx.csv'</span>) <span class="comment"># 如果csv中没有表头，就要加入head参数</span></span><br></pre></td></tr></table></figure></li><li><p>在已有的DataFrame中插入N列或者N行。 </p><p>① 添加列 insert方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_columns = [<span class="number">92</span>,<span class="number">94</span>,<span class="number">89</span>,<span class="number">77</span>,<span class="number">87</span>,<span class="number">91</span>]</span><br><span class="line">test_dict_df.insert(<span class="number">2</span>,<span class="string">'pyhsics'</span>,new_columns)</span><br><span class="line"><span class="comment"># DataFrame默认不允许添加重复的列，但是在insert函数中有参数 allow_duplicates=True，设置为True后，就可以添加重复的列了，列名也是重复的：</span></span><br></pre></td></tr></table></figure><p>② 添加行 loc方法或者append方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">new_line = [<span class="number">7</span>,<span class="string">'Iric'</span>,<span class="number">99</span>]</span><br><span class="line"><span class="comment"># 但是十分注意的是，这样实际是改的操作，如果loc[index]中的index已经存在，则新的值会覆盖之前的值。</span></span><br><span class="line">test_dict_df.loc[<span class="number">3</span>]= new_line </span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以把这些新的数据构建为一个新的DataFrame，然后两个DataFrame拼起来。可以用append方法</span></span><br><span class="line">test_dict_df.append(pd.DataFrame([new_line],columns=[<span class="string">'id'</span>,<span class="string">'name'</span>,<span class="string">'physics'</span>]))</span><br></pre></td></tr></table></figure></li></ol><h5 id="df-reset-index"><a href="#df-reset-index" class="headerlink" title="df.reset_index()"></a>df.reset_index()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据清洗时，会将带空值的行删除，此时DataFrame或Series类型的数据不再是连续的索引，可以使用reset_index()重置索引。 在获得新的index，原来的index变成数据列，保留下来。不想保留原来的index，使用参数 drop=True，默认 False。</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df.reset_index()</span><br><span class="line">df.reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h5 id="df-isnull"><a href="#df-isnull" class="headerlink" title="df.isnull()"></a>df.isnull()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># df.isnull() 元素级别的判断，把对应的所有元素的位置都列出来，元素为空或者NA就显示True，否则就是False</span></span><br><span class="line"><span class="comment"># df.isnull().any() 各个列是否存在缺失值</span></span><br><span class="line"><span class="comment"># df.isnull().any(1) 查看行包含缺失值情况</span></span><br><span class="line"><span class="comment"># df.isnull().values.any()  检查DataFrame 是否还有缺失值 返回True/False</span></span><br><span class="line"><span class="comment"># df.isnull().sum() 检查字段缺失值的数量</span></span><br><span class="line"><span class="comment"># df.isnull().sum().sum() 计算所有缺失值的数量</span></span><br></pre></td></tr></table></figure><h5 id="df-nunique"><a href="#df-nunique" class="headerlink" title="df.nunique()"></a>df.nunique()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回的是唯一值的个数</span></span><br><span class="line"><span class="comment"># 区别df.unique(), unique()是以数组形式（numpy.ndarray）返回列的所有唯一值（特征的所有唯一值）</span></span><br></pre></td></tr></table></figure><h5 id="df-value-counts"><a href="#df-value-counts" class="headerlink" title="df.value_counts()"></a>df.value_counts()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看表格某列中有多少个不同值的快捷方法，并计算每个不同值有在该列中有多少重复值,且默认从高到低排序</span></span><br><span class="line"><span class="comment"># 想升序排列，设置参数 ascending = True</span></span><br><span class="line"><span class="comment"># 想得出计数占比，可以加参数 normalize=True</span></span><br></pre></td></tr></table></figure><h5 id="df-groupby"><a href="#df-groupby" class="headerlink" title="df.groupby()"></a>df.groupby()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># groupby就是按xx分组, 基本操作：组内计数, 求和, 求均值, 求方差...</span></span><br><span class="line"><span class="comment"># 例如：对不同取值的计数: .value_counts()</span></span><br></pre></td></tr></table></figure><h5 id="sns-heatmap"><a href="#sns-heatmap" class="headerlink" title="sns.heatmap()"></a>sns.heatmap()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">seaborn.heatmap(data, vmin=<span class="literal">None</span>, vmax=<span class="literal">None</span>, cmap=<span class="literal">None</span>, center=<span class="literal">None</span>, robust=<span class="literal">False</span>, annot=<span class="literal">None</span>, fmt=<span class="string">'.2g'</span>, annot_kws=<span class="literal">None</span>, linewidths=<span class="number">0</span>, linecolor=<span class="string">'white'</span>, cbar=<span class="literal">True</span>, cbar_kws=<span class="literal">None</span>, cbar_ax=<span class="literal">None</span>, square=<span class="literal">False</span>, xticklabels=<span class="string">'auto'</span>, yticklabels=<span class="string">'auto'</span>, mask=<span class="literal">None</span>, ax=<span class="literal">None</span>, **kwargs)</span><br><span class="line"><span class="comment"># 共20个参数，除参数data外其他都是默认</span></span><br><span class="line"><span class="comment"># 对于二维数组：热力图就是把这个二维的数组的数字用热力图的颜色值来表示</span></span><br><span class="line"><span class="comment"># 即热力图的作用：可视化已有的数字，数据表里多个特征两两的相似度</span></span><br><span class="line"><span class="comment"># annot: 默认为False，为True的话，会在格子上显示数字</span></span><br><span class="line"><span class="comment"># vmax, vmin: 热力图颜色取值的最大值，最小值，默认会从data中推导</span></span><br></pre></td></tr></table></figure><h5 id="pd-concat"><a href="#pd-concat" class="headerlink" title="pd.concat()"></a>pd.concat()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以将数据根据不同的轴作简单的融合</span></span><br><span class="line">pd.concat(objs, axis=<span class="number">0</span>, join=<span class="string">'outer'</span>, join_axes=<span class="literal">None</span>, ignore_index=<span class="literal">False</span>, keys=<span class="literal">None</span>, levels=<span class="literal">None</span>, names=<span class="literal">None</span>, verify_integrity=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 参数说明 </span></span><br><span class="line"><span class="comment"># objs: series，dataframe或者是panel构成的序列lsit </span></span><br><span class="line"><span class="comment"># axis： 需要合并链接的轴，0是行，1是列 </span></span><br><span class="line"><span class="comment"># join：连接的方式 inner，或者outer</span></span><br></pre></td></tr></table></figure><h5 id="sklearn-decomposition-PCA"><a href="#sklearn-decomposition-PCA" class="headerlink" title="sklearn.decomposition.PCA"></a>sklearn.decomposition.PCA</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参考：https://www.cnblogs.com/pinard/p/6243025.html</span></span><br><span class="line"><span class="comment"># 在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。最常用的PCA类是sklearn.decomposition.PCA， 除了PCA类以外，最常用的PCA相关类还有KernelPCA类（主要用于非线性数据的降维，需要用到核技巧。因此在使用的时候需要选择合适的核函数并对核函数的参数进行调参）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn.decomposition.PCA参数介绍：</span></span><br><span class="line">   <span class="comment"># 1）n_components：指定希望PCA降维后的特征维度数目，也即保留下来的特征个数n。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于等于1的整数。也可以指定主成分的方差和所占的最小比例阈值，让PCA类根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。还可以将参数设置为"mle", 此时PCA类会用MLE算法根据特征的方差分布情况去选择一定数量的主成分特征来降维。也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。</span></span><br><span class="line">   <span class="comment"># 2）whiten ：判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为1.对于PCA降维本身来说，一般不需要白化。如果你PCA降维后有后续的数据处理动作，可以考虑白化。默认值是False，即不进行白化。</span></span><br><span class="line">   <span class="comment"># 3）svd_solver：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：&#123;‘auto’, ‘full’, ‘arpack’, ‘randomized’&#125;。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。 full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。</span></span><br><span class="line"> 　<span class="comment"># 除了这些输入参数外，有两个PCA类的成员值得关注。第一个是explained_variance_，它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。第二个是explained_variance_ratio_，它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。</span></span><br><span class="line">   <span class="comment"># fit_transform(X), 用X来训练PCA模型，同时返回降维后的数据。</span></span><br></pre></td></tr></table></figure><h5 id="sns-FacetGrid"><a href="#sns-FacetGrid" class="headerlink" title="sns.FacetGrid()"></a>sns.FacetGrid()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先sns.FacetGrid画出轮廓 然后用map填充内容</span></span><br><span class="line"><span class="comment"># plt.hist直方图 plt.scatter散点图 sns.barplot条形图</span></span><br><span class="line"><span class="comment"># 参数alpha，设置点的大小</span></span><br></pre></td></tr></table></figure><h5 id="pd-crosstab"><a href="#pd-crosstab" class="headerlink" title="pd.crosstab"></a>pd.crosstab</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉表是用于统计分组频率的特殊透视表</span></span><br><span class="line"><span class="comment"># 透视表：pd.pivot_table, 透视表就是将指定原有DataFrame的列分别作为行索引和列索引，然后对指定的列应用聚集函数(默认情况下式mean函数)。</span></span><br></pre></td></tr></table></figure><h5 id="df-drop-duplicates"><a href="#df-drop-duplicates" class="headerlink" title="df.drop_duplicates()"></a>df.drop_duplicates()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除特定列下面的重复行, 返回DataFrame格式的数据</span></span><br><span class="line">DataFrame.drop_duplicates(subset=<span class="literal">None</span>, keep=<span class="string">'first'</span>, inplace=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># subset : column label or sequence of labels, optional, 用来指定特定的列，默认所有列</span></span><br><span class="line"><span class="comment"># keep : &#123;‘first’, ‘last’, False&#125;, default ‘first’, 删除重复项并保留第一次出现的项</span></span><br><span class="line"><span class="comment"># inplace : boolean, default False, 是直接在原来数据上修改还是保留一个副本</span></span><br></pre></td></tr></table></figure><h5 id="sklearn-LabelEncoder"><a href="#sklearn-LabelEncoder" class="headerlink" title="sklearn.LabelEncoder()"></a>sklearn.LabelEncoder()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LabelEncoder可以将标签分配一个0—n_classes-1之间的编码, 将各种标签分配一个可数的连续编号</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">le = preprocessing.LabelEncoder()</span><br><span class="line">le.fit([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>])  <span class="comment"># ---&gt; LabelEncoder()</span></span><br><span class="line">le.classes_   <span class="comment"># --&gt; array([1, 2, 6])</span></span><br><span class="line">le.transform([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">6</span>]) <span class="comment"># --&gt; array([0, 0, 1, 2], dtype=int64)</span></span><br><span class="line">le.inverse_transform([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># Transform Integers Into Categories  --&gt; array([1, 1, 2, 6])</span></span><br></pre></td></tr></table></figure><h5 id="pd-get-dummies"><a href="#pd-get-dummies" class="headerlink" title="pd.get_dummies()"></a>pd.get_dummies()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one-hot的基本思想：将离散型特征的每一种取值都看成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。</span></span><br><span class="line"><span class="comment"># dummy encoding 直观的解释就是任意的将一个状态位去除。</span></span><br><span class="line"><span class="comment"># pandas提供对one-hot编码的函数是：pd.get_dummies()</span></span><br><span class="line">pandas.get_dummies(data, prefix=<span class="literal">None</span>, prefix_sep=<span class="string">'_'</span>, dummy_na=<span class="literal">False</span>, columns=<span class="literal">None</span>, sparse=<span class="literal">False</span>, drop_first=<span class="literal">False</span>, dtype=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 输入：array-like, Series, or DataFrame</span></span><br><span class="line"><span class="comment"># 输出：DataFrame</span></span><br><span class="line"><span class="comment"># 主要参数说明：</span></span><br><span class="line"><span class="comment"># data : array-like, Series, or DataFrame</span></span><br><span class="line"><span class="comment"># prefix : string, list of strings, or dict of strings, default None, 给输出的列添加前缀，如prefix="A",输出的列会显示类似</span></span><br><span class="line"><span class="comment"># prefix_sep : 设置前缀跟分类的分隔符sepration，默认是下划线"_"</span></span><br><span class="line"><span class="comment"># 直接在原始数据中操作，可以使用columns参数</span></span><br><span class="line"><span class="comment"># drop_first : bool, default False, 获得k中的k-1个类别值，去除第一个</span></span><br></pre></td></tr></table></figure><h5 id="round"><a href="#round" class="headerlink" title="round()"></a>round()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># round() 方法返回浮点数x的四舍五入值。</span></span><br></pre></td></tr></table></figure></0$为最大值），>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、GBDT回归算法&quot;&gt;&lt;a href=&quot;#一、GBDT回归算法&quot; class=&quot;headerlink&quot; title=&quot;一、GBDT回归算法&quot;&gt;&lt;/a&gt;一、GBDT回归算法&lt;/h2&gt;&lt;p&gt;参考：&lt;/p&gt;
&lt;p&gt; &lt;a href=&quot;https://www.cnblo
      
    
    </summary>
    
    
      <category term="Kaggle比赛" scheme="http://pl741.github.io/categories/Kaggle%E6%AF%94%E8%B5%9B/"/>
    
    
      <category term="GBDT" scheme="http://pl741.github.io/tags/GBDT/"/>
    
  </entry>
  
  <entry>
    <title>今日头条App爬虫</title>
    <link href="http://pl741.github.io/2019/10/31/%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1app%E7%88%AC%E8%99%AB/"/>
    <id>http://pl741.github.io/2019/10/31/今日头条app爬虫/</id>
    <published>2019-10-31T11:58:19.000Z</published>
    <updated>2019-11-11T03:18:58.725Z</updated>
    
    <content type="html"><![CDATA[<h2 id="今日头条App爬虫"><a href="#今日头条App爬虫" class="headerlink" title="今日头条App爬虫"></a>今日头条App爬虫</h2><h3 id="一、安装App爬虫模拟器与抓包软件（11-04）"><a href="#一、安装App爬虫模拟器与抓包软件（11-04）" class="headerlink" title="一、安装App爬虫模拟器与抓包软件（11.04）"></a>一、安装App爬虫模拟器与抓包软件（11.04）</h3><h4 id="Fiddler抓包工具安装与配置"><a href="#Fiddler抓包工具安装与配置" class="headerlink" title="Fiddler抓包工具安装与配置"></a>Fiddler抓包工具安装与配置</h4><ol><li><p>下载地址：<a href="https://www.telerik.com/fiddler" target="_blank" rel="noopener">https://www.telerik.com/fiddler</a></p></li><li><p>设置相关选项：打开fiddler，选择Tools——&gt;Options——&gt;HTTPS</p><p>勾选“Capture HTTPS CONNECTs”，同时勾选“Decrypt HTTPS traffic”—- Fiddler就可以截获HTTPS请求。</p><p>如果需要监听不可信的证书的HTTPS请求的话，需要勾选“Ignore server certificate errors (unsafe)”</p><img src="/2019/10/31/今日头条app爬虫/fiddler_1.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/fiddler_1.png" alt="1" style="zoom:60%;"></p></li><li><p>手机配置：Tools——&gt;Options——&gt;Connection</p><p>输入端口号：8889 ——手机输入Ip端口与Fiddler端口匹配</p><p>勾选”Allow remote computers to connect”。——允许别的机器把HTTP/HTTPS请求发送到Fiddler上来</p><img src="/2019/10/31/今日头条app爬虫/fiddler_2.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/fiddler_2.png" alt="1" style="zoom:60%;"></p></li></ol><h4 id="夜神Android模拟器安装与配置"><a href="#夜神Android模拟器安装与配置" class="headerlink" title="夜神Android模拟器安装与配置"></a>夜神Android模拟器安装与配置</h4><ol><li><p>下载网址：<a href="https://www.yeshen.com/" target="_blank" rel="noopener">https://www.yeshen.com/</a></p><p>选择历史版本，高版本在为wifi设置网桥代理时容易出错，需要下载专门的代理软件</p><img src="/2019/10/31/今日头条app爬虫/yeshen_1.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_1.png" alt="1" style="zoom:60%;"></p><p>我选择的版本：V6.2.2.3</p><img src="/2019/10/31/今日头条app爬虫/yeshen_2.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_2.png" alt="1" style="zoom:60%;"></p></li><li><p>为Android模拟器设置：</p><p>设置——&gt;WLAN，鼠标点击wifi位置，长按左键，会出现一个修改网络的弹窗</p><img src="/2019/10/31/今日头条app爬虫/yeshen_3.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_3.png" alt="1" style="zoom:60%;"></p><img src="/2019/10/31/今日头条app爬虫/yeshen_4.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_4.png" alt="1" style="zoom:60%;"></p></li><li><p>点击上图中的“修改网络”，会出现下图中的弹窗，勾选”显示高级选项”，代理选择“手动”</p><p>代理服务器主机名填写自己电脑的ip地址，接着再填写端口，与fiddler设置的端口号相同</p><img src="/2019/10/31/今日头条app爬虫/yeshen_5.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_5.png" alt="1" style="zoom:60%;"></p></li><li><p>安装Fiddler证书：在模拟器浏览器输入<a href="http://ipv4.fiddler:8888/" target="_blank" rel="noopener">http://ipv4.fiddler:8888</a>，下载安装证书</p><img src="/2019/10/31/今日头条app爬虫/yeshen_6.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_6.png" alt="1" style="zoom:60%;"></p><p>为证书命名：</p><img src="/2019/10/31/今日头条app爬虫/yeshen_7.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_7.png" alt="1" style="zoom:60%;"></p><p>检查证书安装成功：设置———&gt;安全</p><img src="/2019/10/31/今日头条app爬虫/yeshen_9.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_9.png" alt="1" style="zoom:60%;"></p><img src="/2019/10/31/今日头条app爬虫/yeshen_8.png"><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/yeshen_8.png" alt="1" style="zoom:60%;"></p></li></ol><h4 id="Postman网页调试工具安装"><a href="#Postman网页调试工具安装" class="headerlink" title="Postman网页调试工具安装"></a>Postman网页调试工具安装</h4><ol><li><p>下载地址：<a href="https://www.getpostman.com/apps" target="_blank" rel="noopener">https://www.getpostman.com/apps</a></p><p>选择自己电脑对应的版本：Windows 32-bit 或者 Windows 64-bit</p></li></ol><h3 id="二、今日头条官方账号网址解析（11-04）"><a href="#二、今日头条官方账号网址解析（11-04）" class="headerlink" title="二、今日头条官方账号网址解析（11.04）"></a>二、今日头条官方账号网址解析（11.04）</h3><p><strong>今日头条官方账号：</strong></p><img src="/2019/10/31/今日头条app爬虫/今日头条官方账号.png"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 爬取到的网址</span><br><span class="line">https://ic.snssdk.com/api/search/content/?from=media&amp;keyword=%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1&amp;search_position=search_bar&amp;action_type=&amp;iid=90943616846&amp;device_id=68611601593&amp;ac=wifi&amp;channel=lite2_tengxun&amp;aid=35&amp;app_name=news_article_lite&amp;version_code=718&amp;version_name=7.1.8&amp;device_platform=android&amp;abflag=3&amp;device_type=HUAWEI%2BMLA-AL10&amp;device_brand=HUAWEI&amp;language=zh&amp;os_api=19&amp;os_version=4.4.2&amp;uuid=863064010001412&amp;openudid=000ec6d69fe08850&amp;manifest_version_code=718&amp;resolution=720*1280&amp;dpi=240&amp;update_version_code=71805&amp;_rticket=1572515243458&amp;sa_enable=0&amp;rom_version=19&amp;plugin_state=32727103&amp;fp=a_fake_fp&amp;is_ttwebview=0&amp;search_sug=1&amp;is_native_req=1&amp;fetch_by_ttnet=0&amp;search_start_time=1572515243460&amp;forum=1&amp;count=10&amp;format=json&amp;source=search_subtab_switch&amp;pd=user&amp;keyword_type=&amp;from_search_subtab=synthesis&amp;offset=0&amp;search_id=&amp;has_count=0&amp;qc_query=</span><br></pre></td></tr></table></figure><p>利用Postman进行网址分析：剔除不必要的字段，必须含有的字段包括<strong><em>count,   keyword,   offset,   channel,    app_name,    device_platform,    from,   device_type,   pd</em></strong>，字段<strong><em>pd</em></strong>表示搜索的是<strong>用户</strong>，字段<strong><em>keyword</em></strong>表示搜索的关键字，例如“今日头条”，其他字段为必须含有的固定字段。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 简化后的网址</span><br><span class="line">https://ic.snssdk.com/api/search/content/?count=10&amp;keyword=%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1&amp;offset=0&amp;channel=lite2_tengxun&amp;app_name=news_article_lite&amp;device_platform=android&amp;from=media&amp;device_type=HUAWEI%2BMLA-AL10&amp;pd=user</span><br></pre></td></tr></table></figure><p><strong>今日头条官方账号主页：</strong></p><img src="/2019/10/31/今日头条app爬虫/今日头条官方账号主页.png"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 爬取到的网址：</span><br><span class="line">https://ic.snssdk.com/api/feed/profile/v1/?category=profile_all&amp;visited_uid=3612927328&amp;stream_api_version=47&amp;count=20&amp;offset=0&amp;client_extra_params=%7B%7D&amp;iid=90943616846&amp;device_id=68611601593&amp;ac=wifi&amp;channel=lite2_tengxun&amp;aid=35&amp;app_name=news_article_lite&amp;version_code=718&amp;version_name=7.1.8&amp;device_platform=android&amp;ab_version=668906%2C1248366%2C1183179%2C631598%2C668903%2C679107%2C668905%2C643996%2C661929%2C785656%2C668907%2C808414%2C1210943%2C668908%2C668904%2C908123%2C928942%2C1211862&amp;ab_client=a1%2Cc2%2Ce1%2Cf2%2Cg2%2Cf7&amp;ab_feature=z1&amp;abflag=3&amp;ssmix=a&amp;device_type=HUAWEI+MLA-AL10&amp;device_brand=HUAWEI&amp;language=zh&amp;os_api=19&amp;os_version=4.4.2&amp;uuid=863064010001412&amp;openudid=000ec6d69fe08850&amp;manifest_version_code=718&amp;resolution=720*1280&amp;dpi=240&amp;update_version_code=71805&amp;_rticket=1572515346612&amp;sa_enable=0&amp;rom_version=19&amp;plugin_state=32727103&amp;fp=a_fake_fp&amp;ts=1572515346&amp;as=ab6c1ade505dbaae126c1a&amp;mas=01199333939919b9b3f95379b985624a77b9b3f95373b399b9b979</span><br></pre></td></tr></table></figure><p>利用Postman进行网址解析：剔除不必要的字段，最终必须包含的字段有<strong><em>category,   visited_uid,   count,   offset,   device_id,    channel,  app_name,   version_code,   device_platform,   device_type,   os_version</em></strong>，字段<strong><em>category</em></strong>可以是<strong>profile_all(全部)， profile_article(文章)， profile_video(视频)， profile_wenda(问答)</strong>，<font color="#D2691E">小视频</font>暂时还未抓取到关键字，其中有两个关键字段分别为<strong><em>visited_uid,   device_id</em></strong>，<strong><em>device_id</em></strong>是设备的ID，<strong><em>visited_uid</em></strong>在今日头条官方账号网页响应中可以获得，如下图：</p><img src="/2019/10/31/今日头条app爬虫/visited_id.png"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 简化后的网址</span><br><span class="line">https://ic.snssdk.com/api/feed/profile/v1/?category=profile_all&amp;visited_uid=3612927328&amp;count=20&amp;offset=0&amp;device_id=68611601593&amp;channel=lite2_tengxun&amp;app_name=news_article_lite&amp;version_code=717&amp;device_platform=android&amp;device_type=HUAWEI+MLA-AL10&amp;os_version=4.4.2</span><br></pre></td></tr></table></figure><h3 id="三、MongoDB中存储数据（11-04）"><a href="#三、MongoDB中存储数据（11-04）" class="headerlink" title="三、MongoDB中存储数据（11.04）"></a>三、MongoDB中存储数据（11.04）</h3><p>按照给定的MongoDB地址连接数据库，并将数据存入数据库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#pymongo.MongoClient连接到数据库</span></span><br><span class="line">        client = pymongo.MongoClient(<span class="string">"mongodb://toutiao:Toutiao#2019@47.92.136.180/toutiao"</span>)</span><br><span class="line">        <span class="comment"># 创建数据库'db1'</span></span><br><span class="line">        db = client[<span class="string">'toutiao'</span>]</span><br><span class="line">        <span class="comment"># 连接到数据集'toutiao'，类型为dict</span></span><br><span class="line">        self.collection=db[<span class="string">'toutiao'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">        <span class="comment">#插入数据到数据库</span></span><br><span class="line">        self.collection.update_one(&#123;<span class="string">'_id'</span>: item[<span class="string">'_id'</span>]&#125;, &#123;<span class="string">'$set'</span>: dict(item)&#125;, <span class="literal">True</span>)</span><br><span class="line">        print(<span class="string">u'-----------插入/更新成功!'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>在robomongo数据库可视化中查看数据是否插入成功：</p><p><img src="//pl741.github.io/2019/10/31/今日头条app爬虫/mongo_1.png" alt="1"></p><h3 id="四、今日头条主页返回信息分析（11-11）"><a href="#四、今日头条主页返回信息分析（11-11）" class="headerlink" title="四、今日头条主页返回信息分析（11.11）"></a>四、今日头条主页返回信息分析（11.11）</h3><p>根据要提取的内容：帖子内容，转发数，评论数，点赞数以及评论下的每条评论用户名，评论内容，此条评论的评论和获赞数</p><ol><li><p>首先根据要求先获取帖子内容，转发数，评论数，点赞数：</p><p>get请求：<code>https://ic.snssdk.com/api/feed/profile/v1/?category=profile_all&amp;visited_uid=3612927328&amp;offset=0&amp;device_id=68611601593&amp;channel=lite2_tengxun&amp;app_name=news_article_lite&amp;version_code=717&amp;device_platform=android&amp;device_type=HUAWEI+MLA-AL10&amp;os_version=4.4.2</code></p><p>转发数字段：forward_count</p><p>评论数字段：comment_count</p><p>点赞数字段：digg_count</p><p>帖子内容字段：title字段（帖子的标题， 有些帖子此字段为空） abstract， content 其中 abstract 字段为空的帖子的返回信息中含有 content 字段，content 字段包含了帖子的全部内容，abstract 字段不为空的帖子的返回信息中不含有 content 字段，abstract 字段不含有帖子的全部内容，此时在返回的信息中没有帖子的全部内容，<font color="red">是否需要获取此类帖子的全部内容，如何获取？</font></p><p>其他需要获取的字段：group_id（帖子id字段，为获取帖子的评论内容）</p><p>其他问题：手机端中带有 #头条抽奖# 的帖子无法爬取到，原因是在模拟器中这些内容不显示，因此获得URL的响应也中不含有这些内容</p></li><li><p>获取每条评论的评论用户名，评论内容，以及评论获赞数</p><p>获取每条评论的评论用户，评论内容，以及评论获赞数</p><p>get请求：<code>https://ic.snssdk.com/article/v4/tab_comments/?group_id=&amp;count=20&amp;offset=0&amp;device_id=68611601593&amp;aid=35</code></p><p>deviced_id为可选字段，group_id是评论帖子的id</p><p>评论用户名字段：user_name</p><p>评论内容字段：text</p><p>评论获赞字段：digg_count</p><p>其他字段：id（评论的id）</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;今日头条App爬虫&quot;&gt;&lt;a href=&quot;#今日头条App爬虫&quot; class=&quot;headerlink&quot; title=&quot;今日头条App爬虫&quot;&gt;&lt;/a&gt;今日头条App爬虫&lt;/h2&gt;&lt;h3 id=&quot;一、安装App爬虫模拟器与抓包软件（11-04）&quot;&gt;&lt;a href=&quot;#一
      
    
    </summary>
    
    
      <category term="Python爬虫" scheme="http://pl741.github.io/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="今日头条App爬虫 + Scrapy框架" scheme="http://pl741.github.io/tags/%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1App%E7%88%AC%E8%99%AB-Scrapy%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫</title>
    <link href="http://pl741.github.io/2019/10/29/Python%E7%88%AC%E8%99%AB/"/>
    <id>http://pl741.github.io/2019/10/29/Python爬虫/</id>
    <published>2019-10-29T04:37:21.000Z</published>
    <updated>2019-11-04T02:27:25.339Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h2><h4 id="安装python库：-pip-install-pymongo"><a href="#安装python库：-pip-install-pymongo" class="headerlink" title="安装python库： pip install pymongo"></a>安装python库： <code>pip install pymongo</code></h4><h4 id="使用方法："><a href="#使用方法：" class="headerlink" title="使用方法："></a>使用方法：</h4><h5 id="emsp-emsp-连接到MongoClient"><a href="#emsp-emsp-连接到MongoClient" class="headerlink" title="&emsp;&emsp;连接到MongoClient"></a>&emsp;&emsp;连接到MongoClient</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 连接到数据库，为空默认ip为localhost，端口为27017</span></span><br><span class="line"><span class="comment"># 以下三种方式都相同</span></span><br><span class="line">client = pymongo.MongoClient()</span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">client = pymongo.MongoClient(<span class="string">'mongodb://localhost:27017/'</span>)</span><br></pre></td></tr></table></figure><h5 id="emsp-emsp-连接指定的数据库"><a href="#emsp-emsp-连接指定的数据库" class="headerlink" title="&emsp;&emsp;连接指定的数据库"></a>&emsp;&emsp;连接指定的数据库</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db = client.test_database</span><br><span class="line">db = client[<span class="string">'test_database'</span>]</span><br></pre></td></tr></table></figure><h5 id="emsp-emsp-连接指定的表"><a href="#emsp-emsp-连接指定的表" class="headerlink" title="&emsp;&emsp;连接指定的表"></a>&emsp;&emsp;连接指定的表</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">collection = db.test_collection</span><br><span class="line">collection = db[<span class="string">'test_collection'</span>]</span><br></pre></td></tr></table></figure><h5 id="emsp-emsp-插入文档"><a href="#emsp-emsp-插入文档" class="headerlink" title="&emsp;&emsp;插入文档"></a>&emsp;&emsp;插入文档</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">post = &#123;</span><br><span class="line">    <span class="string">'author'</span>:<span class="string">'Mike'</span>,</span><br><span class="line">    <span class="string">'text'</span>:<span class="string">'My first blog post!'</span>,</span><br><span class="line">    <span class="string">'tags'</span>:[<span class="string">'mongodb'</span>, <span class="string">'python'</span>, <span class="string">'pymongo'</span>],</span><br><span class="line">    <span class="string">'date'</span>;datetime.datetime.utcnow()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接到test_collection表,插入一条数据</span></span><br><span class="line">collection.insert_one(post)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找一条数据</span></span><br><span class="line">collection.find_one()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定条件查询，如果没有查询到，返回空值</span></span><br><span class="line">collection.find_one(&#123;<span class="string">'author'</span>:<span class="string">'Mike'</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定id查询，需要使用ObjectId()</span></span><br><span class="line">collection.find_one(&#123;<span class="string">'_id'</span>:ObjectId(<span class="string">'_id值'</span>)&#125;)</span><br></pre></td></tr></table></figure><h5 id="emsp-emsp-插入多条数据"><a href="#emsp-emsp-插入多条数据" class="headerlink" title="&emsp;&emsp;插入多条数据"></a>&emsp;&emsp;插入多条数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">new_posts = [&#123;<span class="string">"author"</span>: <span class="string">"Mike"</span>,</span><br><span class="line">              <span class="string">"text"</span>: <span class="string">"Another post!"</span>,</span><br><span class="line">               <span class="string">"tags"</span>: [<span class="string">"bulk"</span>, <span class="string">"insert"</span>],</span><br><span class="line">               <span class="string">"date"</span>: datetime.datetime(<span class="number">2009</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">14</span>)&#125;,</span><br><span class="line">              &#123;<span class="string">"author"</span>: <span class="string">"Eliot"</span>,</span><br><span class="line">               <span class="string">"title"</span>: <span class="string">"MongoDB is fun"</span>,</span><br><span class="line">               <span class="string">"text"</span>: <span class="string">"and pretty easy too!"</span>,</span><br><span class="line">               <span class="string">"date"</span>: datetime.datetime(<span class="number">2009</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">45</span>)&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入多条数据</span></span><br><span class="line">result = collection.insert_many(new_posts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回插入的id</span></span><br><span class="line">result.inserted_ids</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询所有结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> result.find():</span><br><span class="line">    print(i)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#查询指定结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> result.find(&#123;<span class="string">'author'</span>:<span class="string">'Mike'</span>&#125;):</span><br><span class="line">    print(i)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 总数</span></span><br><span class="line">result.count()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定查询的条数</span></span><br><span class="line">result.find(&#123;<span class="string">'author'</span>:<span class="string">'Mike'</span>&#125;).count()</span><br></pre></td></tr></table></figure><h5 id="emsp-emsp-插入或者更新"><a href="#emsp-emsp-插入或者更新" class="headerlink" title="&emsp;&emsp;插入或者更新"></a>&emsp;&emsp;插入或者更新</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果有找到x=1, 则改成x=3,否则插入&#123;‘x':3&#125;,单条</span></span><br><span class="line">result = collection.update_one(&#123;<span class="string">'x'</span>:<span class="number">1</span>&#125;, &#123;<span class="string">'$set'</span>:&#123;<span class="string">'x'</span>:<span class="number">3</span>&#125;&#125;, upsert=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果没有找到x=1,则改成x='mangmang',否则插入&#123;'x':'mangmang'&#125;,多条</span></span><br><span class="line">result = collection.update_many(&#123;<span class="string">'x'</span>:<span class="number">1</span>&#125;, &#123;<span class="string">'$set'</span>:&#123;<span class="string">'x'</span>:<span class="string">'mangmang'</span>&#125;&#125;, upsert=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="Scrapy中POST请求"><a href="#Scrapy中POST请求" class="headerlink" title="Scrapy中POST请求"></a>Scrapy中POST请求</h2><h4 id="Fiddler中抓取到的请求网址在postman中的使用方式："><a href="#Fiddler中抓取到的请求网址在postman中的使用方式：" class="headerlink" title="Fiddler中抓取到的请求网址在postman中的使用方式："></a>Fiddler中抓取到的请求网址在postman中的使用方式：</h4><ol><li><p>首先获取Fiddler中的网址，即右键——&gt;复制——&gt;仅复制URL</p></li><li><p>Postman中，粘贴刚复制好的URL，将请求方式修改为POST</p></li><li><p>然后在Fiddler中获取请求的body字段：TextView中的文本</p></li><li><p>Postman中在body中的raw字段下粘贴复制好的body字段</p></li></ol><h4 id="scrapy框架下的post请求"><a href="#scrapy框架下的post请求" class="headerlink" title="scrapy框架下的post请求"></a>scrapy框架下的post请求</h4><p>在Fiddler左侧的抓取结果区域中显示的则都是该域名相关的抓取结果：</p><img src="/2019/10/29/Python爬虫/1.png"><p><img src="//pl741.github.io/2019/10/29/Python爬虫/1.png" alt="1"></p><p>在该图中可以得到得信息如下：</p><ul><li>请求地址：</li><li>设备信息：deviceinfos</li><li>认证信息：authorization</li><li>请求方式：POST</li><li>媒体类型：Content-Type: application/json;charset=utf-8</li><li>主机信息：host:</li><li>用户标识：User-Agent: okhttp/3.6.0</li></ul><p>经验证，请求得头信息中，authorization、Content-Type、User-Agent是必填项。</p><h4 id="开始构造spider"><a href="#开始构造spider" class="headerlink" title="开始构造spider"></a>开始构造spider</h4><h5 id="1-settings-py："><a href="#1-settings-py：" class="headerlink" title="1. settings.py："></a>1. settings.py：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line">USER_AGENT = <span class="string">" "</span>   <span class="comment"># 你获取的User-Agent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure a delay for requests for the same website (default: 0)</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay</span></span><br><span class="line"><span class="comment"># See also autothrottle settings and docs</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">0</span></span><br><span class="line"><span class="comment"># The download delay setting will honor only one of:</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_DOMAIN = <span class="number">16</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_IP = <span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line"><span class="string">'authorization'</span>: <span class="string">''</span>   <span class="comment"># authorization的值为抓取的值</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'wltx_app.pipelines.BidPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2-定义Item："><a href="#2-定义Item：" class="headerlink" title="2. 定义Item："></a>2. 定义Item：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XXXItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h5 id="3-编写spider"><a href="#3-编写spider" class="headerlink" title="3. 编写spider:"></a>3. 编写spider:</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> XXX.items <span class="keyword">import</span> XXXItem</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> CloseSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XXXSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'XXX'</span></span><br><span class="line">    allowed_domains = [<span class="string">' '</span>]</span><br><span class="line">    Num = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 为当前时间到当天0点的秒数、86400为1天的秒数、北京时间的time.timezone值是-28800，所以是+</span></span><br><span class="line">    zeroPoint = int(time.time()) - int(time.time()-time.timezone) % <span class="number">86400</span> </span><br><span class="line">    <span class="comment"># 昨天0点时间戳</span></span><br><span class="line">    yesterdayZeroPoint = zeroPoint - <span class="number">86400</span> * <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    url = <span class="string">" "</span></span><br><span class="line">    payloads = &#123;<span class="string">" "</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> payload <span class="keyword">in</span> payloads:</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(url, body=json.dumps(payload), callback=self.parse, method=<span class="string">'POST'</span>, headers=&#123;<span class="string">'Content-Type'</span>:<span class="string">'application/json'</span>&#125;, dont_filter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        data = response.text.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">        results = json.loads(data)[<span class="string">'results'</span>][<span class="number">0</span>]</span><br><span class="line">        print(<span class="string">"- "</span>*<span class="number">50</span>, type(results))</span><br><span class="line">        hits = results[<span class="string">'hits'</span>]</span><br><span class="line">        <span class="keyword">for</span> hit <span class="keyword">in</span> hits:</span><br><span class="line">        print(<span class="string">"- "</span>*<span class="number">50</span>, type(hit))</span><br><span class="line">        item = EnsItem()</span><br><span class="line">        last_sale = hit[<span class="string">'last_sale'</span>]</span><br><span class="line">        print(<span class="string">"- "</span>*<span class="number">50</span>, type(last_sale))</span><br><span class="line">        item[<span class="string">'Num'</span>] = self.Num</span><br><span class="line">        <span class="keyword">if</span> last_sale[<span class="string">'timestamp'</span>] &gt;= self.yesterdayZeroPoint:</span><br><span class="line">        item[<span class="string">'name'</span>] = hit[<span class="string">'name'</span>]</span><br><span class="line">        item[<span class="string">'total_price_eth'</span>] = last_sale[<span class="string">'total_price_eth'</span>]</span><br><span class="line">        deal_time = time.strftime(<span class="string">'%Y/%m/%d %H:%M:%S'</span>,time.localtime(last_sale[<span class="string">'timestamp'</span>]))</span><br><span class="line">        item[<span class="string">'timestamp'</span>] = deal_time</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># self.crawler.engine.close_spider(self, '结束爬虫!')</span></span><br><span class="line">        <span class="keyword">raise</span> CloseSpider(<span class="string">'结束爬虫'</span>)</span><br><span class="line">        self.Num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h5 id="4-总结："><a href="#4-总结：" class="headerlink" title="4. 总结："></a>4. 总结：</h5><ol><li><p>headers信息中的Content-Type在settings.py中设置是没有效果的。</p></li><li><p>重写start_requests()方法,中data列表的值必须是str、bytes，不能为int</p></li><li><p>post请求中formdata和body的效果是一样的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.FormRequest(url, method = <span class="string">'POST'</span>, headers = headers, body=json.dumps(data), callback = self.parse_list, dont_filter = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">yield</span> scrapy.FormRequest(url, method = <span class="string">'POST'</span>, headers = headers, formdata=data, callback = self.parse_list, dont_filter = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li><li><p>重复爬取的地址需要设置 dont_filter = True</p></li></ol><h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><h3 id="1-解决scrapy爬虫问题：Connection-was-refused-by-other-side-10061-由于目标计算机积极拒绝，无法连接"><a href="#1-解决scrapy爬虫问题：Connection-was-refused-by-other-side-10061-由于目标计算机积极拒绝，无法连接" class="headerlink" title="1. 解决scrapy爬虫问题：Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接"></a>1. 解决scrapy爬虫问题：Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接</h3><p>参考：<a href="https://blog.csdn.net/Edisonleeee/article/details/90383256" target="_blank" rel="noopener">https://blog.csdn.net/Edisonleeee/article/details/90383256</a></p><p>网络设置中的代理关掉：</p><img src="/2019/10/29/Python爬虫/2.png"><p><img src="//pl741.github.io/2019/10/29/Python爬虫/2.png" alt="1"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;MongoDB&quot;&gt;&lt;a href=&quot;#MongoDB&quot; class=&quot;headerlink&quot; title=&quot;MongoDB&quot;&gt;&lt;/a&gt;MongoDB&lt;/h2&gt;&lt;h4 id=&quot;安装python库：-pip-install-pymongo&quot;&gt;&lt;a href=&quot;#安装p
      
    
    </summary>
    
    
      <category term="Python爬虫" scheme="http://pl741.github.io/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="MongoDB + Scrpay爬虫框架" scheme="http://pl741.github.io/tags/MongoDB-Scrpay%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>《Modeling Vehicle Interactions via Modified LSTM Models for Trajectory Prediction》</title>
    <link href="http://pl741.github.io/2019/10/10/%E3%80%8AModeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction%E3%80%8B/"/>
    <id>http://pl741.github.io/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/</id>
    <published>2019-10-10T07:42:13.000Z</published>
    <updated>2019-10-17T06:31:09.775Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://xs.scihub.ltd/https://ieeexplore.ieee.org/abstract/document/8672889/" target="_blank" rel="noopener">《Modeling Vehicle Interactions via Modified LSTM Models for Trajectory Prediction》</a></p><h3 id="通过改进的LSTM模型对车辆相互作用进行轨迹预测建模"><a href="#通过改进的LSTM模型对车辆相互作用进行轨迹预测建模" class="headerlink" title=" 通过改进的LSTM模型对车辆相互作用进行轨迹预测建模 "></a><center> 通过改进的LSTM模型对车辆相互作用进行轨迹预测建模 </center></h3><h3 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h3><p>长短期记忆（<code>LSTM</code>）模型是最常用的车辆轨迹预测模型之一。在本文中，我们研究了现有的<code>LSTM</code>模型在密集交通中进行长期轨迹预测的两个问题。首先，现有的<code>LSTM</code>模型无法同时描述不同车辆之间的空间相互作用以及轨迹时间序列之间的时间关系。因此，现有的模型不能准确地估计车辆交互在密集交通中的影响。其次，基本的<code>LSTM</code>模型经常会遇到梯度消失的问题，因此很难长期训练。这两个问题有时会导致车辆轨迹预测中存在较大的预测误差。 在本文中，我们提出了一种基于时空<code>LSTM</code>的轨迹预测模型（<code>STLSTM</code>），其中涵盖了两个地方的修改。我们将空间交互嵌入到<code>LSTM</code>模型中，以隐式测量相邻车辆之间的交互。 我们还在两个连续<code>LSTM</code>层的输入和输出之间的引入了<code>shortcut connection</code>，以处理梯度消失问题。在<code>I-80</code>和<code>US-101</code>数据集上评估了提出的新模型。 结果表明，我们的新模型比一种最新模型（<code>maneuver-LSTM</code>（<code>M-LSTM</code>））具有更高的轨迹预测精度。</p><h3 id="INDEX-TERMS"><a href="#INDEX-TERMS" class="headerlink" title="INDEX TERMS"></a>INDEX TERMS</h3><p>轨迹预测     车辆交互作用   <code>Shortcut Connection</code>  <code>LSTM</code>（<code>long short-term memory</code>)</p><h3 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h3><p>自主车辆始终通过各种传感器来感知静态交通设施以及周围车辆和行人的运动，以预测其轨迹以用于将来的运动计划。车辆轨迹预测模型的输入是最近几秒钟内目标车辆的历史轨迹，输出是接下来几秒钟内的预测轨迹。</p><p>根据详细的预测过程，可以将先前工作中使用的预测模型大致分为两种类型：基于<code>Maneuver</code>的模型和端到端模型。</p><h4 id="A-MANEUVER-BASED-MODELS"><a href="#A-MANEUVER-BASED-MODELS" class="headerlink" title="A. MANEUVER-BASED MODELS"></a>A. MANEUVER-BASED MODELS</h4><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/1.png" title="Figure1. 两类轨迹预测模型:(a)基于Maneuver的模型，(b)端到端模型"><p>如图<code>Fig.1(a)</code>所示，基于<code>Maneuver</code>的轨迹预测模型包含两个连续步骤：机动识别步骤和预测步骤 [1] – [4]。机动识别步骤输出中间结果，该中间结果指示预定类型的目标车辆的操纵。例如，在直行的情况下，车辆的运动可以分为两种：直行和换道。</p><p>在机动识别步骤中，常规方法使用SVM<sup> [5]，[6]</sup> 或概率图形模型，例如HMM <sup>[3]，[5]，[7]</sup>，贝叶斯网络<sup>[4]</sup>，随机森林分类器<sup>[8]</sup>。最近的一些工作还使用RNN<sup> [9]，[10]</sup>来提高识别精度。在预测步骤中，常规方法使用原型轨迹方法<sup>[2]，[11]</sup>，基于簇的模型<sup>[3]</sup>，动力学模型<sup>[12]</sup>，多项式模型<sup>[13]-[15]</sup>，高斯过程<sup>[16]，[ 17]</sup>，RRT <sup>[18]</sup>，高斯混合模型<sup>[1]，[19]</sup>等。然而，但是，由于车辆的轨迹是非线性的<sup>[9]</sup>，并且运动模型的特征很复杂，因此许多先前的方法不能用于长期预测<sup>[4]，[20]</sup>。因此，最近的一些工作开始将RNN用于基于长期机动的轨迹预测<sup>[9]</sup>。</p><p>基于<code>Maneuver</code>的模型具有两个优点。首先，车辆的运动可以分解为几个相对容易识别的简洁的动作<sup>[2]</sup>。 其次，预定义的操纵类别与驾驶员的直觉一致。 因此，基于<code>Maneuver</code>的模型通常是可以解释的。</p><p>但是，基于机动的模型有两个缺点，可能导致较大的预测误差。首先，当场景或车辆运动变得复杂时，很难自然地将车辆的运动合理地分为几类。 例如，一些明显不同的轨迹可以被分类为同一操作，但是它们不能通过一个简单的轨迹模型来建模。其次，我们必须为训练过程手动标记轨迹的操作； 然而标签任务既费时又昂贵。 同时错误的标签也可能会增加训练模型的错误。</p><h4 id="B-END-TO-END-MODELS"><a href="#B-END-TO-END-MODELS" class="headerlink" title="B. END-TO-END MODELS"></a>B. END-TO-END MODELS</h4><p>如图<code>Fig.1(b)</code>所示，端到端模型试图跳过机动识别过程并直接执行轨迹预测。一些研究证明，即使没有操纵识别步骤，<code>LSTM</code>模型也能够为车辆或行人的复杂运动建模<sup>[21] – [23]</sup>。此外，使用端到端模型可以避免因不正确的机动划分而导致的错误。</p><p>但是，端到端<code>LSTM</code>模型仍然存在两个问题。首先，基本的<code>LSTM</code>模型不能同时对车辆和轨迹序列之间的空间相互作用进行建模。其次，在训练长期轨迹预测模型时，很难训练基本的<code>LSTM</code>模型。在对长时间序列进行建模时，此类<code>LSTM</code>模型在时间维度上等效于非常深的神经网络。因此，这种<code>LSTM</code>模型在反向传播中可能会遇到梯度消失的问题<sup>[24]</sup>。</p><p>为了解决上述两个问题，我们提出了一种新的模型<code>ST-LSTM</code>，该模型对基本<code>LSTM</code>模型的结构进行了两处结构修改，以进行轨迹预测。</p><p>第一个修改是，我们采用了<code>Structural-RNN（S-RNN）</code><sup>[25]</sup>的想法，并通过新的<code>LSTM</code>模型对所有车辆轨迹和车辆之间的相互作用进行建模。特别地，我们将时间关系和空间相互作用构造为不同的时间序列，并通过<code>LSTM</code>模型分别处理它们。与在时间关系和空间交互作用之间没有明确区分的那些模型（例如<code>S-RNN</code>）相比，新模型更适合交通场景。</p><p>第二个修改是在每个<code>LSTM</code>层的输入和输出之间引入<code>shortcut connections</code>，旨在将历史轨迹的先验信息直接传递到后续层。 这种结构可以减轻反向传播中的梯度消失<sup>[26]</sup>。</p><p>我们在<code>NGSIM I-80</code>和<code>US-101</code>数据集上训练和评估<code>ST-LSTM</code>。 所获得的预测结果比一种最新模型（即[9]中的<code>M-LSTM</code>）的结果具有更高的准确性，这证明了我们所做修改的有效性。</p><p>为了更好地展示我们的发现，本文的其余部分安排如下。 第二节介绍了我们研究的问题，声明了本文的符号并列出了<code>ST-LSTM</code>的预测步骤。 第三节详细介绍了<code>ST-LSTM</code>的结构。 第四节介绍了详细的训练过程，介绍了实验结果并讨论了<code>ST-LSTM</code>的有效性。 最后，第五节总结了论文。</p><h3 id="II-PROBLEM-PRESENTATION-AND-THE-MODEL-问题描述与模型"><a href="#II-PROBLEM-PRESENTATION-AND-THE-MODEL-问题描述与模型" class="headerlink" title="II. PROBLEM PRESENTATION AND THE MODEL 问题描述与模型"></a>II. PROBLEM PRESENTATION AND THE MODEL 问题描述与模型</h3><h4 id="A-THE-FRAMEWORK-OF-ST-LSTM"><a href="#A-THE-FRAMEWORK-OF-ST-LSTM" class="headerlink" title="A. THE FRAMEWORK OF ST-LSTM"></a>A. THE FRAMEWORK OF ST-LSTM</h4><h5 id="1-RESEARCH-SCENARIO-研究场景"><a href="#1-RESEARCH-SCENARIO-研究场景" class="headerlink" title="1) RESEARCH SCENARIO  研究场景"></a>1) RESEARCH SCENARIO  研究场景</h5><p>在本文中，我们研究了密集交通中的端到端长期轨迹预测。这里的“长期”是指该模型能够预测整个非平凡运动（除了直线运动以外的运动）的轨迹，同时保持较低的预测误差。这里的“密集交通”意味着每辆车都可以影响周围车辆的行驶轨迹，但道路并未完全阻塞。在这种情况下，由于周围车辆的影响，车辆的运动变得复杂。</p><p>我们将轨迹预测的目标载体表示为$V_{S}$。我们假设，当且仅当周围的车辆$V_{i}$靠近目标车辆时，它才能影响$V_{s}$的未来运动。如果$V_{i}$和$V_{s}$之间的纵向距离（以及车道）大于$80 m$，我们将忽略$V_{i}$对$V_{s}$的影响。我们仅注意在六个方向（左前，前，右前，右后，后，左，后）的六个最接近的周围车辆，分别表示为$V_{1}〜V_{6}$。参见<code>Fig.2</code>。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/2.png" title="Figure2. ST-LSTM的整体结构和计算过程"><h5 id="2-INPUT-AND-OUTPUT"><a href="#2-INPUT-AND-OUTPUT" class="headerlink" title="2) INPUT AND OUTPUT"></a>2) INPUT AND OUTPUT</h5><p>我们将$V_{s}$和$V_{1}〜V_{6}$的历史轨迹表示为$ \boldsymbol{x}^{t}_{s}$和$\boldsymbol{x}_{i}^{t}(i=1,2, \dots, 6)$。如图<code>Fig2</code>所示，我们的端到端模型的输入为$\boldsymbol{x}^{t}=\left[\boldsymbol{x}_{s}^{t}, \boldsymbol{x}_{1}^{t}, \boldsymbol{x}_{2}^{t}, \ldots, \boldsymbol{x}_{6}^{t}\right]$。输出$\boldsymbol{y}^{t}_{s}$是$V_{s}$的预测轨迹。</p><p>我们将车辆轨迹表示为位置位移序列$[\Delta X, \Delta Y]$。$X$和$Y$分别是横向和纵向的坐标。也就是说，输入和输出是：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}{\boldsymbol{x}_{s}^{t}=\left[\Delta X_{s}^{\left(t-t_{h}+\Delta t\right)}, \Delta Y_{s}^{\left(t-t_{h}+\Delta t\right)}, \ldots, \Delta X_{s}^{t}, \Delta Y_{s}^{t}\right]} \\ {\boldsymbol{x}_{i}^{t}=\left[\Delta X_{i}^{\left(t-t_{h}+\Delta t\right)}, \Delta Y_{i}^{\left(t-t_{h}+\Delta t\right)}, \ldots, \Delta X_{i}^{t}, \Delta Y_{i}^{t}\right]}\end{array}\right.            \tag{1}</script><script type="math/tex; mode=display">\boldsymbol{y}_{s}^{t}=\left[\Delta X_{s}^{(t+\Delta t)}, \Delta Y_{s}^{(t+\Delta t)}, \cdots, \Delta X_{s}^{\left(t+t_{p}\right)}, \Delta Y_{s}^{\left(t+t_{p}\right)}\right]  \tag{2}</script><p>其中$t$是当前时间。$t_{h}$定义了历史时间范围。 $t_{p}$定义了预测时间范围。 $\Delta t$定义预测步长。 $[\Delta X_{s}^{t}; \Delta Y_{s}^{t}]$和$[\Delta X_{i}^{t}; \Delta Y_{i}^{t}]$定义从时间$(t-\Delta t)$到$t$的$V_{s}$和$V_{i}$的位置位移。 预测时间数$n_{p}=t_{p}/\Delta t$。 历史时间数$n_{h}=t_{h}/\Delta t$。</p><h5 id="3-THE-STRUCTURE-OF-ST-LSTM"><a href="#3-THE-STRUCTURE-OF-ST-LSTM" class="headerlink" title="3) THE STRUCTURE OF ST-LSTM"></a>3) THE STRUCTURE OF ST-LSTM</h5><p>我们通过<code>LSTM</code>模型<sup>1</sup>对所有车辆轨迹和所有交互进行建模，并将它们集成到高级结构中。 如图<code>Fig2</code>所示，$V_{i}(V_{s})$轨迹的<code>LSTM</code>模型表示为$C_{i}(C_{s})$。 $V_{i}$和$V_{s}$之间相互作用的<code>LSTM</code>模型表示为$I_{i}$。 为简单起见，图中仅显示<code>LSTM</code>模型的一部分。</p><h4 id="B-A-THREE-STEP-TRAJECTORY-PREDICTION-三步轨迹预测"><a href="#B-A-THREE-STEP-TRAJECTORY-PREDICTION-三步轨迹预测" class="headerlink" title="B. A THREE-STEP TRAJECTORY PREDICTION 三步轨迹预测"></a>B. A THREE-STEP TRAJECTORY PREDICTION 三步轨迹预测</h4><p>如图<code>Fig2</code>所示，我们将整个轨迹预测过程分为三个步骤：</p><p><strong>Step 1:</strong> 通过$C_{i}（C_{s}）$初步预测$V_{i}（V_{s}）$的轨迹。</p><p>$C_{i}（C_{s}）$的输入是历史轨迹$\boldsymbol{x}^{t}_{i}（\boldsymbol{x}^{t}_{s}）$。输出表示为$\boldsymbol{h}^{t}_{i}（\boldsymbol{h}^{t}_{s}）$，是$V_{i}（V_{s}）$的初步预测轨迹：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}{\boldsymbol{h}_{i}^{t}=\left[\Delta X_{i}^{(t+\Delta t)}, \Delta Y_{i}^{(t+\Delta t)}, \ldots, \Delta X_{i}^{\left(t+t_{p}\right)}, \Delta Y_{i}^{\left(t+t_{p}\right)}\right]} \\ {\boldsymbol{h}_{s}^{t}=\left[\Delta X_{s}^{(t+\Delta t)}, \Delta Y_{s}^{(t+\Delta t)}, \ldots, \Delta X_{s}^{\left(t+t_{p}\right)}, \Delta Y_{s}^{\left(t+t_{p}\right)}\right]}\end{array}\right.  \tag{3}</script><p><strong>Step 2:</strong> 通过$I_{i}$评估空间相互作用，并输出$V_{s}$的轨迹校正序列。</p><p>$I_{i}$的输入是$\boldsymbol{h}^{t}_{s}$和$\boldsymbol{h}^{t}_{i}$。输出$\boldsymbol{h}^{t}_{i,s}$是$\boldsymbol{h}^{t}_{s}$的轨迹校正序列，它考虑了$V_{i}$和$V_{s}$之间的空间相互作用对$V_{s}$运动的影响。 $\boldsymbol{h}^{t}_{i, s}$，$\boldsymbol{h}^{t}_{s}$和$\boldsymbol{h}^{t}_{i}$的长度都等于$2n_{p}$。</p><p><strong>Step 3:</strong> 校正$V_{s}$的预测轨迹并输出最终的预测轨迹。</p><p>此步骤的输入为$\boldsymbol{h}^{t}_{s}$和所有$\boldsymbol{h}^{t}_{i, s}$，最终输出预测序列$y^{t}_{s}$由输入的加权和计算得出：</p><script type="math/tex; mode=display">\boldsymbol{y}_{s}^{t}=\boldsymbol{h}_{s}^{t}+\sum_{i} w_{i} \boldsymbol{h}_{i,s}^{t}=\boldsymbol{h}_{s}^{t}+\left(\boldsymbol{W}^{t}\right)^{T} \boldsymbol{H}^{t}      \tag{4}</script><p>其中$\boldsymbol{W}^{t}=\left[w_{1}, w_{2}, \cdots, w_{6}\right]^{T}$，$\boldsymbol{H}^{t}=\left[\boldsymbol{h}_{1, s}^{t}, \boldsymbol{h}_{2, s}^{t}, \cdots, \boldsymbol{h}_{6,s}^{t}\right]^{T}$</p><p>权重$w_{i}$反映了$I_{i}$对$V_{s}$未来轨迹的影响程度。我们可以从安全距离的角度估计$w_{i}$，因为每辆车都会主动保持与周围车辆的安全距离，从而产生空间相互作用。</p><p>在本文中，我们通过[27]，[28]中提出的安全间隙公式来测量安全距离。令$V_{l}（V_{f}）$为$V_{i}$和$V_{s}$之中的领先（跟随）车辆。两辆车之间的安全距离为：</p><script type="math/tex; mode=display">D_{i, s}=v_{f} \rho+\frac{\left(v_{f}\right)^{2}-\left(v_{l}\right)^{2}}{2 a_{b r a k e}}+L=\frac{\bar{v} \Delta v}{a_{b r a k e}}+\left(v_{f} \rho+L\right)  \tag{5}</script><p>其中$v_{l}（v_{f}）$是$V_{l}（V_{f}）$的纵向速度。 $L$是两辆车的平均长度。 $ρ$是驾驶员的平均响应时间。 $a_{brake}$是两辆车的制动减速度。平均速度$\bar{v}=\left(v_{f}+v_{l}\right) / 2$，相对速度$\Delta v=v_{f}-v_{l}$。</p><p>该安全距离设想了一个汽车跟随的情况，先行车辆通过$a_{brake}$制动直到完全停止，随后的车辆在响应时间$ρ$内保持均速，然后通过$a_{brake}$制动直到完全停止。该公式可被视为安全距离的下限，因为当两辆车的距离小于$D_{i,s}$时，随后的车可能无法通过制动避免碰撞。</p><p>我们假设$α = 1/a_{brake}$和$β=v_{2}ρ+ L$，并进一步简化$β$为常数。我们考虑以下一种权重的先验知识：</p><script type="math/tex; mode=display">w_{i} \propto \frac{D_{i, s}}{\Delta d_{i, s}}=\frac{\alpha \bar{v} \Delta v+\beta}{\Delta d_{i, s}} \tag{6}</script><p>其中$\Delta d_{i, s}$定义了$V_{i}$和$V_{s}$之间的纵向距离。 在训练过程中需要确定常数项$α$和$β$。 在执行<strong>Step 3</strong>之前，应将所有$w_{i}$标准化。</p><p>在公式（6）中，$\bar{v}$或$\Delta v$的增加和$\Delta d_{i, s}$的减少将导致$w_{i}$的增加。 这表明$V_{i}$将对$V_{s}$产生更大的影响，这与我们的直觉是一致的。</p><p>如果$V_{i}$不存在或$\Delta d_{i, s}$太大，我们可以直接设置$w_{i}=0$，这样$V_{i}$的缺乏将不会影响预测过程。 因此，ST-LSTM可以应用于具有任意数量的周围车辆的车辆，反映了ST-LSTM的灵活性。</p><h3 id="III-MODEL-DESIGN"><a href="#III-MODEL-DESIGN" class="headerlink" title="III. MODEL DESIGN"></a>III. MODEL DESIGN</h3><h4 id="A-THE-STRUCTURE-AND-FEATURES-OF-ST-LSTM"><a href="#A-THE-STRUCTURE-AND-FEATURES-OF-ST-LSTM" class="headerlink" title="A. THE STRUCTURE AND FEATURES OF ST-LSTM"></a>A. THE STRUCTURE AND FEATURES OF ST-LSTM</h4><p>为了使<code>LSTM</code>模型能够同时对车辆之间的空间相互作用和轨迹序列之间的时间关系进行建模，Jain等人。在[25]中提出了一种高级时空模型，称为S-RNN（S-RNN）。关键思想是将空间相互作用视为可以由<code>LSTM</code>模型处理的时间序列。</p><p>如<code>Fig2</code>所示，我们采用<code>S-RNN</code>的思想，并类似地构建时空模型。但是，<code>ST-LSTM</code>和<code>S-RNN</code>之间有两个主要区别：</p><p>第一个区别是$C_{i}$和$I_{i}$（$\boldsymbol{h}^{t}_{i}$和$\boldsymbol{h}^{t}_{i, s}$）的输出定义。 在<code>S-RNN</code>中，未指定$C_{i}$和$I_{i}$的输出。 相反，<code>ST-LSTM</code>清楚地区分了$\boldsymbol{h}^{t}_{i}$（时间关系）和$\boldsymbol{h}^{t}_{i, s}$（空间相互作用），以突出它们在轨迹预测中的不同作用。 这是因为时间关系通常主导着整个驾驶行为，而不是空间相互作用。</p><p>第二个差异是<strong>Step 3</strong>中的权重$\boldsymbol{W}^{t}$。在S-RNN中，所有$I_{i}$的输出都通过在训练过程中直接学习的加权总和进行积分。 不同地，我们根据等式（4）-（6）中所示的相互作用影响程度的先验知识来评估$w_{i}$。 引入这些先验知识也将有助于加速学习。</p><p>根据比较，ST-LSTM比S-RNN更适合表征交通场景。</p><h4 id="B-THE-SHORTCUT-CONNECTIONS-FOR-LSTM-MODELS"><a href="#B-THE-SHORTCUT-CONNECTIONS-FOR-LSTM-MODELS" class="headerlink" title="B. THE SHORTCUT CONNECTIONS FOR LSTM MODELS"></a>B. THE SHORTCUT CONNECTIONS FOR LSTM MODELS</h4><p>由于基本的LSTM模型很难在长期轨迹预测中进行训练，因此我们对基本LSTM模型的结构进行了一些修改，如图3所示。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/3.png" title="Figure3. 带有shortcut connections的修改后的LSTM模型结构。"><p>受ResNet [26]的启发，我们引入了<code>shortcut connections</code>来解决梯度消失的问题。输入序列的信息可以直接传输到后续的层。因此，修改后的<code>LSTM</code>模型可以减轻反向传播时的梯度消失。换句话说，修改后的<code>LSTM</code>模型的学习对象变成了类似残差的序列，而不是轨迹序列。我们将证明此更改可以使<code>LSTM</code>在<code>IV-D</code>部分中更容易训练。</p><p>为了构造<code>shortcut connections</code>，输入和输出序列的长度应相等（$n=2n_{h}$）。 <code>LSTM</code>层的数量（<code>Fig3</code>中的$m$）和每层中的<code>LSTM</code>单元的数量（<code>Fig3</code>中的$n$）是修改后的<code>LSTM</code>模型的两个主要结构参数。</p><h4 id="C-NETWORK-PARAMETER-SELECTION"><a href="#C-NETWORK-PARAMETER-SELECTION" class="headerlink" title="C. NETWORK PARAMETER SELECTION"></a>C. NETWORK PARAMETER SELECTION</h4><p>现在，我们专注于每个<code>LSTM</code>（$C_{i}$和$I_{i}$）的详细网络结构。 具体来说，我们需要设置历史时间范围$t_{h}$（$n=2n_{h}=2t_{h}/\Delta t$）和隐藏的<code>LSTM</code>层数$N_{hidden}$（$m=N_{hidden}$）。</p><p>$t_{h}$和$N_{hidden}$的值可以直接影响修改后的<code>LSTM</code>模型的性能。 通常，当$t_{h}$太小时，它无法提供足够的有效历史信息来进行轨迹预测； 当$t_{h}$太大时，冗余的历史信息可能会降低预测的准确性。 当$N_{hidden}$太小时，<code>LSTM</code>模型可能无法对复杂的车辆轨迹进行建模。 当$N_{hidden}$太大时，网络可能太深且收敛速度较慢。</p><p>所有$C_{i}$和$I_{i}$的详细网络参数选择如附录A所示。选择结果如<code>IV-B</code>节所示。</p><h3 id="IV-TRAINING-RESULTS-AND-DISCUSSIONS"><a href="#IV-TRAINING-RESULTS-AND-DISCUSSIONS" class="headerlink" title="IV. TRAINING RESULTS AND DISCUSSIONS"></a>IV. TRAINING RESULTS AND DISCUSSIONS</h3><h4 id="A-DATASET"><a href="#A-DATASET" class="headerlink" title="A. DATASET"></a>A. DATASET</h4><p>本文的研究场景是一条多车道的直路。在这种情况下，我们可以将车辆运动分为两种：直行和换道。</p><p>我们使用<code>NGSIM I-80</code>和<code>US-101</code>轨迹数据集来训练和评估ST-LSTM。</p><p><code>I-80</code>数据集于2005年4月13日在加利福尼亚州埃默里维尔的80号州际公路的东行方向上收集，其中包含下午4:00到下午4:15，下午5:00到下午5:15以及下午5:15到下午5:30。</p><p><code>US-101</code>数据集于2005年6月15日在加利福尼亚州洛杉矶的<code>US Highway 101</code>的南行方向上收集，其中包含从7:50 AM至8:05 AM，8:05 AM至8:20 AM，8:20 AM至8:35 AM。这两个数据集的实验场景都是交通密集的直行道路，其中包括大量的车道变换和跟随汽车的运动。因此，数据集非常适合我们在本文中研究的场景。</p><p>在<code>I-80</code>数据集中，在三个时间段内分别收集了2052、1836、1790个轨迹。这三个数据集分别包含1025、913、945个成功的车道变更。在US-101数据集中，在三个时间段内收集了2169、2017、1915个轨迹，分别包含1006、660、657个成功的车道变换。每个数据帧都包括车辆的位置，速度，偏航角，大小等。数据集的采样频率为10 Hz，因此我们在本文中设置$\Delta t=0.1s$。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/4.png" title="Table 1. I-80中不同类型轨迹的统计信息。"><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/5.png" title="Table 2. US-101中不同类型轨迹的统计数据。"><p>但是，如表1和表2所示，两个数据集严重不平衡。 因此，我们仅对<strong>left only</strong>和<strong>no lane change</strong>进行降采样，而对<strong>right only</strong>进行过采样，以确保不同类型轨迹的比例大致相等[29]。</p><p>为了便于学习车道变更的特性，我们使用车道变更来修剪每个轨迹，并且仅保留包含车道变更的部分数据。 我们还在不改变车道的情况下随机修整了轨迹，以保证不同类型轨迹的平衡。</p><p>我们从平衡数据集中随机抽取N条轨迹（表1和表2中所示）进行训练，剩下的轨迹进行测试。 我们将在附录B中讨论训练集大小<code>N</code>的选择。选择结果在<code>IV-B</code>节中显示。</p><h4 id="B-TRAINING-DETAILS"><a href="#B-TRAINING-DETAILS" class="headerlink" title="B. TRAINING DETAILS"></a>B. TRAINING DETAILS</h4><p>根据附录A中的参数选择，我们为所有$C_{i}$和$I_{i}$设置$N_{hidden} =3$和$t_{h} =3s$。也就是说，所有的$C_{i}$和$I_{i}$都有三个隐藏层，每个层都有60个<code>LSTM</code>单元。根据附录B中训练集规模的选择，我们分别采用临界规模（具有580条轨迹）和较大规模（具有1350条轨迹）来训练ST-LSTM模型。</p><p>由于大多数车辆具有相似的轨迹特性，因此<code>ST-LSTM</code>中的所有$C_{i}$都共享一个公共<code>LSTM</code>模型。相反，由于周围车辆在不同方向上对$V_{s}$的影响是不同的，因此每个$I_{i}$都是独立训练的，彼此之间不共享网络参数。</p><p>使用<code>Tensorflow</code>对<code>ST-LSTM</code>进行了逐步训练。我们首先对$C_{i}$进行预训练，然后根据$C_{i}$对$I_{i}$进行预训练。所有的预训练LSTM模型都以$0.001$的初始学习率进行了约$20k$次迭代的训练。最后，将所有预训练的LSTM模型进行组合并训练约$5k$次迭代，以训练等式（6）的常数项$α$和$β$。</p><p>在训练过程中，每个训练步骤的批次大小$N_{b}$设置为$50×50$，即每个批次含有50条轨迹，并且每个轨迹序列的长度为$50$（即$5 s$）。 所有网络参数都是随机初始化的。 与其他参数不同，$α$和$β$具有特定的物理含义，但是我们测试了$α$和$β$的初始化值对整体训练结果的影响很小。 我们在训练过程中使用50％的随机失活。 优化器采用<code>ADAM</code>优化器。 训练过程的损失函数在预测位移序列$\boldsymbol{y}_{s}^{t}$和地面真实位移序列$\boldsymbol{y}^{t}$之间采用$MSE$（均方误差）。</p><script type="math/tex; mode=display">\boldsymbol{y}^{t}=\left[\Delta \hat{X}^{(t+\Delta t)}, \Delta \hat{Y}^{(t+\Delta t)}, \cdots, \Delta \hat{X}^{\left(t+t_{p}\right)}, \Delta \hat{Y}^{\left(t+t_{p}\right)}\right]  \tag{7}</script><p>每条轨迹的$MSE$损失函数为：</p><script type="math/tex; mode=display">\operatorname{loss}=\sum_{i=1}^{n_{p}}\left(\delta_{i}^{t}\right)^{T} \delta_{i}^{t}  \tag{8}</script><p>其中$\delta_{i}^{t}=\left[a\left(\Delta X^{(t+i \Delta t)}-\Delta \hat{X}^{(t+i \Delta t)}), \Delta Y^{(t+i \Delta t)}-\right. \Delta \hat{Y}^{(t+i \Delta t)}\right]^{T} $。我们在本文中设置常数$a =10$来放大横向误差对损失函数的影响。</p><p>总体MSE损失函数为：</p><script type="math/tex; mode=display">\text {Loss}=\frac{1}{N_{b}} \sum \text {loss}  \tag{9}</script><p>其中$N_{b}$是训练数据集中的轨迹总数。</p><h4 id="C-TESTING-RESULTS"><a href="#C-TESTING-RESULTS" class="headerlink" title="C. TESTING RESULTS"></a>C. TESTING RESULTS</h4><p>我们比较以下模型之间的$RMS$预测误差：<code>Mabeuver-LSTM</code>（<code>M-LSTM</code>，在[9]中提出），由$580$条轨迹训练的<code>ST-LSTM</code>（表示为<code>ST-LSTM-580</code>）和由$1350$条轨迹训练的<code>ST-LSTM</code>（表示为<code>ST-LSTM-1350</code>）。根据我们的统计，车道变更过程的平均时间为$4$到$5s$，因此我们将预测范围设置为$t_{p} =6s$。由于$t_{p}&gt; t_{h}$，我们应该重复几次预测过程以执行更大范围的轨迹预测。</p><p>在此，根据预测轨迹和地面真实轨迹计算出$RMS$预测误差。因此，我们需要累加位移序列$\boldsymbol{y}^{t}_{s}$和$\boldsymbol{y}^{t}$以获得轨迹表示$z^{t}_{s}$和$z^{t}$：</p><script type="math/tex; mode=display">z_{s}^{t}=\left[X^{(t+\Delta t)}, Y^{(t+\Delta t)}, \cdots, X^{\left(t+t_{p}\right)}, Y^{\left(t+t_{p}\right)}\right]\tag{10}</script><script type="math/tex; mode=display">z^{t}=\left[\hat{X}^{(t+\Delta t)}, \hat{Y}^{(t+\Delta t)}, \cdots, \hat{X}^{\left(t+t_{p}\right)}, \hat{Y}^{\left(t+t_{p}\right)}\right]\tag{11}</script><script type="math/tex; mode=display">\left\{\begin{array}{l}{X^{(t+i \Delta t)}=\sum_{j=1}^{i} \Delta X^{(t+j \Delta t)}} \\ {Y^{(t+i \Delta t)}=\sum_{j=1}^{i} \Delta^{(t+j \Delta t)}} \\ {\hat{X}^{(t+i \Delta t)}=\sum_{j=1}^{i} \Delta \hat{X}^{(t+j \Delta t)}} \\ {\hat{Y}^{(t+i \Delta t)}=\sum_{j=1}^{i} \Delta \hat{Y}^{(t+j \Delta t)}}\end{array}\right.  \tag{12}</script><p>因此，在给定预测范围$P$的情况下，每个单个轨迹的$RMS$预测误差为：</p><script type="math/tex; mode=display">r m s^{P}=\left(\frac{1}{P} \sum_{i=1}^{P}\left(\sigma_{i}^{t}\right)^{T} \sigma_{i}^{t}\right)^{\frac{1}{2}}\tag{13}</script><script type="math/tex; mode=display">R M S^{P} =\frac{1}{N_{t}} \sum r m s^{P} \tag{14}</script><p>其中$\sigma_{i}^{t}=\left[X^{(t+i \Delta t)}-\hat{X}^{(t+i \Delta t)}, Y^{(t+i \Delta t)}-\hat{Y}^{(t+i \Delta t)}\right]^{T}$。$N_{t}$是测试数据集中的轨迹总数。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/6.png" title="Table 3.预测误差的RMS值。"><p>$RMS$值显示在表3中，该值说明<code>ST-LSTM-580</code>和<code>ST-LSTM-1350</code>均胜过<code>M-LSTM</code>。从<code>ST-LSTM-580</code>和<code>ST-LSTM-1350</code>之间的比较中，我们可以发现后者模型使用了较大的训练集，但预测准确率仅得到了一点改进。这种现象证明了“临界点”的有效性，它可以指导我们适当地减少训练量，从而损失一点预测精度，以换取训练速度的显着提高。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/7.png" title="Fig. 4 速度偏差的平均值和标准偏差的统计数据（I-80数据集上的ST-LSTM-1350）。"><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/8.png" title="Fig. 5 位置偏差的平均绝对偏差（I-80数据集上的ST-LSTM-1350）。"><p>为了分析轨迹预测误差的主要来源，我们计算了不同预测范围内速度偏差（表示为$VD_{i}$）的平均值和标准偏差，如Fig. 4所示。在Fig. 5中绘制了不同预测范围的位置偏差（表示为$PD_{i}$）的平均绝对偏差。$VD_{i}$和$PD_{i}$的定义为：</p><script type="math/tex; mode=display">V D_{i}=\frac{1}{\Delta t}\left[\Delta X^{(t+i \Delta t)}-\Delta \hat{X}^{(t+i \Delta t)}, \Delta Y^{(t+i \Delta t)}-\Delta \hat{Y}^{(t+i \Delta t)}\right]^{T}\tag{15}</script><script type="math/tex; mode=display">P D_{i}=\sigma_{t}^{i}\tag{16}</script><p>$Fig. 4$显示了不同预测水平下$VD_{i}$的分布大致相同。速度误差的累积极大的影响了总的长期预测误差。这就是我们使用位置误差的$RMS$评估<code>ST-LSTM</code>的原因。</p><p>当预测范围变得太大时，平均绝对偏差会增加，并且由于误差累积，预测会变得非常不稳定。从另一个角度看，历史轨迹不足以为大时间间隔的预测提供有效的信息。因此，本文选择的预测范围（$6s$）已经足够大。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/9.png" title="Fig. 6 一个典型情况下的几个快照的预测轨迹和地面真实轨迹的示意图：(a) T=0s, (b) T=0.5s, (c) T=1s, (d) T=1.5 s, (e) T=2s, (f) T=4s。"><p>为了更直观地显示轨迹预测结果，我们随机选择车道变化轨迹，并在整个车道变化过程中观察预测结果。如图$6$所示，我们在此时间间隔中拍摄了六个快照。图$6(a)(b)$说明了预测的轨迹在车道变更过程的最开始还没有呈现出车道变更的特征。图$6(c)-(e)$中的预测轨迹显示了车道变化的典型特征。图$6(f)$说明了预测的轨迹在车道变化结束时变回直线。</p><h4 id="D-THE-ROLE-OF-SHORTCUT-CONNECTIONS"><a href="#D-THE-ROLE-OF-SHORTCUT-CONNECTIONS" class="headerlink" title="D. THE ROLE OF SHORTCUT CONNECTIONS"></a>D. THE ROLE OF SHORTCUT CONNECTIONS</h4><p>在训练过程中，我们发现修改后的<code>LSTM</code>模型更容易训练，并且可以将$RMS$误差降低到较低的值。</p><p>在一个实验中，我们通过基本的LSTM模型训练了另一个$C_{s}$，并将其与$IV-C$节中的$C_{s}$进行了比较。 除了<code>shortcut connections</code>外，这两个模型的其他部分都相同，例如输入和输出，$N_{hidden}$，$t_{h}$，训练过程等。在训练过程中没有人工干预。 损失函数的值绘制在$Fig. 7$中，表明修改后的LSTM模型收敛速度快于基本LSTM模型。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/10.png" title="Fig. 7 训练过程中两个模型的损失变化。"><h3 id="V-CONCLUSION"><a href="#V-CONCLUSION" class="headerlink" title="V. CONCLUSION"></a>V. CONCLUSION</h3><p>在本文中，我们提出了一种用于密集交通的新颖的端到端长期轨迹预测模型。 为了解决密集交通中长期预测的两个问题，我们通过添加<code>shortcut connections</code>来修改经典<code>LSTM</code>模型，并通过修改后的<code>LSTM</code>模型对所有空间相互作用和时间关系进行建模。 最后，我们构建了一个时空轨迹预测模型。 在<code>NGSIM I-80​</code>和<code>US-101</code>上进行的实验证明，与最新模型（<code>M-LSTM</code>）相比，<code>ST-LSTM</code>可以获得更精确的轨迹预测。</p><p>但是，我们仅在<code>NGSIM I-80</code>和<code>US-101</code>上测试<code>ST-LSTM</code>，它们在单个简单场景中是相对较小的数据集。 在将来的工作中，我们将收集更多数据并在更复杂的情况下评估<code>ST-LSTM</code>。</p><h3 id="APPENDIX-A"><a href="#APPENDIX-A" class="headerlink" title="APPENDIX A"></a>APPENDIX A</h3><h5 id="NETWORK-PARAMETER-SELECTION"><a href="#NETWORK-PARAMETER-SELECTION" class="headerlink" title="NETWORK PARAMETER SELECTION"></a>NETWORK PARAMETER SELECTION</h5><p>我们通过$N_{\text {hidden}} \in\{2,3,4\}$和$t_{h} \in\{2s,3s,4s,5s\}$来训练$C_{i}$。在本实验中，我们在<code>I-80</code>数据集上随机选择一个轨迹数量$N =1200$的训练集。详细的训练过程与第<code>IV-B</code>节中所描述的相同。我们对训练集采用10倍交叉验证，以计算不同预测范围内的均方根误差（$RMS$)。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/11.png" title="Fig. 8 比较具有不同$N_{hidden}$和$t_{h}$的修改LSTM模型之间的RMS误差: (a) $N_{hidden} = 2$，(b) $N_{hidden} =3$, (c) $N_{hidden}= 4$。"><p>如图8所示，我们比较了这些模型在不同预测范围内的RMS误差。 我们选择最佳组合（$t_{h} =3s$和$N_{hidden} =3$）作为$C_{i}$的参数，因为具有该组参数的模型的RMS误差几乎总是最低的（$t_{p} =2s$时第二低）。 我们还测试了更深的网络和更大$t_{h}$的网络，但是这些模型的性能并不比所选模型更好。</p><p>由于模型结构的限制，$I_{i}$的$t_{h}$应该等于$C_{i}$的$t_{h}$。 我们重复前面的步骤，发现$N_{hidden} =3$对于$I_{i}$也是一个很好的值。 </p><p>同样，我们尝试使用不同的训练集大小和不同的数据集（即<code>US-101</code>），发现上述$t_{h}$和$N_{hidden}$值也相对适合这些训练集。 因此，我们在<code>ST-LSTM</code>中为所有$C_{i}$和$I_{i}=3$和$N_{hidden} =3$。</p><h3 id="APPENDIX-B"><a href="#APPENDIX-B" class="headerlink" title="APPENDIX B"></a>APPENDIX B</h3><h5 id="OVERFITTING-ANALYSIS-AND-TRAINING-SET-SCALE-SELECTION"><a href="#OVERFITTING-ANALYSIS-AND-TRAINING-SET-SCALE-SELECTION" class="headerlink" title="OVERFITTING ANALYSIS AND TRAINING SET SCALE SELECTION"></a>OVERFITTING ANALYSIS AND TRAINING SET SCALE SELECTION</h5><p>深度学习模型（包括ST-LSTM）需要大量的数据进行训练。训练数据不足会导致拟合过度（泛化能力低），但是过多的数据也无法无限地提高模型泛化能力。特别是，通常很难获得用于深度学习的大规模数据集，因此我们需要找到数据量的“临界值”，以在提高模型泛化能力和减少训练数据需求之间达成折衷。该临界点也可以视为有效训练集的最小大小。</p><p>在本文中，我们分别使用不同大小的训练集来训练<code>ST-LSTM</code>模型，并计算它们的训练和测试误差（具有不同的预测范围），以观察模型是否过度拟合。</p><p>我们以<code>US-101</code>数据集为例。根据<code>IV-A</code>节，平衡数据集总共包含$1919$条有效轨迹。我们分别随机采样$40、95、190、380、580、960、1350、1540$轨迹（约占总数据集的$2％，5％，10％，20％，30％，50％，70％，80％$）进行训练，并在图9中绘制训练和测试误差。网络参数遵循附录A的结果。详细的训练过程遵循<code>IV-B</code>节）。</p><img src="/2019/10/10/《Modeling-Vehicle-Interactions-via-Modified-LSTM-Models-for-Trajectory-Prediction》/12.png" title="Fig. 9 .通过不同规模的训练集训练的ST-LSTM模型之间的训练和测试误差（不同预测范围）的比较。"><p>$Fig.9$说明了当训练集太小时，训练得到的<code>ST-LSTM</code>会产生严重的过拟合。随着轨迹数量的增加，过度拟合现象会减弱，而当训练集包含$580$多个轨迹时，这种过度拟合现象几乎可以消除。之后，尽管训练集数量大大增加，但是模型泛化能力仅略有改善。因此，我们认为$580$条轨迹是适合于我们的<code>ST-LSTM</code>模型的训练量。在其他数据集（例如<code>I-80</code>）上训练<code>ST-LSTM</code>模型时，我们构建了具有$580$条随机轨迹的训练集，并且还可以获得训练较好的模型。</p><p>上面的方法探讨了模型的“边际”性能，即对于特定模型，“临界点”代表我们至少应使用多少训练数据进行训练。我们认为，我们上面提出的方法值得推广。基于“临界点”，我们可以直接比较不同深度网络模型对训练数据的需求。当我们的训练数据规模有限时，我们可以基于该指标选择在此条件下具有更强泛化能力（并且不能过度拟合）的模型，并获得更好的性能。确定模型后，还可以根据该指标设置合理的训练集规模，避免训练数据冗余造成的计算资源浪费。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://xs.scihub.ltd/https://ieeexplore.ieee.org/abstract/document/8672889/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Modeling Vehicle I
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://pl741.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="LSTM" scheme="http://pl741.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>《Long Short Term Memory Networks for Anomaly Detection in Time Series》</title>
    <link href="http://pl741.github.io/2019/09/19/%E3%80%8ALong-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series%E3%80%8B/"/>
    <id>http://pl741.github.io/2019/09/19/《Long-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series》/</id>
    <published>2019-09-19T01:24:08.000Z</published>
    <updated>2019-09-19T11:52:34.929Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf" target="_blank" rel="noopener">Long Short Term Memory Networks for Anomaly Dection in Time Series</a></p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>&emsp;&emsp;长短期记忆（LSTM）网络已被证明对于学习包含未知长度的长期模式的序列特别有用，因为它们能够维持长期记忆。在这样的网络中堆叠循环隐藏层还能够学习更高级别的时间特征，以便通过更稀疏的表示来更快地学习。在本文中，我们使用堆叠式LSTM（Stacked LSTM）网络进行时间序列中的异常检测。网络在非异常数据上进行训练，并用作多个时间步长的预测器。预测误差被建模为多变量高斯分布，用于评估异常行为的可能性。此方法的有效性在四个数据集上得到证实：ECG（心电图），航天飞机，电力需求和多传感器引擎数据集。</p><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><p>&emsp;&emsp;传统的过程监控技术在时间窗<sup>[1]</sup>上使用统计测量值，如累积和（CUSUM）和指数加权移动平均值（EWMA），以检测基础分布的变化。该时间窗口的长度通常需要预先确定，同时结果很大程度上取决于该参数。LSTM神经网络<sup>[2]</sup>通过采用乘法门来克服递归神经网络（RNN）所遇到的梯度消失问题。<u>该乘法门能保证常量错误一定从“内存单元”的内部状态通过</u>。输入$(IG)$，输出$(OG)$和遗忘$(FG)$门防止记忆内容被不相关的输入和输出扰动（参见$Fig.1(a)$），从而允许长期记忆存储。由于能够在序列中学习长期相关性，LSTM网络避免了对预先指定时间窗口的需要，并且能够精确地建模复杂的多变量序列。在本文中，我们通过堆叠式LSTM网络对时间序列的正常行为进行建模，我们可以准确地检测出与正常行为的偏差，而无需任何预先指定的上下文窗口或预处理。</p><img src="/2019/09/19/《Long-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series》/1.png" title="$Fig.1:(a)$长短期记忆单元$(b)$堆叠结构"><p>&emsp;&emsp;已经表明，在网络中$Sigmoid$激活单元的堆叠循环隐藏层能更轻易地捕获时间序列的结构，并允许在不同的时间尺度处理时间序列<sup>[3]</sup>。使用分层时间处理技术进行异常检测的一个值得注意的例子是分层时间记忆（HTM）系统，它试图模仿新皮层中细胞，区域和水平的层次结构<sup>[4]</sup>。此外，像[5,6]这样的时间异常检测方法学习预测时间序列并使用预测误差来检测新颖性。然而，据我们所知，LSTM提供的记忆性功能尚未与循环分层处理层相结合，以用于预测时间序列并进行异常检测。</p><p>&emsp;&emsp;如[5]中所述，我们使用预测器建模正常行为，然后使用预测误差来识别异常行为。（这在现实世界的异常检测场景中特别有用，在这种场景中，正常行为的实例可能很多，但异常行为的实例很少见。）为了确保网络捕获序列的时间结构，我们预测未来的几个时间步骤。因此，序列中的每个点具有在过去的不同点处产生的多个对应的预测值，从而产生多个误差值。然后使用预测正常数据时的错误概率分布来获得在测试数据上正常行为的可能性。当控制变量（例如车辆加速器或制动器）也存在时，除了因变量之外，网络还用来预测控制变量。这迫使网络通过控制和相关传感器变量预测误差的联合分布来学习正常的使用模式：结果，当控制输入改变时已经捕获了明显的预测误差，并且不会有助于声明异常。</p><p>&emsp;&emsp;本文的其余部分安排如下：第2节描述了我们的方法。在第3节中，我们使用堆叠式LSTM方法（LSTM-AD）以及循环$Sigmoid$单元的堆叠RNN方法（RNN-AD），在四个真实世界数据集上呈现时间异常检测结果。第4节提供结论性意见。</p><h3 id="2-LSTM-AD：基于LSTM的异常检测"><a href="#2-LSTM-AD：基于LSTM的异常检测" class="headerlink" title="2. LSTM-AD：基于LSTM的异常检测"></a>2. LSTM-AD：基于LSTM的异常检测</h3><p>&emsp;&emsp;考虑一个时间序列$X=\left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(n)}\right\}$，时间序列中的每个点$\mathbf{x}^{(t)} \in R^{m}$是一个$m$维的向量$\left\{x_{1}^{(t)}, x_{2}^{(t)}, \ldots, x_{m}^{(t)}\right\}$，，这些元素对应于输入变量。预测模型学习去预测输入变量$d$ $(s.t. 1 \leq d \leq m)$的接下来$l$个值。正常序列$(s)$被分为四组：正常训练集$(s_{N})$，正常验证集-1$(v_{N1})$，正常验证集-2$(v_{N2})$和正常测试集$(t_{N})$。异常序列$(s)$被分为两组：异常验证集$(v_{A})$和异常测试集$(t_{A})$。我们首先使用堆叠式LSTM网络学习一个预测模型，然后计算我们检测到异常的预测误差分布：</p><p>&emsp;&emsp;<strong>基于堆叠式LSTM的预测模型：</strong>我们考虑以下LSTM网络架构：对于$m$个维度中的每一个维度，我们都会对输入层放置一个单元，输出层有$d×l$个单元，使其满足对每一个维度的$l$个预测输出都有一个单元对应。隐藏层中的LSTM单元通过循环连接完全连接。我们基于如下方式堆叠LSTM层：较低层的LSTM隐藏层中的每个单元通过前馈连接完全连接到其上方的较高层的LSTM隐藏层中的每个单元（参见$Fig. 1(b)$）。使用$s_{N}$中的序列来学习预测模型。集合$v_{N1}$用于在学习网络权重时的提前停止。</p><p>&emsp;&emsp;<strong>基于预测误差分布的异常检测：</strong>在预测长度为$l$的情况下，对于$l&lt;t \leq n-1$，$\mathbf{x}^{(t)} \in X$中每个$d$维被预测$l$次。我们计算点$\mathbf{x}^{(t)}$的误差向量$ \mathbf e^{(t)}$,$\mathbf{e}^{(t)}=\left[e_{11}^{(t)}, \ldots, e_{1 l}^{(t)}, \ldots, e_{d 1}^{(t)}, \ldots, e_{d l}^{(t)}\right]$，其中$e_{ij}^{(t)}$表示$x_{i}^{(t)}$和其在$t-j$时间上的预测值之间的差。</p><p>&emsp;&emsp;在$s_{N}$上训练的预测模型用于计算验证和测试序列中每个点的误差向量。对误差向量进行建模以拟合多元高斯分布$\mathcal{N}=\mathcal{N}(\mu, \mathbf{\Sigma})$。观察误差向量$\mathbf e^{(t)}$的似然性$p^{(t)}$由$\mathcal{N}$在$\mathbf e^{(t)}$处的值给出（类似于使用基于卡尔曼滤波器的动态预测模型<sup>[5]</sup>的新颖性检测所使用的基于归一化新息平方$(NIS)$）。来自$v_{N1}$的点的误差向量用于使用最大似然估计来估计参数$μ$和$Σ$。如果$p^{(t)}&lt;τ$，则观察值$\mathbf x^{(t)}$被分类为“异常”，否则观察值被分类为“正常”。集合$v_{N2}$和$v_{A}$用于通过最大化$F_{β}-socre$来学习$τ$（其中异常点属于正类，而正常点属于负类）。</p><h3 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h3><p>&emsp;&emsp;我们在四个真实世界数据集上呈现LSTM-AD的结果，这些数据集在检测它们中的异常时具有不同的难度级别。在使用第2节中描述的验证集选择最大化$F_{0.1}-score$的网络架构以及和$τ$后，我们在表1中展示了采用LSTM-AD和RNN-AD两种方法的精度，召回率，$F_{0.1}-score$和架构。</p><h4 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h4><p>&emsp;&emsp;<em>心电图（ECGs)</em>：qtdb / sel102心电图数据集包含对应于心室前收缩的单个短期异常$(Fig.2 (a))$。由于ECG数据集只有一个异常，我们不计算该数据集的阈值和相应的$F_{0.1}-score$；我们只使用正常的ECG子序列学习预测模型，并计算剩余序列的误差向量的可能性。</p><p>&emsp;&emsp;<em>航天飞机Marotta阀门时间序列</em>：该数据集具有短时间段模式和长时间段模式，持续100个时间步长。数据集中存在三个异常区域在$Fig.2 (b)$中被标记为$a_{1}, a_{2}, a_{3}$。区域$a_{3}$是更容易辨别的异常，而区域$a_{1}$和$a_{2}$对应于在该方法下不易辨别的更微妙的异常。</p><p>&emsp;&emsp;<em>电力需求数据集</em>：电力消耗的正常行为是在每周中有五个峰值对应于五个工作日和两个低谷对应于周末。该数据集属于长期模式具有跨越数百个的时间步长。此外，此数据集有噪声，因为峰值不会在每条的同一时间出现。</p><p>&emsp;&emsp;<em>多传感器引擎数据</em>：该数据集具有来自12个不同的传感器：其中一个传感器是发动机的“控制”部件，其余的测量依赖如温度，扭矩等变量。我们使用对应于三个独立故障的序列来训练异常检测模型，并在三个不同的独立故障集上测量$F_{β}-score$。我们选择“控制”传感器和其他一个因变量作为要预测的值。</p><img src="/2019/09/19/《Long-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series》/2.png" title="Fig.2: 样本序列（正常：绿色， 异常：蓝色）和相关似然$p$（红色）具有相同$S_{i}(i = 1,2,3)$的图具有相同的y轴"><h4 id="3-2-结果"><a href="#3-2-结果" class="headerlink" title="3.2 结果"></a>3.2 结果</h4><p>&emsp;&emsp;我们的实验结果的主要观察结果如下：</p><p>&emsp;&emsp;（i）在$Fig.2$ 中，对于所有数据集，异常区域中的似然值$p^{(t)}$显着低于正常区域。而且，$p^{(t)}$值在整个异常区域中不会保持低值。我们使用$β&lt;&lt; 1(0.1)$以便更加重视召回率的精确度：请注意，异常子序列中的所有点都具有“异常”标签，但实际上在这些中，也会存在许多“正常”行为点。因此，如果“异常”子序列中的很大一部分点被预测为异常就足够了。获得的$τ$值（$Fig.2. (a)- (f)$中的$p^{(t)}$图中的红色虚线）表明$F_{β}-score$（表1）是所考虑的数据集的合适度量。</p><p>&emsp;&emsp;（ii）对于所有数据集，发现阳性似然比（真阳性率/假阳性率）高（超过34.0）。高正似然比值表明在异常区域是异常的概率远高于在正常区域是异常的概率。</p><p>&emsp;&emsp;（iii）$Fig.2$ （f.1）和（f.2）显示了所选隐藏单元的激活，分别来自功率数据集的LSTM-L1（30个单元的低隐藏层）和LSTM-L2（20个单元的高隐藏层）各4个。在$Fig.2$ （f.2）中所示的最后激活序列中标记为$w_{1}$和$w_{2}$的子序列表示该隐藏单元激活在工作日期间高而在周末期间低。这些是由较高隐藏层学习的<em>high-level</em>特征的实例，其似乎以周为时间尺度操作。</p><p>&emsp;&emsp;（iv）如表1所示，对于没有任何长期时间依赖性的“ECG”和“引擎”数据集，LSTM-AD和RNN-AD的表现同样良好。另一方面，对于具有长期时间依赖性和短期依赖性的“航天飞机”和“电力需求”数据集，在$F_{0.1}-score$上LSTM-AD比RNN-AD分别显着提高了18％和30％。 </p><p>&emsp;&emsp;（v）“发动机”数据集故障前检测到的异常点的比例高于正常运行期间的异常点。这表明我们的方法可能对早期故障预测很有用。</p><img src="/2019/09/19/《Long-Short-Term-Memory-Networks-for-Anomaly-Detection-in-Time-Series》/3.png" title="Table 1: RNN和LSTM结构的精度，召回和$F_{0.1}-score$ 注意:（30-20）分别表示第一和第二隐藏层中的30和20个单位。"><h3 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h3><p>&emsp;&emsp;我们已经证明了（i）堆叠式LSTM网络能够在没有模式持续时间的先验知识的情况下学习更高级别的时间模式，因此（ii）堆叠的LSTM网络可能是建模正常时间序列行为的可行技术，然后可以用于检测异常。 我们的LSTM-AD方法在四个真实世界数据集上产生了有希望的结果，这些数据集涉及建模短期和长期时间依赖性。 与RNN-AD相比，LSTM-AD给出了更好或类似的结果，表明与基于RNN的模型相比，基于LSTM的预测模型可能更稳健，特别是当我们事先不知道正常行为是否涉及长期依赖性时。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] M. Basseville and I. V. Nikiforov. Detection of abrupt changes: theory and application. Prentice Hall, 1993.<br>[2] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation,9(8):1735–1780, 1997.<br>[3] M. Hermans and B. Schrauwen. Training and analysing deep recurrent neural networks. Advances in Neural Information Processing Systems 26, pages 190–198, 2013.<br>[4] D. George. How the brain might work: A hierarchical and temporal model for learning and recognition. PhD Thesis, Stanford University, 2008.<br>[5] P. Hayton et al. Static and dynamic novelty detection methods for jet engine health monitoring. Philosophical Transactions of the Royal Society of London, 365(1851):493–514, 2007.<br>[6] J. Ma and S. Perkins. Online novelty detection on temporal sequences. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 613–618. ACM, 2003. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Long Short Term Memory Netwo
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://pl741.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="时间序列异常检测" scheme="http://pl741.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>《Detecting Spacecraf Anomalies Using LSTMs and Nonparametric Dynamic Thresholding》</title>
    <link href="http://pl741.github.io/2019/09/16/%E3%80%8ADetecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding%E3%80%8B/"/>
    <id>http://pl741.github.io/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/</id>
    <published>2019-09-16T10:00:30.000Z</published>
    <updated>2019-10-09T03:11:31.363Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1802.04431.pdf" target="_blank" rel="noopener">Detecting Spacecraf Anomalies Using LSTMs and Nonparametric Dynamic Thresholding</a></p><h3 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h3><p>异常检测&emsp;神经网络&emsp;RNN&emsp;LSTM&emsp;时间序列</p><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><h4 id="利用LSTMs进行遥测值预测"><a href="#利用LSTMs进行遥测值预测" class="headerlink" title="利用LSTMs进行遥测值预测"></a>利用LSTMs进行遥测值预测</h4><p>&emsp;&emsp;模型确定后，提供一种非参数、动态无监督的阈值方法来评估残差。</p><p>&emsp;&emsp;为每个单通道创建一个单独的模型，使用每个模型预测该通道的值。为每个通道单独建模还可以跟踪通道级别，实现航天器异常模式的细粒度检测。考虑时间序列$X=\left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(n)}\right\}$，其中时间序列中的每一步$\mathbf{x}^{(t)} \in R^{m}$为$m$维向量$\left\{x_{1}^{(t)}, x_{2}^{(t)}, \ldots, x_{m}^{(t)}\right\}$对应于输入向量，对于每个点$\mathbf{x}^{(t)}$，序列长度$l_s$决定输入模型进行预测的点的数量，预测长度$l_p$决定预测的步长，预测维度$d$的范围为$1 \leq d \leq m$。要预测单个通道的遥测值则$d=1$，同时使用$l_p=1$限制每个步骤$t$的预测数量，以减少运行时间。在每个步骤$t$为实际遥测值生成单个标量预测值$\hat{y}^{(t)}$。在本次实验中输入到LSTM中的$x^{(t)}$包括给定信道的先验遥测值和发送到航天器的编码命令信息。发出命令的模块和发送或接收命令的模块的组合是一个 one-hot 编码的模块，插入到每个步骤$t$中。</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/1.png" title="图1. 每个时间步长$t$预测所用输入矩阵的可视化表示。将当前预测误差与过去预测误差进行比较，以确定是否异常"><h4 id="动态误差阈值"><a href="#动态误差阈值" class="headerlink" title="动态误差阈值"></a>动态误差阈值</h4><p>&emsp;&emsp;本文提出一种方法，可以在不做对过去平滑误差分布做高斯假设的情况下有效地标识极值。每一步$t$产生一个预测值$\hat{y}^{(t)}$，预测误差$e^{(t)}=\left|y^{(t)}-\hat{y}^{(t)}\right|$，其中$y^{(t)}=x_{i}^{(t+1)}$，其中$i$对应于真实遥测值得维度，将每个误差$e^{(t)}$添加到一维误差$e$ 向量中，</p><script type="math/tex; mode=display">\mathbf{e}=\left[e^{(t-h)}, \ldots, e^{\left(t-l_{s}\right)}, \ldots, e^{(t-1)}, e^{(t)}\right]</script><p>其中$h$表示用于评估当前误差的历史误差值的数量，然后对误差集$\mathbf e$进行平滑以抑制LSTM预测中的尖锐误差值，这些尖锐误差会影响预测结果，即使在正常情况下，也会出现误差值的急剧峰值。本文使用指数加权平均（EWMA）来产生平滑误差$\mathbf{e}_{s}=\left[e_{s}^{(t-h)}, \ldots, e_{s}^{(t-l s)}, \ldots, e_{s}^{(t-1)}, e_{s}^{(t)}\right]$。为评估这些值是否为正常值，本文将为平滑预测误差设置一个阈值，将阈值以上的平滑预测误差值对应的值分类为异常。</p><p>&emsp;&emsp;<strong>阈值计算和异常评分</strong>：本文提出一种无监督的异常阈值计算方法，可以在低开销、不使用标记数据或误差统计假设的情况下实现高效的阈值计算。阈值$\epsilon$从以下集合中选出：</p><script type="math/tex; mode=display">\boldsymbol{\epsilon}=\mu\left(\mathbf{e}_{s}\right)+\mathbf{z} \sigma\left(\mathbf{e}_{s}\right)</script><p>其中$\epsilon$取决于：</p><script type="math/tex; mode=display">\epsilon=\arg \max (\boldsymbol{\epsilon})=\frac{\Delta \mu\left(\mathbf{e}_{s}\right) / \mu\left(\mathbf{e}_{s}\right)+\Delta \sigma\left(\mathbf{e}_{s}\right) / \sigma\left(\mathbf{e}_{s}\right)}{\left|\mathbf{e}_{a}\right|+\left|\mathbf{E}_{s e q}\right|^{2}}</script><p>其中：</p><script type="math/tex; mode=display">\begin{array}{l}{\Delta \mu\left(\mathbf{e}_{s}\right)=\mu\left(\mathbf{e}_{s}\right)-\mu\left(\left\{e_{s} \in \mathbf{e}_{s} | e_{s}<\epsilon\right\}\right)} \\ {\Delta \sigma\left(\mathbf{e}_{s}\right)=\sigma\left(\mathbf{e}_{s}\right)-\sigma\left(\left\{e_{s} \in \mathbf{e}_{s} | e_{s}<\epsilon\right\}\right)} \\ {\mathbf{e}_{a}=\left\{e_{s} \in \mathbf{e}_{s} | e_{s}>\epsilon\right\}} \\ {\mathbf{E}_{s e q}=\text { continuous sequences of } e_{a} \in \mathbf{e}_{a}}\end{array}</script><p>使用$z \in \mathbf{z}$来确定$\epsilon$的评估值，其中$\mathbf z$是一个有序正值集，表示标准差大于$\mu\left(\mathbf{e}_{s}\right)$的数量。$\mathbf z$的值取决于上下文，但根据实验结果，$2 - 10$之间的范围可以很好的工作。$z &lt;2$的值通常会导致过多的假阳性。一旦确定了$\arg \max (\boldsymbol{\epsilon})$，每个得到的平滑错误序列$\mathbf{e}_{s e q} \in \mathbf{E}_{s e q}$都会得到一个异常分数$s$，用来表示异常的严重程度：</p><script type="math/tex; mode=display">s^{(i)}=\frac{\max \left(\mathbf{e}_{s e q}^{(i)}\right)-\arg \max (\boldsymbol{\epsilon})}{\mu\left(\mathbf{e}_{s}\right)+\sigma\left(\mathbf{e}_{s}\right)}</script><p>也就是说，如果找到一个阈值，去掉超过它的所有值，平滑误差$\mathbf e_{s}$的均值和标准差都会下降最大的百分比。该函数还惩罚具有最大异常值$\left(\left|\mathbf{e}_{a}\right|\right)$和序列$\left(\left|\mathbf{E}_{s e q}\right|\right)$以防止过度贪心行为。然后根据每个异常误差序列到所选阈值的距离，给出平滑误差最大值的归一化分数。</p><h4 id="减少误报"><a href="#减少误报" class="headerlink" title="减少误报"></a>减少误报</h4><p>&emsp;&emsp;<strong>修剪异常</strong>：基于预测的异常检测方法的精度很大程度上取决于用于设置阈值和判定当前预测误差的历史数据量$(h)$。为了减少误报、限制内存和计算成本，我们引入了一个剪枝过程，创建一个新集合$\mathbf{e}_{m a x}$，包含按照降序排序的所有$\mathbf{e}_{s e q}$的$\max \left(\mathbf{e}_{s e q}\right)$。同时在$\mathbf e_{max}$的末尾添加一个非异常$\max \left(\left\{e_{s} \in \mathbf{e}_{s} \in \mathbf{E}_{s e q} | e_{s} \in \mathbf{e}_{a}\right\}\right)$的最大平滑误差。之后以增量的方式逐步执行序列，计算每一步的减少百分比$d^{(i)}=\left(e_{\max }^{(i-1)}-e_{\max }^{(i)}\right) / e_{\max }^{(i-1)}$，其中$i \in\left\{1,2, \ldots,\left(\left|\mathbf{E}_{s e q}\right|+1\right)\right\}$。如果在某个步骤$i$中，$d^{(i)}$超过了最小百分比降幅$p$，则所有$e_{m a x}^{(j)} \in \mathbf{e}_{m a x} | j&lt;i$及其对应的异常序列均为异常。如果$d^{(i)}$没有满足最小减少量$p$，对于所有后续平滑的误差序列$d^{(i)}, d^{(i+1)}, \ldots, d^{\left(i+\left|\mathbf{E}_{s e q}\right|+1\right)}$都将被重新分类为正常误差。这种剪枝有助于确保异常序列不是流中常规噪声的结果，并且可通过阈值处理来初始识别异常序列。将评估仅限于少数潜在异常序列中的最大误差比没有阈值处理所需的大量值 - 值比较要有效得多。</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/2.png" title="图2. 展示异常修剪过程的例子"><p>&emsp;&emsp;在这种情况下，$\mathbf{e}_{m a x}=[0.01396,0.01072,0.00994]$，最小下降百分比 $p=0.1$。从异常$2$到异常$1$的下降幅度为 $d^{(1)}=0.23&gt;p$，该序列保留为异常分类。从异常$1$到下一个最高平滑误差$\left(e_{s}=0.0099 \right)$的下降幅度为 $d^{(2)}=.07&lt;p$，因此这个序列被重新分类为正常序列。</p><p>&emsp;&emsp;<strong>学习历史数据</strong>：一旦收集到少量异常历史纪录或标记数据，就可以使用这种学习策略来抑制假阳性。基于相似度$s$的异常通常不会在同一频道内频繁重复出现的假设，可以设置最小分数$s_{min}$，以便在$\boldsymbol{s}&lt;\boldsymbol{s}_{\min }$时，将未来的异常重新分类为正常。最低分数只适用于系统产生异常率超过某一比率的数据通道，并为所有这些通道单独设置$s_{min}$。可以使用通道的先验异常得分来设置适当的$s_{min}$，具体取决于精确度和召回率之间的期望平衡。此外，如果异常检测系统有一种机制，用户可以通过该机制为异常提供标签，那么这些标签还可以用于为给定流设置$s_{min}$。例如，如果一个流或通道有多个合并的假阳性异常，那么$s_{min}$可以设置在这些假阳性异常分数的上界附近。</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/3.png" title="图3 包含上下文异常的遥测流命令信息编码"><p>&emsp;&emsp;这种异常不太可能使用基于限制或距离的方法进行识别。使用已编码的命令信息和信道的先前遥测值生成下一个时间步骤的预测，并产生误差。在这个例子中，一步预测和实际遥测值非常接近，如顶部时间序列所示。利用非参数阈值化方法设置误差阈值，得到标记异常区域内的两个预测异常序列，一个为假阳性，一个为真阳性。假阳性表明需要对序列进行修剪，如果该序列相对接近阈值以下的值，则将该序列重新分类为正常序列（参见图2）。</p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>&emsp;&emsp;我们将异常分为两类：点异常和上下文异常，以区分可能由适当设置的警报或忽略时间信息的基于距离的方法（点异常）识别的异常和需要更复杂的方法（如LSTMs或分层时间记忆（HTM）方法）来检测（上下文异常）的异常。这个特征是从前面提到的三个类别中改编而来的——点异常、上下文异常、集合异常。由于上下文异常和集合异常都需要时间上下文，并且比较难以检测，因此它们都被合并到上下文类别中。</p><p>&emsp;&emsp;<strong>设置：</strong>对于主要发生在时间$t_{a}$包含一个或多个异常序列的每个唯一数据流，我们评估从$t_{s}=t_{a}-3 d$到$t_{f}=t_{a}+2 d$ 时间帧之间的所有遥测值， 其中$d$表示天。使用从$t_{s_{\text {train}}}=t_{s}-2 d$到$t_{f_{\text {train}}}=t_{s}$的值和命令数据为每个唯一流训练模型。如果在这些时间范围内没有足够的数据，则增加额外的天数。5天的异常周期被选择用来平衡两个目标：精度和计算成本。预测异常区域略微扩大，以便将扩展后重叠或邻近的异常区域合并为一个区域，来解释多个异常区域代表一个时间的情况。根据系统识别出的最后一组预测异常序列，对每个标记的遥测异常序列$x_{a} \in \mathbf{x}_{a}$按照下面规则进行评估：</p><p>&emsp;&emsp;（1）真阳性：</p><script type="math/tex; mode=display">\left|e_{a}^{(t)} \in e_{s e q} \in \mathbf{e}_{s e q} : x_{i}^{(t)} \in x_{a}\right|>0</script><p>&emsp;&emsp;对于任意的$x_{a} \in \mathbf{x}_{a}$，换句话说，预测异常序列的任何一部分都属于任何真实标记序列，则结果为真阳性。即使许多预测序列的一部分属于标记序列也仅纪录一个真阳性。</p><p>&emsp;&emsp;（2）如果没有预测序列与阳性标记序列重叠，标记为假阴性。</p><p>&emsp;&emsp;（3）所有没有与标记异常区域重叠的预测序列，标记为假阳性。</p><p>&emsp;&emsp;为简单起见，我们不会根据检测到异常的早期程度或误报与标记区域之间的距离进行评分调整。</p><p>&emsp;&emsp;<strong>批处理：</strong>遥测数据被聚合到一分钟窗口中，按照SMAP和当前系统实现的下行计划，以70分钟/组进行评估。使用$h=2100$计算每70分钟批处理的值，其中$h$是用于计算错误阈值和评估当前批处理的先前值的数量。该系统还非常适合以实时流式方式处理值。</p><p>&emsp;&emsp;<strong>模型参数和评估：</strong>在实验中所有模型使用相同的架构和参数：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/4.png"><p>&emsp;&emsp;每个模型都是只含有两层隐藏层和80个（/层）单元的浅层模型。我们发现这种结构能够提供足够的能力来很好的预测单个通道，增加额外的能力在增加模型大小和训练时间的同时对提供预测效益没有任何用处。所有通道不一定都需要这么大的容量，未来的改进可能包括根据信道复杂度自动选择合适的模型容量。类似地，序列长度$l_{s}=250$提供了性能和训练时间之间的平衡。SMAP和MSL在输入维度上的差异是由于每个任务都有不同的命令模块集。在模型训练过程中，早期停止被用来防止过度拟合，并不是所有的模型都经过了完整35次迭代的训练。预测生成后，采用3.2节的方法计算平滑误差的异常阈值，其中$\mathbf{z}=\{2.5,3.0,3.5, \ldots, 10.0\}$，并且后续异常值之间的最小百分比只差$p=0.13$。参数$p$是控制精确度和召回率的重要标准，当标签可用时可推断出一个适当的值。在我们的设置中，在$0.05&lt;p&lt;0.20$达到了合理的结果。</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/5.png" title="图4 参数方法和本文提出的方法（标记为“非参数”）在不同参数设置下的总体精度和召回结果的比较的图。"><p>&emsp;&emsp;<strong>参数阈值比较：</strong>使用原始的LSTM进行误差预测，我们还使用参数误差评估方法生成异常，该方法与Numenta异常基准的最精确模型相协调。这种方法递增地处理原始误差-在每个步骤$t$，将历史误差的窗口$W$建模为正态分布，并且在每个步骤$t$更新均值$μ_{w}$和方差$σ_{W}^{2}$。我们设置$W$的长度为$l_{w}=h=2100$，并对两种方法使用相同的预测误差集。然后计算预测误差长度$l_{short}$的短期平均值$μ_{s}$，其平滑效果与第3.2节中的EWMA平滑相似。然后使用尾概率$Q$来定义异常$L$的可能性:</p><script type="math/tex; mode=display">L=1-Q\left(\frac{\mu_{s}-\mu_{W}}{\sigma_{W}^{2}}\right)</script><p>如果$L \geq 1-\epsilon_{n o r m}$的值为分类为异常。在下一节中，使用$l_{short}=10$和$\epsilon_{n o r m}=\{0.01,0.0001\}$生成的结果与3.2节中的方法进行比较。</p><h3 id="复现实验"><a href="#复现实验" class="headerlink" title="复现实验"></a>复现实验</h3><p>&emsp;&emsp;GitHub地址：<a href="https://github.com/khundman/telemanom" target="_blank" rel="noopener">https://github.com/khundman/telemanom</a></p><p>&emsp;&emsp;<strong>遥测数据：包含两个分支</strong>  </p><p>&emsp;&emsp;&emsp;1.  master：包含标记异常的数据并用于复现KDD论文中的实验<br>&emsp;&emsp;&emsp;2.  no-labels：未标记的数据（一组时间序列流）</p><p>&emsp;&emsp;<strong>开始</strong>：</p><p>&emsp;&emsp;&emsp;克隆仓库——<code>git clone https://github.com/khundman/telemanom.git &amp;&amp; cd telemanom</code></p><p>&emsp;&emsp;&emsp;Curl和Unzip数据——<code>curl -O https://s3-us-west-2.amazonaws.com/telemanom/data.zip &amp;&amp; unzip data.zip &amp;&amp; rm data.zip</code></p><p>&emsp;&emsp;&emsp;使用Python 3.6+安装依赖项——<code>pip install -r requirements.txt</code></p><p>&emsp;&emsp;&emsp;在<code>config.yaml</code>文件中配置系统/模型参数：<br>&emsp;&emsp;&emsp;&emsp;&emsp;<code>train:</code> 如果是<code>True</code>，将为每个输入流训练一个新模型；如果是 <code>False</code> ，将加载现有的训练模型并用于生成预测。<br>&emsp;&emsp;&emsp;&emsp;&emsp;<code>predict:</code> 如果是<code>Ture</code>，使用模型生成新的预测；如果是<code>False</code>，请在评估中使用现有的已保存预测（用于调整错误阈值并跳过先前的处理步骤）<br>&emsp;&emsp;&emsp;&emsp;&emsp;<code>l_s:</code> 250 确定在每个时间步$t$上输入到模型的先前时间步数（用于生成预测）</p><p>&emsp;&emsp;&emsp;开始实验——<code>python run.py</code></p><p>&emsp;&emsp;&emsp;利用<code>Jupyter notebook</code>来进行结果评估并比较不同参数的运行结果在<code>results/</code>下提供。启动notebook——<code>jupyter notebook results/result-viewer.ipynb</code><br>&emsp;&emsp;&emsp;Plotly用于生成交互式内联图，例如：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/6.png"><p>&emsp;&emsp;<strong>数据</strong>：</p><p>&emsp;&emsp;&emsp;&emsp;<strong>使用你自己的数据</strong></p><p>&emsp;&emsp;&emsp;首先划分数据集为训练数据集和测试数据集并分别放在<code>data/train/</code>和<code>data/test/</code>文件夹下。对于训练集和测试集为每个通道或流生成形状为<code>(n_timesteps, n_inputs)</code>的一个<code>.npy</code>文件。每个文件名应该具有唯一的名称或ID。在测试数据中预测的遥测值必须是输入中的第一个特征。</p><p>&emsp;&emsp;&emsp;例如，<code>T-1</code>通道应该具有被命名为<code>T-1.npy</code>的训练集/测试集，形状类似于<code>(4900, 61)</code>和<code>(3925, 61)</code>，其中输入维度大小是<code>(61)</code>。实际遥测值应沿第一维<code>(4900, 1)</code>和<code>(3925, 1)</code>。</p><p>&emsp;&emsp;&emsp;&emsp;<strong>原始数据</strong></p><p>&emsp;&emsp;&emsp;可供下载的原始数据代表了来自SMAP卫星和MSL卫星真实的航天器遥测数据和异常。所有数据都已在时间上匿名化，并且所有遥测值都根据测试集中的最小值/最大值在<code>(-1,1)</code>之间预先缩放。通道ID也被匿名化，但是第一个字母表示出通道的类型（<code>P</code>=功率，<code>R</code>=辐射）。模型输入数据还包括有关特定航天器模块在给定时间窗口内发送或接收的命令的<code>one-hot</code>编码信息。模型输入数据还包括有关特定航天器模块在给定时间窗口内发送或接收的命令的一键编码信息。数据中不包含与命令的时间或特性有关的识别信息。 例如：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/3.png"><p>&emsp;&emsp;&emsp;此数据还包括使用<code>config.yaml</code>中的默认设置生成的预分割测试和训练数据，预训练模型，预测和平滑错误。当熟悉仓库时，可通过运行<code>result-viewer.ipynb</code>来可视化结果。数据对于隔离系统的各个部分也很有用。例如，如果你希望在不训练新模型的情况下看到阈值参数更改的影响，可以在<code>config.yaml</code>中将<code>Train</code>和<code>Predict</code>设置为<code>False</code>，以使用从之前模型生成的预测。</p><p>&emsp;&emsp;<strong>异常标签和元数据</strong>：</p><p>&emsp;&emsp;&emsp;异常标签和元数据可在<code>labeled_anomalies.csv</code>中使用，包括：</p><p>&emsp;&emsp;&emsp; <code>channel_id</code>: 匿名的频道ID———首字母代表频道的性质（<code>P</code>=功率，<code>R</code>=辐射）。<br>&emsp;&emsp;&emsp; <code>spacecraft</code>: 产生遥测流的航天器<br>&emsp;&emsp;&emsp; <code>anomaly_sequences</code>: 流中真实异常开始和结束的索引<br>&emsp;&emsp;&emsp; <code>class</code>: 异常的类别<br>&emsp;&emsp;&emsp; <code>num values</code>: 每个流中的遥测值数量</p><p>&emsp;&emsp;&emsp;要提供自己的标签，请使用<code>labeled_anomalies.csv</code>文件作为模板。  唯一需要的字段/列是<code>channel_id</code>和<code>anomaly_sequences</code>。 <code>anomaly_sequences</code>是列表中的一列，其中包含通道的测试数据集中异常区域的开始和结束索引。</p><p>&emsp;&emsp;<strong>数据集和性能统计</strong>：</p><p>&emsp;&emsp;&emsp;&emsp;数据：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/7.png"><p>&emsp;&emsp;&emsp;&emsp;性能统计：</p><img src="/2019/09/16/《Detecting-Spacecraf-Anomalies-Using-LSTMs-and-Nonparametric-Dynamic-Thresholding》/8.png"><p>&emsp;&emsp;<strong>处理</strong>：</p><p>&emsp;&emsp;每次启动系统时，唯一的日期时间ID（例如2018-05-17_16.28.00）将用于创建以下内容:</p><p>&emsp;&emsp;<code>results</code>文件：（在<code>results /</code>中），该文件扩展了<code>labeled_anomalies.csv</code>包括已识别的异常序列和相关信息。</p><p>&emsp;&emsp;<code>data subdirectory</code>: 包含用于每个通道的已创建模型，预测和平滑错误的数据文件。 还创建了一个名为<code>params.log</code>的文件，其中包含参数设置和处理期间的日志记录输出。</p><p>&emsp;&emsp;如前所述，jupyter notebook的<code>results / result-viewer.ipynb</code>可用于可视化每个流的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.04431.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Detecting Spacecraf Anomalies Using LSTMs and Nonparametric
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://pl741.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="时间序列异常检测" scheme="http://pl741.github.io/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>使用链接</title>
    <link href="http://pl741.github.io/2019/09/16/%E9%93%BE%E6%8E%A5/"/>
    <id>http://pl741.github.io/2019/09/16/链接/</id>
    <published>2019-09-16T07:38:06.000Z</published>
    <updated>2019-09-16T10:09:59.426Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.jianshu.com/p/4eaddcbe4d12" target="_blank" rel="noopener">搭建个人博客</a></p><p><a href="http://blog.rexking6.top/2017/03/30/hexo%E4%B8%BB%E9%A2%98%E5%92%8C%E6%B7%BB%E5%8A%A0%E8%AF%84%E8%AE%BA%E3%80%81%E6%89%93%E8%B5%8F%E3%80%81%E6%90%9C%E7%B4%A2%E3%80%81%E9%98%85%E8%AF%BB%E9%87%8F%E7%AD%89%E5%8A%9F%E8%83%BD/" target="_blank" rel="noopener">博客个性化设置</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/4eaddcbe4d12&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;搭建个人博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.rexking6.top/2017/
      
    
    </summary>
    
    
    
      <category term="配置" scheme="http://pl741.github.io/tags/%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>jupyter主题设置</title>
    <link href="http://pl741.github.io/2019/09/16/jupyter_%E4%B8%BB%E9%A2%98%E8%AE%BE%E7%BD%AE/"/>
    <id>http://pl741.github.io/2019/09/16/jupyter_主题设置/</id>
    <published>2019-09-16T05:38:06.000Z</published>
    <updated>2019-09-16T09:57:34.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="jupyter-主题设置"><a href="#jupyter-主题设置" class="headerlink" title="jupyter 主题设置"></a>jupyter 主题设置</h2><p>安装主题：pip install jupyterthemes</p><p>如果之前安装过可以更新一下：pip install —upgrade jupyterthemes</p><p>设置主题：蓝色主题——jt -t onedork -fs 95 -altp -tfs 11 -nfs 115 -cellw 88% -T</p><p>​                    黑色主题——jt -t monokai -f fira -fs 13 -cellw 90% -ofs 11 -dfs 11 -T -N</p><p>​                    部分参数：-f(字体)  -fs(字体大小) -cellw(占屏比或宽度)  </p><p>​                                      -ofs(输出段的字号)  -T(显示工具栏)  -N(显示自己主机名)</p><h4 id="Conda环境自由切换："><a href="#Conda环境自由切换：" class="headerlink" title="Conda环境自由切换："></a>Conda环境自由切换：</h4><p>​        看一下是否已经把 Anaconda 中创建的所有定制环境作为核心添加在了 Jupyter Notebook 中。这样我们就能简单地利用 Kernel 按钮切换环境。换核的时候不需要重启 notebook。</p><p>​        假设你的 Anaconda 环中有两个自定义的环境 my_NLP 和 gym。按照下面的步骤将这些添加到你的 Jupyter Notebook 中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda activate my_NLP</span><br><span class="line"># Install the IPython Kernel </span><br><span class="line">pip install ipykernel</span><br><span class="line"># Link your environment with Jupyter </span><br><span class="line"># Repeat steps for the other environment, gym</span><br><span class="line">python -m ipykernel install --user --name=my_NLP</span><br><span class="line">pip install ipykernel </span><br><span class="line">python -m ipykernel install --user --name=gym</span><br></pre></td></tr></table></figure><p>​        现在打开你的 Jupyter Notebook，找到 kernel 按钮下的 Change Kernel 选项，接下来就是见证奇迹的时刻：所有的核都被列举出来了，你可以通过简单地点击来激活一个服务核。</p><h4 id="其他功能："><a href="#其他功能：" class="headerlink" title="其他功能："></a>其他功能：</h4><p>​        安装 nbextensions for Jupyter Notebooks</p><p>​        安装 nbextensions 是很容易的，简单地遵循下面的步骤就行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Stop and exit your Jupyter Notebook server </span><br><span class="line"># Make sure you are in the base environment</span><br><span class="line">conda activate base</span><br><span class="line"># Install the nbextensions </span><br><span class="line">pip install jupyter_contrib_nbextensions</span><br><span class="line"># Install the necessary JS and CSS files </span><br><span class="line">jupyter contrib nbextension install --system</span><br></pre></td></tr></table></figure><p>​        启动 Jupyter notebook 服务，你可以在起始页看到第四个叫做 Nbextensions 的选项。点击这个选项，然后就可以看到极妙的功能集，这些都是你一直希望在 Jupyter Notebooks 中拥有的。</p><p>其中一些的简单介绍：</p><p>​          Table of Contents(2)：单击生成整个笔记本的目录，不同的 section 都有对应的超链接。</p><p>​         Scratchpad：在我看来绝对是最好的扩展了。这是一个你可以在里面做代码实验的独立空间，不会干扰笔记本中的其他部分。</p><p>​        Codefolding ：代码折叠，这个不需要做过多的解释。</p><p>​        Hide Input All：隐藏所有的代码单元，同时保持所有的输出和 markdown 单元可见。如果你要向非技术人员解释你的结果，那么这就会是一个很有用的功能。</p><p>​        Variable Inspector：将你从调试的忧伤中拯救出来，这与 Spyder IDE 中的变量检查窗口有些类似。</p><p>​        Spellchecker：对 markdown 单元中的内容进行拼写检查。</p><p>​        Zenmode：移除掉屏幕中杂乱无关的内容，以便你能够聚焦于重要的东西上，例如代码。</p><p>​        Snippets Menu：从 list comprehension 到 pandas 以及它们之间的所有常用代码片段的一个很酷的集合。这是最好的部分？你可以修改窗口的小部件来添加你自己的定制片段。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;jupyter-主题设置&quot;&gt;&lt;a href=&quot;#jupyter-主题设置&quot; class=&quot;headerlink&quot; title=&quot;jupyter 主题设置&quot;&gt;&lt;/a&gt;jupyter 主题设置&lt;/h2&gt;&lt;p&gt;安装主题：pip install jupyterthemes
      
    
    </summary>
    
    
    
      <category term="配置" scheme="http://pl741.github.io/tags/%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>机器学习评价标准</title>
    <link href="http://pl741.github.io/2019/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86/"/>
    <id>http://pl741.github.io/2019/09/16/机器学习评价标准/</id>
    <published>2019-09-16T05:38:06.000Z</published>
    <updated>2019-09-16T07:24:19.337Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本内容"><a href="#基本内容" class="headerlink" title="基本内容"></a>基本内容</h1><p><strong>True真&emsp; False假&emsp;Positive正&emsp;Negative负</strong></p><p>&emsp;&emsp;异常检测中，P和N一般是针对预测来说的，Positive正类指的是你更关心的那一类！即“异常”，P指预测为正类，即预测为异常。T和F针对预测与真实情况的比较， True指正确匹配，F指错误匹配。</p><div class="table-container"><table><thead><tr><th></th><th style="text-align:center">实际正例</th><th style="text-align:center">实际负例</th><th style="text-align:center"></th></tr></thead><tbody><tr><td>预测正例P</td><td style="text-align:center">TP</td><td style="text-align:center">FP</td><td style="text-align:center">所有预测为正的个数TP+FP</td></tr><tr><td>预测负例N</td><td style="text-align:center">FN</td><td style="text-align:center">TN</td><td style="text-align:center">所有预测为负的个数FN+TN</td></tr><tr><td></td><td style="text-align:center">所有实际正例的个数TP+FN</td><td style="text-align:center">所有实际负例的个数FP+TN</td></tr></tbody></table></div><img src="/2019/09/16/机器学习评价标准/72.png"><ul><li><p>TPR：真正类率，代表预测是异常实际也是异常的样本数，占实际总异常数的比例——值越大 性能越好</p></li><li><p>FPR：假正类率，代表预测是异常但实际是正常的样本数，占实际正常总数的比例——值越小 性能越好</p></li><li><p>R：召回率，意义同TPR——值越大 性能越好</p></li><li><p>P：精确率Precision，代表预测是异常实际也是异常的样本数，占预测是异常的总数的比例——值越大 性能越好</p></li><li><p>F：P和R的加权调和平均，常用的是F1值——值越大 性能越好</p></li><li><p>A：正确率Accuracy，与精确率的区别是，不仅考虑异常类也考虑正常类，即所有匹配样本数，占所有样本的比例——值越大 性能越好</p></li></ul><p>另外还有两个，分别为：虚警率和漏警率</p><ul><li>虚警率（<strong>False alarm</strong>）表示负类样本被分为正类样本在所有负类样本中的比例</li></ul><img src="/2019/09/16/机器学习评价标准/71.png"><ul><li>漏警率表示（漏警率表示（Missing alarm）表示正类样本被分为负类样本在所有正类样本中的比例Missing alarm）表示正类样本被分为负类样本在所有正类样本中的比例</li></ul><img src="/2019/09/16/机器学习评价标准/70.png"><h1 id="Tensorflow实现"><a href="#Tensorflow实现" class="headerlink" title="Tensorflow实现"></a>Tensorflow实现</h1><p><strong>&emsp;损失值：</strong><br>    tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)<br><strong>&emsp;参数解析：</strong><br>&emsp; logits:神经网络最后一层的输出，如果有batch，大小为[batch_size, n_classes]<br>&emsp; labels:实际标签，大小同上</p><p><strong>&emsp;执行过程：</strong><br>&emsp;&emsp;先对网络最后一层的输出做一个softmax，通常是求取输出属于某一类的概率，对于单样本而言，输出就是一个num_classes大小的向量。</p><img src="/2019/09/16/机器学习评价标准/73.png"><p>&emsp;然后将softmax的输出向量与样本的实际标签做一个交叉熵。</p><img src="/2019/09/16/机器学习评价标准/74.png"><p>&emsp;其中 y’ 指代实际的标签中第i个的值（用mnist数据举例，如果是3，那么标签是[0，0，0，1，0，0，0，0，0，0]，除了第4个值为1，其他全为0）; y就是softmax的输出向量[Y1，Y2,Y3…]中，第i个元素的值</p><p>&emsp;&emsp;显而易见，预测越准确，结果的值越小（别忘了前面还有负号），最后求一个平均，得到我们想要的loss</p><p>&emsp;&emsp;注意！！！这个函数的返回值并不是一个数，而是一个向量，如果要求交叉熵，我们要再做一步tf.reduce_sum操作,就是对向量里面所有元素求和，最后才得到H(y)，如果求loss，则要做一步tf.reduce_mean操作，对向量求均值！</p><pre><code>tf.reduce_mean(input_tensor, axis=None,keep_dims=False,name=None,               reduction_indices=None)</code></pre><h4 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h4><ul><li>&emsp; 第一个参数input_tensor： 输入的待降维的tensor;</li><li>&emsp; 第二个参数axis： 指定的轴，如果不指定，则计算所有元素的均值;</li><li>&emsp; 第三个参数keep_dims：是否降维度，设置为True，输出的结果保持输入tensor的形状，设置为False，输出结果会降低维度;</li><li>&emsp; 第四个参数name： 操作的名称;</li><li>&emsp; 第五个参数 reduction_indices：在以前版本中用来指定轴，已弃用;</li></ul><p>&emsp;predict是预测结果，也就是神经网络的输出，real是真实的标签，sess就是tensorflow当前的会话，feed_dict是需要喂的数据。</p><p>&emsp;tf.equal(A, B)是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，反正返回False，返回的值的矩阵维度和A是一样的。</p><p>&emsp;tf.cast()函数的作用是执行 tensorflow 中张量数据类型转换，<br>    tf.cast(x, dtype, name=None)</p><h4 id="参数解析-1"><a href="#参数解析-1" class="headerlink" title="参数解析"></a>参数解析</h4><ul><li>&emsp;第一个参数 x:   待转换的数据（张量）</li><li>&emsp;第二个参数 dtype： 目标数据类型</li><li>&emsp;第三个参数 name： 可选参数，定义操作的名称</li></ul><p>&emsp;tf.loical_and()将数值变成逻辑值<br>    tf.logical_and(x, y, name=None)</p><p>&emsp;tf.argmax()返回值是是数值最大值的索引位置，如果最大值位置相同，则分类正确，反之则分类错误<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">predictions = tf.argmax(predict, <span class="number">1</span>)</span><br><span class="line">actuals = tf.argmax(real, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将上述获得的变量设置成元素为0或者为1的矩阵</span></span><br><span class="line">ones_like_actuals = tf.ones_like(actuals)</span><br><span class="line">zeros_like_actuals = tf.zeros_like(actuals)</span><br><span class="line">ones_like_predictions = tf.ones_like(predictions)</span><br><span class="line">zeros_like_predictions = tf.zeros_like(predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照前面的计算公式编写如下计算代码</span></span><br><span class="line"></span><br><span class="line">tp_op = tf.reduce_sum(tf.cast(tf.logical_and(</span><br><span class="line">            tf.equal(actuals, ones_like_actuals),</span><br><span class="line">            tf.equal(predictions, ones_like_predictions)),<span class="string">"float"</span>))</span><br><span class="line"> </span><br><span class="line">tn_op = tf.reduce_sum(tf.cast(tf.logical_and(</span><br><span class="line">            tf.equal(actuals, zeros_like_actuals),</span><br><span class="line">            tf.equal(predictions, zeros_like_predictions)),<span class="string">"float"</span>))</span><br><span class="line"> </span><br><span class="line">fp_op = tf.reduce_sum(tf.cast(tf.logical_and(</span><br><span class="line">            tf.equal(actuals, zeros_like_actuals),</span><br><span class="line">            tf.equal(predictions, ones_like_predictions)),<span class="string">"float"</span>))</span><br><span class="line"> </span><br><span class="line">fn_op = tf.reduce_sum(tf.cast(tf.logical_and(</span><br><span class="line">            tf.equal(actuals, ones_like_actuals),</span><br><span class="line">            tf.equal(predictions, zeros_like_predictions)),<span class="string">"float"</span>))</span><br><span class="line">            </span><br><span class="line">tp, tn, fp, fn = session.run([tp_op, tn_op, fp_op, fn_op], feed_dict)</span><br><span class="line">tpr = float(tp)/(float(tp) + float(fn))</span><br><span class="line">fpr = float(fp)/(float(fp) + float(tn))</span><br><span class="line">fnr = float(fn)/(float(tp) + float(fn))</span><br><span class="line">accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))</span><br><span class="line">recall = tpr</span><br><span class="line">precision = float(tp)/(float(tp) + float(fp))</span><br><span class="line">f1_score = (<span class="number">2</span> * (precision * recall)) / (precision + recall)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本内容&quot;&gt;&lt;a href=&quot;#基本内容&quot; class=&quot;headerlink&quot; title=&quot;基本内容&quot;&gt;&lt;/a&gt;基本内容&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;True真&amp;emsp; False假&amp;emsp;Positive正&amp;emsp;Negative负&lt;/stro
      
    
    </summary>
    
    
    
      <category term="机器学习" scheme="http://pl741.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
