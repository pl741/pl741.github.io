<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="GBDT,">





  <link rel="alternate" href="/atom.xml" title="木木的博客" type="application/atom+xml">






<meta name="description" content="GBDT回归算法参考：  https://www.cnblogs.com/peizhe123/p/5086128.html?clicktime=1572921140&amp;amp;enterid=1572921140   https://mp.weixin.qq.com/s/adMMz8u29OnvqelgZuXWjg   https://mp.weixin.qq.com/s/M2PwsrAnI1S9S">
<meta name="keywords" content="GBDT">
<meta property="og:type" content="article">
<meta property="og:title" content="Kaggle比赛">
<meta property="og:url" content="http://pl741.github.io/2019/11/04/Kaggle比赛/index.html">
<meta property="og:site_name" content="木木的博客">
<meta property="og:description" content="GBDT回归算法参考：  https://www.cnblogs.com/peizhe123/p/5086128.html?clicktime=1572921140&amp;amp;enterid=1572921140   https://mp.weixin.qq.com/s/adMMz8u29OnvqelgZuXWjg   https://mp.weixin.qq.com/s/M2PwsrAnI1S9S">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/1.png">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/2.png">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/6.png">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/5.png">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/3.png">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/4.png">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/7.png">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/8.png">
<meta property="og:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/9.png">
<meta property="og:updated_time" content="2020-01-08T08:07:19.185Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kaggle比赛">
<meta name="twitter:description" content="GBDT回归算法参考：  https://www.cnblogs.com/peizhe123/p/5086128.html?clicktime=1572921140&amp;amp;enterid=1572921140   https://mp.weixin.qq.com/s/adMMz8u29OnvqelgZuXWjg   https://mp.weixin.qq.com/s/M2PwsrAnI1S9S">
<meta name="twitter:image" content="http://pl741.github.io/2019/11/04/Kaggle比赛/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: 'D7ALJV5SO6',
      apiKey: '6e20ed4f9126e4bd7125665a8b2f2a43',
      indexName: 'MUMU',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"找不到关于 ${query} 的文章","hits_stats":"共找到 ${hits} 篇文章，花了 ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://pl741.github.io/2019/11/04/Kaggle比赛/">





  <title>Kaggle比赛 | 木木的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">木木的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">木木的博客</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          <!--增加的代码 start-->

          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://pl741.github.io/2019/11/04/Kaggle比赛/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="木木">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木木的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Kaggle比赛</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-04T10:52:20+08:00">
                2019-11-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kaggle比赛/" itemprop="url" rel="index">
                    <span itemprop="name">Kaggle比赛</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="GBDT回归算法"><a href="#GBDT回归算法" class="headerlink" title="GBDT回归算法"></a>GBDT回归算法</h2><p>参考：</p>
<p> <a href="https://www.cnblogs.com/peizhe123/p/5086128.html?clicktime=1572921140&amp;enterid=1572921140" target="_blank" rel="noopener">https://www.cnblogs.com/peizhe123/p/5086128.html?clicktime=1572921140&amp;enterid=1572921140</a> </p>
<p> <a href="https://mp.weixin.qq.com/s/adMMz8u29OnvqelgZuXWjg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/adMMz8u29OnvqelgZuXWjg</a> </p>
<p> <a href="https://mp.weixin.qq.com/s/M2PwsrAnI1S9SxSB1guHdg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/M2PwsrAnI1S9SxSB1guHdg</a> </p>
<p>GBDT(Gradient Boosting Decision Tree) 梯度提升决策树，是一种迭代的决策树算法，由多棵决策树组成，所有树的结果累加成最终答案，即基函数的线性组合。主要的优点：1. 在深度学习没有广泛被应用时，比赛的效果很不错；2. 即可以做分类同时也可以做回归，同时还可处理排序任务； 3. 可以筛选特征。</p>
<p>Boosting、Bagging和Stacking是集成学习(Ensemble Learning)的三种主要方法。Boosting是一族可将弱学习器提升为强学习器的算法，不同于Bagging、Stacking方法，Boosting训练过程为串联方式，弱学习器的训练是有顺序的，每个弱学习器都会在前一个学习器的基础上进行学习，最终综合所有学习器的预测值产生最终的预测结果。 </p>
<h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>当基学习器采用决策树时，梯度提升算法就具体到了梯度提升决策树。GBDT算法可看成是$M$棵树组成的加法模型，对应公式：</p>
<script type="math/tex; mode=display">
F(x, w)=\sum_{m=0}^M \alpha_{m}h_{m}(x, w_{m})=\sum_{m=0}^M f_{m}(x, w_{m})</script><p>其中，$x$为输入样本，$w$为模型参数，$h$为分类回归树，$\alpha$为每棵树的权重。</p>
<h4 id="GBDT算法的实现过程"><a href="#GBDT算法的实现过程" class="headerlink" title="GBDT算法的实现过程"></a>GBDT算法的实现过程</h4><p>给定训练数据集：$T=\{(x_{1}, y_{i}), (x_{2}, y_{2}), \ldots, (x_{N}, y_{N})\}$，其中$x_{i} \in \chi \subseteq R^{n}$，$\chi$为输入空间，$y_{i} \in Y \subseteq R$， $Y$为输出空间，损失函数$L(y, f(x))$，找出最终的回归树$F_{M}$。</p>
<ol>
<li>初始化第一个弱学习器$F_{0}(x)$：<script type="math/tex; mode=display">
F_{0}(x)=\mathop {argmin}_{c} \sum_{i=1}^N L(y_{i}, c)</script></li>
<li><p>建立$M$棵分类回归树：$m=1, 2, \ldots, M$</p>
<p>a. 对于$i=1, 2, \ldots, N$，计算第$m$棵树对应的响应值（损失函数的负梯度，即伪残差）</p>
<script type="math/tex; mode=display">
r_{m, i}=-\left[\frac{\partial L(y_{i}, F(x_{i}))}{\partial F(x)}\right]_{F(x)=F_{m-1}(x)}</script><p>b. 对于$i=1, 2, \ldots, N$，利用CART回归树（一般选择）拟合数据$(x_{i}, r_{m, i})$，得到第$m$棵回归树，对应的叶子节点区域为$R_{m,j}$，其中$j=1, 2, \ldots, J_{m}$，同时$J_{m}$是第$m$棵回归树叶子节点的个数。</p>
<p>c. 对于$J_{m}$个叶子节点区域$j=1, 2, \ldots, J_{m}$，计算最佳拟合值：</p>
<script type="math/tex; mode=display">
c_{m, j}=\mathop {argmin}_{c} \sum_{x_{i} \in R_{m, j}} L(y_{i}, F_{m-1}(x_{i}) + c)</script><p>d. 更新强学习器$F_{m}(x)$:</p>
<script type="math/tex; mode=display">
F_{m}(x)=F_{m-1}(x) + \sum_{j=1}^{J_{m}} c_{m, j}I(x \in R_{m, j})</script></li>
<li><p>强学习器$F_{M}(x)$：</p>
<script type="math/tex; mode=display">
F_{M}(x)=F_{0}(x)+\sum_{m=1}^M \sum_{j=1}^{J_{m}} c_{m, j}I(x \in R_{m, j})</script></li>
</ol>
<h3 id="GBDT基本介绍"><a href="#GBDT基本介绍" class="headerlink" title="GBDT基本介绍"></a>GBDT基本介绍</h3><p>GBDT = Regression Decistion Tree (DT)  + Gradient Boosting (GB) + Shrinkage</p>
<h4 id="DT：回归树"><a href="#DT：回归树" class="headerlink" title="DT：回归树"></a>DT：回归树</h4><p>决策树分为两类，回归树和分类树，GBDT核心是可以累加所有树的结果作为最终结果，因此GBDT树属于回归树 。回归树中最好分割点的衡量标准不再是最大熵（分类树的衡量标准），而是最小均方差。</p>
<h4 id="GB：梯度迭代"><a href="#GB：梯度迭代" class="headerlink" title="GB：梯度迭代"></a>GB：梯度迭代</h4><p>Gradient Boosting，Boosting迭代，通过迭代多棵树来共同决策，GBDT中每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。</p>
<p>Boosting最大的优势在于，每一步的残差计算其实增大了分错instance的权重，使得后面的树能越来越专注于分错的instance。</p>
<h4 id="Shrinkage：缩减"><a href="#Shrinkage：缩减" class="headerlink" title="Shrinkage：缩减"></a>Shrinkage：缩减</h4><p>Shrinkage思想认为，每次走一小步逐渐逼近结果，要比每次一大步很快的比逼近结果更容易避免过拟合。即它不完全信任每一颗残差树，累加的时候只累加一小部分。</p>
<p>没用Shrinkage：其中$y_{i}$表示第$i$棵树上$y$的预测值，$y_{1-i}$表示前$i$棵树上$y$的综合预测值，</p>
<p>$y_{i + 1} = 残差(y_{1} \sim y_{i})$，其中：$残差(y_{1} \sim y_{i})=y_{真实值}-y_{1 \sim i}$，$y_{1  \sim  i}=SUM(y_{1}, \ldots, y_{i})$</p>
<p>Shrinkage中：$y_{1 \sim i}=y(1 \sim i-1)+step * y_{i}$</p>
<p>即Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分$（step*残差）$逐步逼近目标，step一般都比较小，如$0.01\sim 0.001$（注意该step非gradient的step） 。本质上，Shrinkage为每棵树设置一个$weight$，累加时要乘以这个$weight$， <font color="red">但和Gradient没有关系</font> 。Shrinkage实践证明可以减少过拟合。</p>
<h4 id="GBDT适用范围"><a href="#GBDT适用范围" class="headerlink" title="GBDT适用范围"></a>GBDT适用范围</h4><p>适用所有回归问题（线性/非线性），也可用于二分类问题（设定阈值，大于阈值为正例，反之为负例），排序</p>
<h4 id="关于RankNet"><a href="#关于RankNet" class="headerlink" title="关于RankNet"></a>关于RankNet</h4><p>搜索引擎排序，RankNet基本是用来定义cost function的，</p>
<h3 id="GBDT在Sklearn中的实现"><a href="#GBDT在Sklearn中的实现" class="headerlink" title="GBDT在Sklearn中的实现"></a>GBDT在Sklearn中的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line">gbdt = GradientBoostingRegressor(loss=<span class="string">'ls'</span>, learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">5</span>, subsample=<span class="number">1</span></span><br><span class="line">                                 , min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, max_depth=<span class="number">3</span></span><br><span class="line">                                 , init=<span class="literal">None</span>, random_state=<span class="literal">None</span>, max_features=<span class="literal">None</span></span><br><span class="line">                                 , alpha=<span class="number">0.9</span>, verbose=<span class="number">0</span>, max_leaf_nodes=<span class="literal">None</span></span><br><span class="line">                                 , warm_start=<span class="literal">False</span></span><br><span class="line">                                 )</span><br></pre></td></tr></table></figure>
<h3 id="GBDT回归任务中常见的损失函数"><a href="#GBDT回归任务中常见的损失函数" class="headerlink" title="GBDT回归任务中常见的损失函数"></a>GBDT回归任务中常见的损失函数</h3><p>对于GBDT回归模型，sklearn中实现了四种损失函数，有均方差’ls’, 绝对损失’lad’, Huber损失’huber’和分位数损失’quantile’。默认是均方差’ls’。一般来说，如果数据的噪音点不多，用默认的均方差’ls’比较好。如果是噪音点较多，则推荐用抗噪音的损失函数’huber’。而如果我们需要对训练集进行分段预测的时候，则采用’quantile’。 </p>
<ol>
<li><p><strong>均方差</strong>：</p>
<script type="math/tex; mode=display">
L(y, f(x)) = (y-f(x))^2</script><p>对应的负梯度误差：$y_{i}-f(x_{i})$</p>
</li>
<li><p><strong>绝对损失</strong>：</p>
<script type="math/tex; mode=display">
L(y, f(x))=|y-f(x)|</script><p>对应负梯度误差：$sign(y_{i}-f(x_{i}))$</p>
</li>
<li><p><strong>Huber损失</strong>: 均方差和绝对损失的折衷， 对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。 </p>
<script type="math/tex; mode=display">
L(y, f(x))=\begin{cases} \frac{1}{2} (y-f(x))^2, & |y - f(x)|\le \delta \\ \delta (|y-f(x)|- \frac {\delta}{2}), & |y - f(x)|\gt \delta \end{cases}</script><p>对应的负梯度误差为：</p>
<script type="math/tex; mode=display">
r(y_{i}, f(x_{i}))=\begin{cases} y_{i}-f(x_{i}), & |y_{i} - f(x_{i})|\le \delta \\ \delta * sign(y_{i}-f(x_{i})), & |y_{i} - f(x_{i})|\gt \delta \end{cases}</script></li>
<li><p><strong>分位数损失</strong>： 对应的是分位数回归的损失函数 </p>
<script type="math/tex; mode=display">
L(y, f(x))=\sum_{y \geq f(x)} \theta|y-f(x)|+\sum_{y<f(x)}(1-\theta)|y-f(x)|</script><p>其中 $ \theta $为分位数，需要指定，对应的负梯度误差为：</p>
<script type="math/tex; mode=display">
r\left(y_{i}, f\left(x_{i}\right)\right)=\left\{\begin{array}{rl}{\theta} & {y_{i} \geq f\left(x_{i}\right)} \\ {\theta-1} & {y_{i}<f\left(x_{i}\right)}\end{array}\right.</script></li>
</ol>
<h3 id="GBDT的正则化：防止过拟合"><a href="#GBDT的正则化：防止过拟合" class="headerlink" title="GBDT的正则化：防止过拟合"></a>GBDT的正则化：防止过拟合</h3><ol>
<li><p><strong>Shrinkage</strong>： 在每次对残差估计进行迭代时，不直接加上当前步所拟合的残差，而是乘以一个系数$\alpha$ 学习率， 对梯度提升的步长进行调整，可以影响设置的回归树个数 。</p>
<p> 加上了正则化项 ：</p>
<script type="math/tex; mode=display">
F_{m}(x)=F_{m-1}(x)+h_{m}(x)  \Rightarrow F_{m}(x)=F_{m-1}(x)+\alpha h_{m}(x)</script><p>$\aloha$的取值范围为$(0, 1]$，较小的$\alpha$ 意味着需要更多的弱学习器的迭代次数 。通常用学习率和迭代最大次数一起来决定算法的拟合效果。  经验上，推荐小一点的<code>learning_rate​</code>会对测试误差(<code>test error</code>)更好。在实际调参中推荐将<code>learning_rate</code>设置为一个小的常数（<code>e.g. learning_rate &lt;= 0.1</code>），并通过<code>early stopping</code>机制来选<code>n_estimators</code>（弱学习器个数）。 </p>
</li>
<li><p><strong>Subsample</strong> ： 通过子采样比例（subsample），取值为 (0,1] 。 这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但会增加样本拟合的偏差，因此取值不能太低。推荐在 [0.5, 0.8]之间。 </p>
<p>使用了子采样的GBDT也称作随机梯度提升树 (Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做Boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。 </p>
</li>
<li><p><strong>对于弱学习器即CART回归树进行正则化剪枝</strong> </p>
</li>
<li><p><strong>Early</strong> <strong>Stopping</strong> ： 具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，控制迭代的轮数（树的个数）。在sklearn的GBDT中可以设置参数n_iter_no_change实现early stopping。 </p>
</li>
<li><p><strong>Dropout</strong> ： Dropout是deep learning里很常用的正则化技巧 ， 具体的做法是：每次新加一棵树，这棵树要拟合的并不是之前全部树ensemble后的残差，而是随机抽取的一些树ensemble；同时新加的树结果要规范化一下。 可参考： AISTATS2015有篇文章《DART: Dropouts meet Multiple Additive Regression Trees》 </p>
</li>
</ol>
<h3 id="有关GBDT常见面试问题"><a href="#有关GBDT常见面试问题" class="headerlink" title="有关GBDT常见面试问题"></a>有关GBDT常见面试问题</h3><h4 id="GBDT与AdaBoost的区别与联系？"><a href="#GBDT与AdaBoost的区别与联系？" class="headerlink" title="GBDT与AdaBoost的区别与联系？"></a>GBDT与AdaBoost的区别与联系？</h4><p>AdaBoost和GBDT都是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过调整错分数据点的权重来改进模型，GBDT是通过计算负梯度来改进模型。因此，相比AdaBoost, GBDT可以使用更多种类的目标函数，而当目标函数是均方误差时，计算损失函数的负梯度值在当前模型的值即为残差。 </p>
<h4 id="GBDT与随机森林（Random-Forest，RF）的区别与联系？"><a href="#GBDT与随机森林（Random-Forest，RF）的区别与联系？" class="headerlink" title="GBDT与随机森林（Random Forest，RF）的区别与联系？"></a>GBDT与随机森林（Random Forest，RF）的区别与联系？</h4><p><strong>相同点：</strong>都是由多棵树组成，最终的结果都是由多棵树一起决定。</p>
<p><strong>不同点：</strong></p>
<ol>
<li>集成的方式：随机森林属于Bagging思想，而GBDT是Boosting思想。</li>
<li>偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差。</li>
<li>训练样本方式：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本。</li>
<li>并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)。</li>
<li>最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合。</li>
<li>数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感。</li>
<li>泛化能力：RF不易过拟合，而GBDT容易过拟合。</li>
</ol>
<h4 id="我们知道残差-真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？"><a href="#我们知道残差-真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？" class="headerlink" title="我们知道残差=真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？"></a>我们知道残差=真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？</h4><ol>
<li><p>在GBDT中，无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损失函数是均方损失时，负梯度刚好是残差，残差只是特例。</p>
</li>
<li><p>GBDT的求解过程就是梯度下降在函数空间中的优化过程。在函数空间中优化，每次得到增量函数，这个函数就是GBDT中一个个决策树，负梯度会拟合这个函数。要得到最终的GBDT模型，只需要把初始值或者初始的函数加上每次的增量即可。参考： <a href="https://mp.weixin.qq.com/s?__biz=MzI5NDMzMjY1MA==&amp;mid=2247485110&amp;idx=1&amp;sn=86bcdb38f51fc5f82236b35c349ada4b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">梯度提升（Gradient Boosting）算法，地址：</a><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDMzMjY1MA==&amp;mid=2247485110&amp;idx=1&amp;sn=86bcdb38f51fc5f82236b35c349ada4b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Ods1PHhYyjkRA8bS16OfCg</a>  </p>
</li>
</ol>
<h4 id="GBDT训练过程"><a href="#GBDT训练过程" class="headerlink" title="GBDT训练过程"></a>GBDT训练过程</h4><img src="/2019/11/04/Kaggle比赛/1.png">
<p> GBDT通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度。</p>
<h4 id="GBDT如何选择特征？"><a href="#GBDT如何选择特征？" class="headerlink" title="GBDT如何选择特征？"></a>GBDT如何选择特征？</h4><p>GBDT选择特征的细节其实是想问CART Tree生成的过程。这里有一个前提，GBDT的弱分类器默认选择的是CART TREE。其实也可以选择其他弱分类器的，选择的前提是低方差和高偏差。框架服从boosting 框架即可。参考： 李航第五章中CART TREE 算法  </p>
<h4 id="GBDT如何构建特征"><a href="#GBDT如何构建特征" class="headerlink" title="GBDT如何构建特征 ?"></a>GBDT如何构建特征 ?</h4><p>其实说GBDT能够构建特征并非很准确，GBDT本身是不能产生特征的，但是可以利用GBDT去产生特征的组合。在CTR预估中，工业界一般会采用<a href="http://www.cnblogs.com/ModifyRong/p/7739955.html#3825035" target="_blank" rel="noopener">逻辑回归</a>去进行处理，逻辑回归本身是适合处理线性可分的数据，如果想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。</p>
<p>Facebook 在2014年发表的一篇论文利用GBDT去产生有效的特征组合，以便用于逻辑回归的训练，提升模型最终的效果。</p>
<img src="/2019/11/04/Kaggle比赛/2.png">
<p>如图所示，我们 使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为 0.</p>
<p>于是对于该样本，我们可以得到一个向量[0,1,0,1,0] 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。</p>
<h4 id="GBDT-如何用于分类-？"><a href="#GBDT-如何用于分类-？" class="headerlink" title="GBDT 如何用于分类 ？"></a>GBDT 如何用于分类 ？</h4><p>首先明确一点，GBDT无论用于分类还是回归一直都是使用的CART回归树。不会因为任务是分类任务就选用分类树，这里面的核心是因为GBDT每轮的训练是在上一轮的训练的残差基础之上进行训练的。这里的残差就是当前模型的负梯度值 。这个要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的。残差相减是有意义的。</p>
<p>如果选用的弱分类器是分类树，类别相减是没有意义的。上一轮输出的是样本 x 属于 A类，本一轮训练输出的是样本 x 属于 B类。 A 和 B 很多时候甚至都没有比较的意义，A 类- B类是没有意义的。</p>
<ol>
<li><p>在训练的时候，是针对样本$x$每个可能的类都训练一个分类回归树。举例说明，目前样本有三类，也就是$K = 3$。样本$x$属于第二类。那么针对该样本$x$类结果，可以用一个 三维向量$ [0,1,0] $来表示。$0$表示样本不属于该类，$1$表示样本属于该类。</p>
</li>
<li><p>针对样本有三类的情况，实质上是在每轮的训练的时候是同时训练三颗树。第一颗树针对样本$x$的第一类，输入为$(x,0)$。第二颗树输入针对样本$x$的第二类，输入为$(x,1)$。第三颗树针对样本$x$的第三类，输入为$(x，0)$。</p>
</li>
<li><p>每颗树的训练过程其实就是CATR TREE 的生成过程。参照之前的生成树的程序即可以就解出三颗树，以及三颗树对$x$类别的预测值$f_{1}(x),f_{2}(x),f_{3}(x)$。那么在此类训练中，仿照多分类的逻辑回归 ，使用softmax来产生概率，则属于类别 1 的概率：</p>
<script type="math/tex; mode=display">
p_{1}=\exp \left(f_{1}(x)\right) / \sum_{k=1}^{3} \exp \left(f_{k}(x)\right)</script><p>针对类别1求出残差$y_{11}(x)=0−p_{1}(x)$;类别2求出残差$y_{22}(x)=1−p_{2}(x)$;类别3求出残差$y_{33}(x)=0−p_{3}(x)$。</p>
</li>
<li><p>开始第二轮训练，针对第一类输入为$(x,y_{11}(x))$, 针对第二类输入为$(x,y_{22}(x))$, 针对 第三类输入为 $(x,y_{33}(x))$，继续训练出三颗树。一直迭代M轮。每轮构建3颗树。 </p>
</li>
<li><p>当$K =3$。三个式子：</p>
<script type="math/tex; mode=display">
F_{1 M}(x)=\sum_{m=1}^{M} \hat{C_{1 m}} I\left(x \epsilon R_{1 m}\right)</script><script type="math/tex; mode=display">
F_{2 M}(x)=\sum_{m=1}^{M} \hat{C_{2 m}} I\left(x \epsilon R_{2 m}\right)</script><script type="math/tex; mode=display">
F_{3 M}(x)=\sum_{m=1}^{M} \hat{C_{3 m}} I\left(x \epsilon R_{3 m}\right)</script></li>
<li><p>训练完毕以后，新来一个样本$x_{1}$ ，需要预测该样本的类别的时候，可以有这三个式子产生三个值，$f_{1}(x),f_{2}(x),f_{3}(x)$。样本属于 某个类别$c$的概率为 :</p>
<script type="math/tex; mode=display">
p_{c}=\exp \left(f_{c}(x)\right) / \sum_{k=1}^{3} \exp \left(f_{k}(x)\right)</script></li>
</ol>
<h4 id="比较LR和GBDT，说说什么情景下GBDT不如LR"><a href="#比较LR和GBDT，说说什么情景下GBDT不如LR" class="headerlink" title="比较LR和GBDT，说说什么情景下GBDT不如LR"></a>比较LR和GBDT，说说什么情景下GBDT不如LR</h4><p>先说说LR和GBDT的区别：</p>
<ul>
<li>LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程</li>
<li>GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；</li>
</ul>
<p>当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下：</p>
<p>先看一个例子：</p>
<blockquote>
<p>假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只有10个正样本1，而这些样本的特征 $f_{1}$的值为全为1，而其余9990条样本的$f_{1}$特征都为0(在高维稀疏的情况下这种情况很常见)。</p>
<p>都知道在这种情况下，树模型很容易优化出一个使用$f_{1}$特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征$f_{1}$只是刚好偶然间跟$y$拟合到了这个规律，这也是我们常说的过拟合。</p>
</blockquote>
<p>那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：$y = W_{1}<em>f_{1} + W_{i}</em>f_{i}+….$，其中 $W_{1}$特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？</p>
<p>仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 $W_{1}$一旦过大，惩罚就会很大，进一步压缩 $W_{1}$的值，使其不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。</p>
<p>这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：<strong>带正则化的线性模型比较不容易对稀疏特征过拟合。</strong></p>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>参考：</p>
<p> <a href="https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ" target="_blank" rel="noopener">终于有人把XGBoost 和 LightGBM 讲明白了，项目中最主流的集成算法！</a></p>
<p><a href="https://mp.weixin.qq.com/s/wLE9yb7MtE208IVLFlZNkw" target="_blank" rel="noopener">XGBoost超详细推导，终于有人讲明白了！</a></p>
<p>XGBoost是大规模并行boosting tree的工具，  Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是<font color="RED">目标函数</font>的定义 。 目前已有的 GBDT 工具基本都是基于预排序的方法（pre-sorted）的决策树算法(如 xgboost)。 </p>
<h3 id="数学推导-1"><a href="#数学推导-1" class="headerlink" title="数学推导"></a>数学推导</h3><img src="/2019/11/04/Kaggle比赛/6.png">
<h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>  XGBoost 是由 $k$ 个基模型组成的一个加法运算式，其中$f_{k}$为第$k$个基模型，$\hat{y}_{i}$为第$i$个样本的预测值：</p>
<script type="math/tex; mode=display">
\hat{y}_{i}=\sum_{t=1}^{k} f_{t}\left(x_{i}\right)</script><p>损失函数可由预测值$\hat{y}_{i}$与真实值$y_{i}$进行表示，其中$n$为样本数量：</p>
<script type="math/tex; mode=display">
L=\sum_{i=1}^n l(y_{i}, \hat{y_{i}})</script><p>常见的损失函数：</p>
<ol>
<li>平方损失函数：$l(y_{i}, \hat{y}_{i}) = (y_{i}-\hat{y}_{i})^2$</li>
<li>逻辑回归损失函数：$l(y_{i}, \hat{y}_{i})=y_{i}ln(1+e^{-\hat{y}_{i}})+(1-y_{i})ln(1+e^{\hat{y}_{i}})$</li>
</ol>
<font color="red"> 模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，方差小需要简单的模型 </font>，目标函数由模型的损失函数 $L$ 与抑制模型复杂度的正则项 $ \Omega $ 组成 ：
$$
O b j=\sum_{i=1}^{n} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{t=1}^{k} \Omega\left(f_{t}\right)
$$
由于Boosting模型是前向加法， 以第 $t$ 个的模型为例，模型对第 $i$个样本 $x_{i}$的预测为： 
$$
\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)
$$
其中 $\hat{y}_{i}^{t-1}$ 由第 $t-1$ 步的模型（第 $t-1$ 棵树）给出的预测值，$f_{t}(x_{i})$是新加入模型的预测值，此时的目标函数可写成：
$$
\begin{aligned} O b j^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+ \Omega(f_{t}) +\sum_{i=1}^{t-1} \Omega\left(f_{i}\right) 
\\ &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+ \Omega(f_{t}) + const \end{aligned}
$$
将正则化项进行拆分，由于前 $t-1$ 棵树的结构已经确定，因此，前 $t-1$ 棵树的复杂度之和可以用一个常量 $const$ 表示。 注意到上式中只有一个变量，即第 $t$ 棵树模型的$f_{t}(x_{i})$，最优化目标函数，就相当于求解  $f_{t}(x_{i})$。


#### 泰勒公式

将一个在 $x = x0$ 处具有 $n$ 阶导数的函数 $f(x)$ 利用关于 $(x-x_{0})$ 的 $n$ 次多项式来逼近。 若函数 $f(x)$ 在包含 $x_{0}$ 的某个闭区间 $[a, b]$ 上具有 $n$ 阶导数，且在开区间 $(a,b)$ 上具有 $n+1$ 阶导数，则对闭区间  $[a, b]$ 上任意一点 $x$ 有：
$$
f(x)=\sum_{i=0}^{n} \frac{f^{(i)}\left(x_{0}\right)}{i !}\left(x-x_{0}\right)^{i}+R_{n}(x)
$$
其中的多项式称为函数在 $x_{0}$处的泰勒展开式，$R_{n}(x)$ 是泰勒公式的余项，且是 $(x-x_{0})^n$ 的高阶无穷小。

函数 $f(x+ \Delta x)$ 在点 $x$ 处进行泰勒公式的二阶展开，形式如下： 
$$
f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}
$$
将损失函数 $l$ 视为上式的 $f(x)$ ，将 $\hat{y}_{i}^{t-1}$ 视为 $x$，将 $f_{t}(x_{i})$ 视为 $\Delta x$，定义损失函数 $l$ 关于 $\hat{y}_{i}^{t-1}$ 的一阶偏导数和二阶偏导数：
$$
g_{i}= \frac {\partial l(y_{i}, \hat{y}^{t-1})}{\partial {\hat{y}^{t-1}}}\\
h_{i}=\frac {\partial^{2} l(y_{i}, \hat{y}^{t-1})}{\partial {\hat{y}^{t-1}}}
$$
损失函数可以表示成如下形式：
$$
l \left (y_{i}, \hat{y}_{i}^{t-1}+f_{t}(x_{i}) \right)=l(y_{i}, \hat{y}_{i}^{t-1})+g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})
$$
由此可以得到目标函数的近似表达式：
$$
Obj^{(t)} \simeq \sum_{i=1}^{n}\left[ l(y_{i}, \hat{y}_{i}^{t-1})+g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})\right]+\Omega (f_{t})+const
$$
由于在第 $t$ 步时 $\hat{y}_{i}^{t-1}$ 已知，所以 $l(y_{i}, \hat{y}_{i}^{t-1})$ 和$const$是一个常量，因此优化目标可写成：
$$
Obj^{(t)} \simeq \sum_{i=1}^{n}\left[g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})\right]+\Omega (f_{t})
$$
 只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 $\hat{y}_{i}^{t-1}$ 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 $f(x)$ ，最后根据加法模型得到一个整体模型。 

#### 基于决策树的目标函数

XGBoost的基模型不仅支持决策树，还支持线性模型。以下主要介绍基于决策树的目标函数。

定义一棵树：

1. 叶子结点的权重向量 $\omega$
2. 实例  —>  叶子结点的映射关系 $q$

决策树的表达式：
$$
f_{t}(x)=w_{q(x)},     w \in \boldsymbol R^{T},q:\boldsymbol R^{d} \to{1, 2, \cdots ,T}
$$
其中 $x$ 为某一样本, $w$ 是长度为 $T$ 的一维向量，$T$ 代表树的叶子结点的个数，代表树 $q$ 各叶子结点的权重，$q$ 代表了一棵树的结构，将输入 $x_{i}\in \boldsymbol R^{d}$ 映射到某叶子结点， $q(x)$ 代表了该样本在哪个叶子结点上，$w_{q}$ 表示叶子结点取值 $\omega$，$\omega_{q(x)}$ 代表了每个样本的取值 $w$。

决策树的复杂度可由叶子数 $T$ 组成，叶子结点越少模型越简单，此外叶子结点也不应该含有过高的权重 $w$ （类比 LR 的每个变量的权重），由此可定义一棵树的复杂度 $\Omega$（即目标函数的正则化项）：
$$
\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}
$$
即决策树模型的复杂度由叶子结点数量 + 叶子结点权重向量的$L2$范数共同决定。

 将属于第 $j$ 个叶子结点的所有样本 $x_{i}$ , 划入到一个叶子结点样本集中，数学表示如下： 
$$
I_{j}=\left\{i | q\left(x_{i}\right)=j\right\}
$$
目标函数可写成如下式子：
$$
\begin{aligned} O b j^{(t)} & \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\ &=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\ &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T \end{aligned} 将训练样本按叶子结点进行分组
$$
 第二步到第三步：第二步是遍历所有的样本后求每个样本的损失函数，但样本最终会落在叶子节点上，所以我们也可以遍历叶子节点，然后获取叶子节点上的样本集合，最后再求损失函数。即之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了 $\sum_{i \in I_{j}} g_{i}$ 和 $\sum_{i \in I_{j}} h_{i}$ 这两项， $w_{j}$为第 $j$ 个叶子节点取值。 

进一步简化表达式，定义：
$$
G_{j} = \sum_{i \in I_{j}} g_{i}, \ H_{j} = \sum_{i \in I_{j}} h_{i}\\
$$

- $G_{j}$ ：叶子结点 $j$ 所包含样本的一阶偏导数累加之和，是一个常量；
- $H_{j}$ ：叶子结点 $j$ 所包含样本的二阶偏导数累加之和，是一个常量；

最终的目标函数：
$$
O b j^{(t)}=\sum_{j=1}^{T}\left[G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2}\right]+\gamma T
$$
此时表达式中只有第 $t$ 棵树的叶子结点权重$w_{j}$是未知量，一元二次函数变量一阶导为$0$的点为最值点（$y=ax^{2}+bx+c$，如果$a>0$，为最小值，若$a<0$为最大值）， 0 分析一下目标函数$obj$，可以发现，各个叶子结点的目标子式是相互独立的，也就是说，当每个叶子结点的子式都达到最值点时，整个目标函数式$obj$才达到最值点。 将目标函数对 $w_{j}$ 求一阶导，并令其等于 $0$ ，则可以求得叶子结点 $j$ 对应的权值： $$ w_{j}^{*}="-\frac{G_{j}}{H_{j}+\lambda}" 目标函数可以化简为： o b j="-\frac{1}{2}" \sum_{j="1}^{T}" \frac{g_{j}^{2}}{h_{j}+\lambda}+\gamma t #### 最优切分点划分算法 在决策树的生长过程中，一个非常关键的问题是如何找到叶子节点的最优切分点，xgboost 支持两种分裂节点的方法——贪心算法和近似算法。 ##### 贪心算法 全局扫描法 从深度为 的树开始： 1. 对树中的每个叶子结点尝试进行分裂； 2. 每次分裂后，原来的一个叶子结点继续分裂为左右两个子叶子结点，原叶子结点中的样本集将根据该结点的判断规则分散到左右两个叶子结点中； 3. 新分裂一个结点后，我们需要检测这次分裂是否会给损失函数带来增益 假设在某一节点完成特征分裂，则分裂前的目标函数可以写为： j_{l+r}="-\frac{1}{2}" \frac{\left(g_{l}+g_{r}\right)^{2}}{h_{l}+h_{r}+\lambda}+\gamma 分裂后的目标函数为： j_{l}+o j_{r}="-\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}\right]+2" \gamma 目标函数来说，每个特征分裂后的收益（ 该特征收益也可作为特征重要性输出的重要依据 ）： \begin{aligned} \text {gain} &="O" j_{l+r}-\left(o j_{r}\right) \\ \frac{\left(g_{l}+g_{r}\right)^{2}}{h_{l}+h_{r}+\lambda}+\gamma\right]-\left[-\frac{1}{2}\left(\frac{g_{l}^{2}}{h_{l}+\lambda}+\frac{g_{r}^{2}}{h_{r}+\lambda}\right)+2 \gamma\right] \end{aligned} 增益$gain>0$，即分裂为两个叶子节点后，目标函数下降了，会考虑此次分裂的结果。但是，在一个结点分裂时，可能有很多个分裂点，每个分裂点都会产生一个增益，如何才能寻找到最优的分裂点呢？ 

寻找最佳分割点的大致步骤如下： 

1. 对每个叶节点枚举所有的可用特征（遍历每个结点的所有特征 ）；
2. 针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；
3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集
4. 回到第 1 步，递归执行到满足特定条件为止

##### 近似算法

当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。 基于此，XGBoost提出了一系列加快寻找最佳分裂点的方案： 

- **特征预排序+缓存：**XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构， 并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储， 后面的迭代中会重复地使用这个结构，使计算量大大减小。

  - 特征预排序：每一个块结构包括一个或多个已经排序好的特征；缺失特征值将不进行排序；每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；

    <img src="/2019/11/04/Kaggle比赛/5.png">

  - 缓存访问优化算法：块结构的设计可以减少节点分裂时的计算量，但特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续，这样会造成缓存命中率低，从而影响到算法的效率。为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。此外适当调整块大小，也可以有助于缓存优化。

- **"核外"快计算：**当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行。

  此外，XGBoost 还用了两种方法来降低硬盘读写的开销：

  - 块压缩：对 Block 进行按列压缩，并在读取时进行解压；

  - 块拆分：将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。

- **分位点近似法：**对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。
- **并行查找：**由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。

该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。

在提出候选切分点时有两种策略：

- Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；
- Local：每次分裂前将重新提出候选切分点。

直观上来看，Local 策略需要更多的计算步骤，而 Global 策略因为节点没有划分所以需要更多的候选点。

事实上， XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值 $h_{i}$ 作为样本的权重进行划分：

1. 按照样本个数进行分位，三分位为例：

   <img src="/2019/11/04/Kaggle比赛/3.png">

2.  以二阶导数值 $h_{i}$ 作为样本的权重进行划分:

   <img src="/2019/11/04/Kaggle比赛/4.png">

为什么要用 $h_{i}$ 进行样本加权：

模型的目标函数为： 
$$
Obj^{(t)} \simeq \sum_{i=1}^{n}\left[g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})\right]+\Omega (f_{t})
$$

稍作整理，便可以看出 $h_{i}$ 有对 loss 加权的作用：

$$
\begin{aligned} O b j^{(t)} & \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right) \color{red} + \frac{1}{2} \frac{g_{i}^{2}}{h_{i}}\right]+\Omega\left(f_{t}\right)+const \\ &=\sum_{i=1}^{n} \color{red}\frac{1}{2} h_{i}\left[f_{t}\left(x_{i}\right)-\left(-\frac{g_{i}}{h_{i}}\right)\right]^{2}+\Omega\left(f_{t}\right)+const \end{aligned}
$$
其中 $h_{i}$ 与 $const$ 皆为常数。可以看到 $h_{i}$ 就是平方损失函数中样本的权重。 <font color="red">[不是很懂]</font>

<h5 id="树停止生长"><a href="#树停止生长" class="headerlink" title="树停止生长"></a>树停止生长</h5><ol>
<li>当新引入的一次分裂所带来的增益 $Gain&lt;0$ 时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 </li>
<li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数 max_depth。 </li>
<li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数：最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施。 </li>
</ol>
<h3 id="稀疏感知算法"><a href="#稀疏感知算法" class="headerlink" title="稀疏感知算法"></a>稀疏感知算法</h3><p>XGBoost 在决策树应对数据缺失时给出的分裂策略，XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。</p>
<p>在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而特征值缺失的样本无需遍历只需直接分配到左右节点，故算法所需遍历的样本量减少。</p>
<h3 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost  对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</li>
<li>灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li>
<li>正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</li>
<li>Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li>
<li>列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</li>
<li>缺失值处理：XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；</li>
<li>可以并行化操作：块结构可以很好的支持并行计算。</li>
</ol>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li>
<li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。</li>
</ol>
<h3 id="高频面试题"><a href="#高频面试题" class="headerlink" title="高频面试题"></a>高频面试题</h3><h4 id="XGB与GBDT、随机森林等模型相比，有什么优缺点"><a href="#XGB与GBDT、随机森林等模型相比，有什么优缺点" class="headerlink" title="XGB与GBDT、随机森林等模型相比，有什么优缺点?"></a>XGB与GBDT、随机森林等模型相比，有什么优缺点?</h4><p>   与GBDT模型相比：</p>
<ul>
<li><strong>基分类器</strong>：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。</li>
<li><strong>导数信息</strong>：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。</li>
<li><strong>正则项</strong>：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。</li>
<li><strong>列抽样</strong>：XGBoost支持列采样，与随机森林类似，用于防止过拟合。</li>
<li><strong>缺失值处理</strong>：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。</li>
<li><strong>并行化</strong>：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</li>
</ul>
<h4 id="XGB为什么可以并行训练？"><a href="#XGB为什么可以并行训练？" class="headerlink" title="XGB为什么可以并行训练？"></a>XGB为什么可以并行训练？</h4><ul>
<li>XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。</li>
<li>XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。</li>
</ul>
<h4 id="XGB用二阶泰勒展开的优势在哪？"><a href="#XGB用二阶泰勒展开的优势在哪？" class="headerlink" title="XGB用二阶泰勒展开的优势在哪？"></a>XGB用二阶泰勒展开的优势在哪？</h4><ul>
<li><strong>精准性</strong>：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数</li>
<li><strong>可扩展性</strong>：损失函数支持自定义，只需要新的损失函数二阶可导。</li>
</ul>
<h4 id="XGB为了防止过拟合，进行了哪些设计？"><a href="#XGB为了防止过拟合，进行了哪些设计？" class="headerlink" title="XGB为了防止过拟合，进行了哪些设计？"></a>XGB为了防止过拟合，进行了哪些设计？</h4><p>XGBoost在设计时，为了防止过拟合做了很多优化，具体如下：</p>
<ul>
<li><strong>目标函数添加正则项</strong>：叶子节点个数+叶子节点权重的L2正则化</li>
<li><strong>列抽样</strong>：训练的时候只用一部分特征（不考虑剩余的block块即可）</li>
<li><strong>子采样</strong>：每轮计算可以不使用全部样本，使算法更加保守</li>
<li><strong>shrinkage</strong>: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间</li>
</ul>
<h4 id="XGB如何处理缺失值？"><a href="#XGB如何处理缺失值？" class="headerlink" title="XGB如何处理缺失值？"></a>XGB如何处理缺失值？</h4><p>XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：</p>
<ul>
<li>在特征$k$上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。</li>
<li>在逻辑实现上，为了保证完备性，会将该特征值 missing 的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。</li>
<li>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。</li>
</ul>
<h4 id="为什么XGBoost相比某些模型对缺失值不敏感"><a href="#为什么XGBoost相比某些模型对缺失值不敏感" class="headerlink" title="为什么XGBoost相比某些模型对缺失值不敏感"></a>为什么XGBoost相比某些模型对缺失值不敏感</h4><p>对存在缺失值的特征，一般的解决方法是：</p>
<ul>
<li>离散型变量：用出现次数最多的特征值填充；</li>
<li>连续型变量：用中位数或均值填充；</li>
</ul>
<p>一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。而树模型对缺失值的敏感度低，大部分时候可以在数据缺失时使用。原因就是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。</p>
<p>XGBoost对缺失数据有特定的处理方法，因此，对于有缺失值的数据在经过缺失处理后：</p>
<ul>
<li>当数据量很小时，优先用朴素贝叶斯</li>
<li>数据量适中或者较大，用树模型，优先XGBoost</li>
<li>数据量较大，也可以用神经网络</li>
<li>避免使用距离度量相关的模型，如KNN和SVM</li>
</ul>
<h4 id="XGB如何分裂一个结点？如何选择特征？"><a href="#XGB如何分裂一个结点？如何选择特征？" class="headerlink" title="XGB如何分裂一个结点？如何选择特征？"></a>XGB如何分裂一个结点？如何选择特征？</h4><p>XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。</p>
<p>如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。</p>
<h4 id="XGB中一颗树停止生长的条件有哪些？"><a href="#XGB中一颗树停止生长的条件有哪些？" class="headerlink" title="XGB中一颗树停止生长的条件有哪些？"></a>XGB中一颗树停止生长的条件有哪些？</h4><ul>
<li>当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。</li>
<li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。</li>
<li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。</li>
</ul>
<h4 id="XGB叶子结点的权重有什么含义？如何计算？"><a href="#XGB叶子结点的权重有什么含义？如何计算？" class="headerlink" title="XGB叶子结点的权重有什么含义？如何计算？"></a>XGB叶子结点的权重有什么含义？如何计算？</h4><p>XGBoost目标函数最终推导形式如下：</p>
<script type="math/tex; mode=display">
O b j^{(t)}=\sum_{j=1}^{T}\left[G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2}\right]+\gamma T</script><p>利用一元二次函数求最值的知识，当目标函数达到最小值$Obj^{<em>}$时，每个叶子结点的权重为$w_{j}^{</em>}$。</p>
<p>具体公式如下：</p>
<script type="math/tex; mode=display">
w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda} \quad, \quad O b j=-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T</script><h4 id="训练一个XGB模型，经历了哪些过程？调参步骤是什么？"><a href="#训练一个XGB模型，经历了哪些过程？调参步骤是什么？" class="headerlink" title="训练一个XGB模型，经历了哪些过程？调参步骤是什么？"></a>训练一个XGB模型，经历了哪些过程？调参步骤是什么？</h4><p>首先需要初始化一些基本变量，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- max_depth = 5</span><br><span class="line">- min_child_weight = 1</span><br><span class="line">- gamma = 0</span><br><span class="line">- subsample, colsample_bytree = 0.8</span><br><span class="line">- scale_pos_weight = 1</span><br></pre></td></tr></table></figure>
<p><strong>(1) 确定learning rate和estimator的数量</strong></p>
<p>&emsp;&emsp;learning rate可以先用0.1，用cv来寻找最优的estimators</p>
<p><strong>(2) max_depth和 min_child_weight</strong></p>
<p>&emsp;&emsp;调整这两个参数是因为，这两个参数对输出结果的影响很大。首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。</p>
<p>&emsp;&emsp;max_depth，每棵子树的最大深度，check from range(3,10,2)。</p>
<p>&emsp;&emsp;min_child_weight，子节点的权重阈值，check from range(1,6,2)。</p>
<p>&emsp;&emsp;如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。</p>
<p><strong>(3) gamma</strong></p>
<p>&emsp;&emsp;也称作最小划分损失<code>min_split_loss</code>，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。</p>
<ul>
<li>如果大于该阈值，则该叶子节点值得继续划分</li>
<li>如果小于该阈值，则该叶子节点不值得继续划分</li>
</ul>
<p><strong>(4) subsample, colsample_bytree</strong></p>
<p>&emsp;&emsp;subsample是对训练的采样比例</p>
<p>&emsp;&emsp;colsample_bytree是对特征的采样比例</p>
<p>&emsp;&emsp;both check from 0.6 to 0.9</p>
<p><strong>(5) 正则化参数</strong></p>
<p>&emsp;&emsp;$alpha$ 是L1正则化系数，try $1e-5, 1e-2, 0.1, 1, 100$</p>
<p>&emsp;&emsp;$lambda$ 是L2正则化系数</p>
<p><strong>(6) 降低学习率</strong></p>
<p>&emsp;&emsp;降低学习率的同时增加树的数量，通常最后设置学习率为$0.01~0.1$</p>
<h4 id="XGB如何给特征评分？"><a href="#XGB如何给特征评分？" class="headerlink" title="XGB如何给特征评分？"></a>XGB如何给特征评分？</h4><p> 采用三种方法来评判XGBoost模型中特征的重要程度： </p>
<ul>
<li><strong>weight</strong> ：该特征在所有树中被用作分割样本的特征的总次数。</li>
<li><strong>gain</strong> ：该特征在其出现过的所有树中产生的平均增益。</li>
<li><strong>cover</strong> ：该特征在其出现过的所有树中的平均覆盖范围。</li>
</ul>
<blockquote>
<p>注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。</p>
</blockquote>
<h4 id="XGBoost如何处理不平衡数据"><a href="#XGBoost如何处理不平衡数据" class="headerlink" title="XGBoost如何处理不平衡数据"></a>XGBoost如何处理不平衡数据</h4><p>对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决：</p>
<p>第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置<em>scale_pos_weight</em>来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，<em>scale_pos_weight</em>可以取10；</p>
<p>第二种，如果你在意概率(预测得分的合理性)，不能重新平衡数据集(会破坏数据的真实分布)，应该设置<em>max_delta_step</em>为一个有限数字来帮助收敛（基模型为LR时有效）。</p>
<p>那么，源码到底是怎么利用<strong>scale_pos_weight</strong>来平衡样本的呢，是调节权重还是过采样呢？请看源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (info.labels[i] == <span class="number">1.0</span>f)  w *= param_.scale_pos_weight</span><br></pre></td></tr></table></figure>
<p>可以看出，应该是增大了少数样本的权重。</p>
<p>除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。</p>
<h4 id="XGBoost中如何对树进行剪枝"><a href="#XGBoost中如何对树进行剪枝" class="headerlink" title="XGBoost中如何对树进行剪枝"></a>XGBoost中如何对树进行剪枝</h4><ul>
<li>在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。</li>
<li>在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。</li>
<li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。</li>
<li>XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。</li>
</ul>
<h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><p>参考：</p>
<p> <a href="https://mp.weixin.qq.com/s/fDwpJxSwWXWjV0y0mtKxAQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/fDwpJxSwWXWjV0y0mtKxAQ</a> </p>
<p> <a href="https://mp.weixin.qq.com/s/441iM0snbfHznewbktUsvQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/441iM0snbfHznewbktUsvQ</a> </p>
<p> <a href="https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ</a> </p>
<p>LightGBM  （Light Gradient Boosting Machine） 由微软提出，用于解决GBDT在海量数据中遇到的问题。 其相对 XGBoost 具有训练速度快、内存占用低的特点。 GBDT在工业界通常被用于点击预测，搜索排序等任务。LightGBM是实现GBDT算法的框架，支持高效的并行训练。</p>
<h3 id="LightGBM在相比XGBoost哪些地方进行了优化？"><a href="#LightGBM在相比XGBoost哪些地方进行了优化？" class="headerlink" title="LightGBM在相比XGBoost哪些地方进行了优化？"></a>LightGBM在相比XGBoost哪些地方进行了优化？</h3><ol>
<li>单边梯度抽样算法；</li>
<li>直方图算法；</li>
<li>互斥特征捆绑算法；</li>
<li>基于最大深度的 Leaf-wise 的垂直生长算法；</li>
<li>类别特征最优分割；</li>
<li>特征并行和数据并行；</li>
<li>缓存优化。</li>
</ol>
<h3 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h3><h5 id="单边梯度抽样算法"><a href="#单边梯度抽样算法" class="headerlink" title="单边梯度抽样算法"></a>单边梯度抽样算法</h5><p>GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法（Gradient-based One-Side Sampling, GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算中只需关注梯度高的样本，极大的减少了计算量。</p>
<p>GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样，为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。 GOSS 事先基于梯度的绝对值对样本进行排序（无需保存排序后结果），然后拿到前 $a \%$ 的梯度大的样本，和剩下样本的 $b\%$，在计算增益时，通过乘上 $\frac{1-a}{b}$来放大梯度小的样本的权重。一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。 </p>
<h5 id="直方图算法"><a href="#直方图算法" class="headerlink" title="直方图算法"></a>直方图算法</h5><h6 id="直方图算法-1"><a href="#直方图算法-1" class="headerlink" title="直方图算法"></a>直方图算法</h6><p>直方图算法的基本思想是将连续的特征离散化为 $k$ 个离散特征 （其实又是分桶的思想，而这些桶称为bin，比如[0,0.1)→0, [0.1,0.3)→1） ，同时构造一个宽度为 $k$ 的直方图用于统计信息（含有 $k$ 个 bin）。利用直方图算法无需遍历数据，只需要遍历 $k$ 个 bin 即可找到最佳分裂点。</p>
<p>我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）：</p>
<ul>
<li>内存占用更小：XGBoost 需要用 32 位的浮点数去存储特征值，并用 32 位的整形去存储索引，而 LightGBM 只需要用 8 位去存储直方图，相当于减少了 1/8；</li>
<li>计算代价更小：计算特征分裂增益时，XGBoost 需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k 次，直接将时间复杂度从 $O(data  <em> feature) $降低到 $O(k  </em> feature)$  ， $data &gt;&gt; k$ 。</li>
</ul>
<p>虽然将特征离散化后无法找到精确的分割点，可能会对模型的精度产生一定的影响，但较粗的分割也起到了正则化的效果，一定程度上降低了模型的方差。</p>
<h6 id="直方图加速"><a href="#直方图加速" class="headerlink" title="直方图加速"></a>直方图加速</h6><p>在构建叶节点的直方图时，还可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。在实际操作过程中，还可以先计算直方图小的叶子节点，然后利用直方图作差来获得直方图大的叶子节点。 </p>
<img src="/2019/11/04/Kaggle比赛/7.png">
<h6 id="稀疏特征优化"><a href="#稀疏特征优化" class="headerlink" title="稀疏特征优化"></a>稀疏特征优化</h6><p> XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。 </p>
<h5 id="带深度限制的-Leaf-wise-算法"><a href="#带深度限制的-Leaf-wise-算法" class="headerlink" title="带深度限制的 Leaf-wise 算法"></a>带深度限制的 Leaf-wise 算法</h5><p>在建树的过程中有两种策略：</p>
<ul>
<li><em>Level-wise</em>：基于层进行生长，直到达到停止条件；</li>
<li><em>Leaf-wise</em>：每次分裂增益最大的叶子节点，直到达到停止条件。</li>
</ul>
<img src="/2019/11/04/Kaggle比赛/8.png">
<p>XGBoost 采用 <em>Level-wise</em> 的增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂，降低了计算量；LightGBM 采用 <em>Leaf-wise</em> 的增长策略减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂。</p>
<h5 id="类别特征最优分割"><a href="#类别特征最优分割" class="headerlink" title="类别特征最优分割"></a>类别特征最优分割</h5><p>大部分的机器学习算法都不能直接支持类别特征，一般都会对类别特征进行编码，然后再输入到模型中。常见的处理类别特征的方法为 one-hot 编码，但对于决策树来说并不推荐使用 one-hot 编码：</p>
<ol>
<li>会产生样本切分不平衡问题，切分增益会非常小。如，国籍切分后，会产生是否中国，是否美国等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0。这种划分的增益非常小：较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零；</li>
<li><p>影响决策树学习：决策树依赖的是数据的统计信息，而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上统计信息不准确的，学习效果变差。本质是因为独热码编码之后的特征的表达能力较差的，特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败，最终该特征得到的重要性会比实际值低。</p>
<p>LightGBM 原生支持类别特征，采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设有某维特征有 $k$ 个类别，则有 $2^{(k-1)} - 1$ 中可能，时间复杂度为 $O(2^k)$ ，LightGBM 实现了 $O(klog_2k)$ 的时间复杂度。 </p>
</li>
</ol>
<img src="/2019/11/04/Kaggle比赛/9.png">
<p>上图为左边为基于 one-hot 编码进行分裂，后图为 LightGBM 基于 many-vs-many 进行分裂，在给定深度情况下，后者能学出更好的模型。其基本思想在于每次分组时都会根据训练目标对类别特征进行分类，根据其累积值 $\frac{\sum gradient }{\sum hessian}$ 对直方图进行排序，然后在排序的直方图上找到最佳分割。此外，LightGBM 还加了约束条件正则化，防止过拟合。</p>
<h3 id="XGBoost和LightGBM的区别"><a href="#XGBoost和LightGBM的区别" class="headerlink" title="XGBoost和LightGBM的区别"></a>XGBoost和LightGBM的区别</h3><ol>
<li><p>树生长策略：XGB采用<code>level-wise</code>的分裂策略，LGB采用<code>leaf-wise</code>的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。<em>Leaf-wise</em>是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。</p>
</li>
<li><p>分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下：</p>
<ul>
<li><p>减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。</p>
</li>
<li><p>计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为$𝑂(𝑓𝑒𝑎𝑡𝑢𝑟𝑒<em>𝑑𝑎𝑡𝑎)$。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为$𝑂(𝑓𝑒𝑎𝑡𝑢𝑟𝑒</em>b𝑖𝑛𝑠)$。</p>
</li>
<li><p>LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算</p>
<blockquote>
<p>但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？</p>
<p>xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。</p>
</li>
<li><p>缓存命中率：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</p>
</li>
<li><p>LightGBM 与 XGboost 的并行策略不同：</p>
<ul>
<li><strong>特征并行</strong> ：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。</li>
<li><strong>数据并行</strong> ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。</li>
<li><strong>投票并行（LGB）</strong>：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。</li>
</ul>
</li>
</ol>
<h2 id="四、BigQuery-Geotab-Intersection-Congestion"><a href="#四、BigQuery-Geotab-Intersection-Congestion" class="headerlink" title="四、BigQuery-Geotab Intersection Congestion"></a>四、BigQuery-Geotab Intersection Congestion</h2><p>BigQuery-Geotab交叉路口拥堵：预测主要城市十字路口的等待时间</p>
<h3 id="1-基本介绍"><a href="#1-基本介绍" class="headerlink" title="1. 基本介绍"></a>1. 基本介绍</h3><h4 id="1-1-概要"><a href="#1-1-概要" class="headerlink" title="1.1 概要"></a>1.1 概要</h4><p>该比赛的数据集包括总计停止的车辆信息和交叉路口等待时间。任务是根据美国四个主要城市（亚特兰大，波士顿，芝加哥和费城）的交叉路口的停车距离和等待时间的总量来预测交通拥堵。</p>
<p>这项比赛是与BigQuery合作举办的，BigQuery是一个用于操纵，联接和查询大规模表格数据集的数据仓库。 BigQuery还提供BigQuery ML，这是用户创建和运行机器学习模型以通过SQL查询界面生成预测的简便方法。</p>
<p>Kaggle最近在我们的内核笔记本环境中发布了BigQuery集成，该入门内核为您提供了使用BQ和BQML的绝佳起点。建议使用精明的数据，机智和直觉来查找并加入其他外部数据集，以提高模型的预测能力。</p>
<h4 id="1-2-评估"><a href="#1-2-评估" class="headerlink" title="1.2 评估"></a>1.2 评估</h4><p>根据均方根误差对提交的内容评分。RMSE定义为：</p>
<script type="math/tex; mode=display">
\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}</script><p>其中$\hat{y}$是预测值，$y$是原始值。</p>
<p>提交文件：</p>
<p>对于测试集中的每一行，必须按照“数据”选项卡上的描述预测六个目标结果的值，每个目标结果都在提交文件中的单独一行中。 该文件应包含标头，并具有以下格式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ID</th>
<th>TARGET</th>
</tr>
</thead>
<tbody>
<tr>
<td>0_1</td>
<td>0</td>
</tr>
<tr>
<td>0_2</td>
<td>0</td>
</tr>
<tr>
<td>0_3</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<h4 id="1-3-资源"><a href="#1-3-资源" class="headerlink" title="1.3 资源"></a>1.3 资源</h4><p>通过内核使用 BigQuery 和 BigQuery ML：除了可以在 Kaggle 上下载之外，该比赛的数据集还托管在一个私有的 BigQuery 数据集中。 您可以按照“数据”页面上 BigQuery-Dataset-Access.md 文档中的说明进行访问。在“设置”面板中，可以在 Kaggle 内核/笔记本上启用 BigQuery。</p>
<p>入门资源：本入门教程向您展示如何开始在 BigQuery ML上运行查询。</p>
<h4 id="1-4-数据描述"><a href="#1-4-数据描述" class="headerlink" title="1.4 数据描述"></a>1.4 数据描述</h4><p>数据包括来自商用车辆（例如半卡车）的汇总行程记录指标。数据已按照交叉路口，月份，一天中的小时，通过交叉路口的方向以及一天是否在周末进行了分组。</p>
<p>对于测试集中的每个分组，需要针对两个不同度量标准的三个不同分位数进行预测，以涵盖该组车辆驶过交叉路口需要多长时间。具体来说，总时间的20％，50％和80％在交叉路口停止，并且交叉路口与车辆在等待时停止的第一位置之间的距离。您可以将目标视为总结每个交叉路口的等待时间和停车距离的分布。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">RowId                int64</span><br><span class="line">IntersectionId       int64</span><br><span class="line">Latitude           float64</span><br><span class="line">Longitude          float64</span><br><span class="line">EntryStreetName     object</span><br><span class="line">ExitStreetName      object</span><br><span class="line">EntryHeading        object</span><br><span class="line">ExitHeading         object</span><br><span class="line">Hour                 int64</span><br><span class="line">Weekend              int64</span><br><span class="line">Month                int64</span><br><span class="line">Path                object</span><br><span class="line">City                object</span><br></pre></td></tr></table></figure>
<p>这六个预测中的每个预测都在提交文件中的新行中进行。 读取提交的TargetId字段，例如1_1，第一个数字是RowId，第二个数字是度量标准ID。 可以使用submission_metric_map.json解压缩提交指标ID代码。</p>
<p>训练集包括一个可选的附加输出指标（TimeFromFirstStop），以防它对构建模型有用。 它仅从测试集中排除，以限制必须进行的预测的次数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">RowId                        int64</span><br><span class="line">IntersectionId               int64</span><br><span class="line">Latitude                   float64</span><br><span class="line">Longitude                  float64</span><br><span class="line">EntryStreetName             object</span><br><span class="line">ExitStreetName              object</span><br><span class="line">EntryHeading                object</span><br><span class="line">ExitHeading                 object</span><br><span class="line">Hour                         int64</span><br><span class="line">Weekend                      int64</span><br><span class="line">Month                        int64</span><br><span class="line">Path                        object</span><br><span class="line">TotalTimeStopped_p20         int64</span><br><span class="line">TotalTimeStopped_p40         int64</span><br><span class="line">TotalTimeStopped_p50         int64</span><br><span class="line">TotalTimeStopped_p60         int64</span><br><span class="line">TotalTimeStopped_p80         int64</span><br><span class="line">TimeFromFirstStop_p20        int64</span><br><span class="line">TimeFromFirstStop_p40        int64</span><br><span class="line">TimeFromFirstStop_p50        int64</span><br><span class="line">TimeFromFirstStop_p60        int64</span><br><span class="line">TimeFromFirstStop_p80        int64</span><br><span class="line">DistanceToFirstStop_p20    float64</span><br><span class="line">DistanceToFirstStop_p40    float64</span><br><span class="line">DistanceToFirstStop_p50    float64</span><br><span class="line">DistanceToFirstStop_p60    float64</span><br><span class="line">DistanceToFirstStop_p80    float64</span><br><span class="line">City                        object</span><br></pre></td></tr></table></figure>
<h3 id="2-实现"><a href="#2-实现" class="headerlink" title="2. 实现"></a>2. 实现</h3><h4 id="2-1-导入库"><a href="#2-1-导入库" class="headerlink" title="2.1 导入库"></a>2.1 导入库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1. scipy：建立在Numpy基础上，用于数值运算，如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵。scipy.stats中包含很多常见的分布</span><br><span class="line">        rvs产生服从制定分布的随机数</span><br><span class="line">        pdf概率密度函数</span><br><span class="line">        cdf累计分布函数</span><br><span class="line">        sf残存函数(1-cdf)</span><br><span class="line">        ppf分位点函数（CDF的你函数）</span><br><span class="line">        isf逆残存函数</span><br><span class="line">        fit对随机取样进行你和，最大似然估计找到的概率密度函数系数</span><br><span class="line">2. Seaborn：对matplotlib的extend，是一个数据可视化库，提供更高级的API封装，在应用中更加的方便灵活。</span><br><span class="line">3. plotly：一款基于D3.js框架的Python库，非常著名且强大的开源数据可视化框架，通过构建基于浏览器显示的web形式的可交互图表来展示信息，可创建多达数十种精美的图表和地图。</span><br><span class="line">4. skleran: sklearn.metrics.mean_squared_error计算均方误差回归误差</span><br><span class="line">sklearn.preprocessing.LabelEncoder可以将标签分配一个0—n_classes-1之间的编码</span><br><span class="line">5. functools：为了高阶函数（该高阶函数的定义为作用于或返回其它函数的函数）而设置的。一般来说，任何可调用的对象在该模块中都可被当做函数而处理。functools.partial 通过包装手法，允许我们 &quot;重新定义&quot; 函数签名，用一些默认参数包装一个可调用对象,返回结果是可调用对象，并且可以像原始对象一样对待。</span><br><span class="line">6. hyperopt: 帮助快速进行机器学习模型参数调试。通常情况下有两种类型的参数调试方法，网格搜索（grid search）和随机搜索（random search）。网格搜索速度慢但是适用于需要随整个参数空间进行搜索的情况；随机搜索速度很快但是容易遗漏一些重要信息。Hyheropt四个重要的因素：指定需要最小化的函数，搜索的空间，采样的数据集(trails database)（可选），搜索的算法（可选）。</span><br></pre></td></tr></table></figure>
<h4 id="2-2-部分函数解读"><a href="#2-2-部分函数解读" class="headerlink" title="2.2 部分函数解读"></a>2.2 部分函数解读</h4><h5 id="pd-DataFrame"><a href="#pd-DataFrame" class="headerlink" title="pd.DataFrame()"></a>pd.DataFrame()</h5><p>在pandas里，DataFrame是最经常用的数据结构，这里总结生成和添加数据的方法：</p>
<ol>
<li><p>把其他格式的数据整理到DataFrame中；</p>
<p>① 字典类型读取到DataFrame: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">test_dict = &#123;<span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],<span class="string">'name'</span>:[<span class="string">'Alice'</span>,<span class="string">'Bob'</span>,<span class="string">'Cindy'</span>],<span class="string">'math'</span>:[<span class="number">90</span>,<span class="number">89</span>,<span class="number">99</span>],<span class="string">'english'</span>:[<span class="number">89</span>,<span class="number">94</span>,<span class="number">80</span>]&#125;</span><br><span class="line"><span class="comment"># 直接使用`pd.DataFrame(data=test_dict)`即可,括号中的`data=`写不写都可以 。</span></span><br><span class="line">test_dict_df = pd.DataFrame(test_dict) <span class="comment">#[1].直接写入参数test_dict</span></span><br><span class="line">test_dict_df = pd.DataFrame(data=test_dict) <span class="comment">#[2].字典型赋值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用from_dict方法</span></span><br><span class="line">test_dict_df = pd.DataFrame.from_dict(test_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：如果你的dict变量很小，例如&#123;'id':1,'name':'Alice'&#125;,想直接写到括号里：</span></span><br><span class="line"><span class="comment"># test_dict_df = pd.DataFrame(&#123;'id':1,'name':'Alice'&#125;) 会报错ValueError: If using all scalar values, you must pass an index</span></span><br><span class="line">test_dict_df = pd.DataFrame(&#123;<span class="string">'id'</span>:<span class="number">1</span>,<span class="string">'name'</span>:<span class="string">'Alice'</span>&#125;,pd.Index(range(<span class="number">1</span>)))</span><br><span class="line">pd.DataFrame(&#123;<span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>],<span class="string">'name'</span>:[<span class="string">'Alice'</span>,<span class="string">'Bob'</span>]&#125;,pd.Index(range(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只需要选择dict中部分的键当做DataFrame的列，可以使用columns参数</span></span><br><span class="line">test_dict_df = pd.DataFrame(data=test_dict,columns=[<span class="string">'id'</span>,<span class="string">'name'</span>])</span><br></pre></td></tr></table></figure>
<p>② csv文件构建DataFrame</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最常用的应该就是pd.read_csv('filename.csv')了，用sep指定数据的分割方式，默认的是','</span></span><br><span class="line">df = pd.read_csv(<span class="string">'./xxx.csv'</span>) <span class="comment"># 如果csv中没有表头，就要加入head参数</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在已有的DataFrame中插入N列或者N行。 </p>
<p>① 添加列 insert方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_columns = [<span class="number">92</span>,<span class="number">94</span>,<span class="number">89</span>,<span class="number">77</span>,<span class="number">87</span>,<span class="number">91</span>]</span><br><span class="line">test_dict_df.insert(<span class="number">2</span>,<span class="string">'pyhsics'</span>,new_columns)</span><br><span class="line"><span class="comment"># DataFrame默认不允许添加重复的列，但是在insert函数中有参数 allow_duplicates=True，设置为True后，就可以添加重复的列了，列名也是重复的：</span></span><br></pre></td></tr></table></figure>
<p>② 添加行 loc方法或者append方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">new_line = [<span class="number">7</span>,<span class="string">'Iric'</span>,<span class="number">99</span>]</span><br><span class="line"><span class="comment"># 但是十分注意的是，这样实际是改的操作，如果loc[index]中的index已经存在，则新的值会覆盖之前的值。</span></span><br><span class="line">test_dict_df.loc[<span class="number">3</span>]= new_line </span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以把这些新的数据构建为一个新的DataFrame，然后两个DataFrame拼起来。可以用append方法</span></span><br><span class="line">test_dict_df.append(pd.DataFrame([new_line],columns=[<span class="string">'id'</span>,<span class="string">'name'</span>,<span class="string">'physics'</span>]))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="df-reset-index"><a href="#df-reset-index" class="headerlink" title="df.reset_index()"></a>df.reset_index()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据清洗时，会将带空值的行删除，此时DataFrame或Series类型的数据不再是连续的索引，可以使用reset_index()重置索引。 在获得新的index，原来的index变成数据列，保留下来。不想保留原来的index，使用参数 drop=True，默认 False。</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df.reset_index()</span><br><span class="line">df.reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h5 id="df-isnull"><a href="#df-isnull" class="headerlink" title="df.isnull()"></a>df.isnull()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># df.isnull() 元素级别的判断，把对应的所有元素的位置都列出来，元素为空或者NA就显示True，否则就是False</span></span><br><span class="line"><span class="comment"># df.isnull().any() 各个列是否存在缺失值</span></span><br><span class="line"><span class="comment"># df.isnull().any(1) 查看行包含缺失值情况</span></span><br><span class="line"><span class="comment"># df.isnull().values.any()  检查DataFrame 是否还有缺失值 返回True/False</span></span><br><span class="line"><span class="comment"># df.isnull().sum() 检查字段缺失值的数量</span></span><br><span class="line"><span class="comment"># df.isnull().sum().sum() 计算所有缺失值的数量</span></span><br></pre></td></tr></table></figure>
<h5 id="df-nunique"><a href="#df-nunique" class="headerlink" title="df.nunique()"></a>df.nunique()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回的是唯一值的个数</span></span><br><span class="line"><span class="comment"># 区别df.unique(), unique()是以数组形式（numpy.ndarray）返回列的所有唯一值（特征的所有唯一值）</span></span><br></pre></td></tr></table></figure>
<h5 id="df-value-counts"><a href="#df-value-counts" class="headerlink" title="df.value_counts()"></a>df.value_counts()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看表格某列中有多少个不同值的快捷方法，并计算每个不同值有在该列中有多少重复值,且默认从高到低排序</span></span><br><span class="line"><span class="comment"># 想升序排列，设置参数 ascending = True</span></span><br><span class="line"><span class="comment"># 想得出计数占比，可以加参数 normalize=True</span></span><br></pre></td></tr></table></figure>
<h5 id="df-groupby"><a href="#df-groupby" class="headerlink" title="df.groupby()"></a>df.groupby()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># groupby就是按xx分组, 基本操作：组内计数, 求和, 求均值, 求方差...</span></span><br><span class="line"><span class="comment"># 例如：对不同取值的计数: .value_counts()</span></span><br></pre></td></tr></table></figure>
<h5 id="sns-heatmap"><a href="#sns-heatmap" class="headerlink" title="sns.heatmap()"></a>sns.heatmap()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">seaborn.heatmap(data, vmin=<span class="literal">None</span>, vmax=<span class="literal">None</span>, cmap=<span class="literal">None</span>, center=<span class="literal">None</span>, robust=<span class="literal">False</span>, annot=<span class="literal">None</span>, fmt=<span class="string">'.2g'</span>, annot_kws=<span class="literal">None</span>, linewidths=<span class="number">0</span>, linecolor=<span class="string">'white'</span>, cbar=<span class="literal">True</span>, cbar_kws=<span class="literal">None</span>, cbar_ax=<span class="literal">None</span>, square=<span class="literal">False</span>, xticklabels=<span class="string">'auto'</span>, yticklabels=<span class="string">'auto'</span>, mask=<span class="literal">None</span>, ax=<span class="literal">None</span>, **kwargs)</span><br><span class="line"><span class="comment"># 共20个参数，除参数data外其他都是默认</span></span><br><span class="line"><span class="comment"># 对于二维数组：热力图就是把这个二维的数组的数字用热力图的颜色值来表示</span></span><br><span class="line"><span class="comment"># 即热力图的作用：可视化已有的数字，数据表里多个特征两两的相似度</span></span><br><span class="line"><span class="comment"># annot: 默认为False，为True的话，会在格子上显示数字</span></span><br><span class="line"><span class="comment"># vmax, vmin: 热力图颜色取值的最大值，最小值，默认会从data中推导</span></span><br></pre></td></tr></table></figure>
<h5 id="pd-concat"><a href="#pd-concat" class="headerlink" title="pd.concat()"></a>pd.concat()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以将数据根据不同的轴作简单的融合</span></span><br><span class="line">pd.concat(objs, axis=<span class="number">0</span>, join=<span class="string">'outer'</span>, join_axes=<span class="literal">None</span>, ignore_index=<span class="literal">False</span>, keys=<span class="literal">None</span>, levels=<span class="literal">None</span>, names=<span class="literal">None</span>, verify_integrity=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 参数说明 </span></span><br><span class="line"><span class="comment"># objs: series，dataframe或者是panel构成的序列lsit </span></span><br><span class="line"><span class="comment"># axis： 需要合并链接的轴，0是行，1是列 </span></span><br><span class="line"><span class="comment"># join：连接的方式 inner，或者outer</span></span><br></pre></td></tr></table></figure>
<h5 id="sklearn-decomposition-PCA"><a href="#sklearn-decomposition-PCA" class="headerlink" title="sklearn.decomposition.PCA"></a>sklearn.decomposition.PCA</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参考：https://www.cnblogs.com/pinard/p/6243025.html</span></span><br><span class="line"><span class="comment"># 在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。最常用的PCA类是sklearn.decomposition.PCA， 除了PCA类以外，最常用的PCA相关类还有KernelPCA类（主要用于非线性数据的降维，需要用到核技巧。因此在使用的时候需要选择合适的核函数并对核函数的参数进行调参）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn.decomposition.PCA参数介绍：</span></span><br><span class="line">   <span class="comment"># 1）n_components：指定希望PCA降维后的特征维度数目，也即保留下来的特征个数n。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于等于1的整数。也可以指定主成分的方差和所占的最小比例阈值，让PCA类根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。还可以将参数设置为"mle", 此时PCA类会用MLE算法根据特征的方差分布情况去选择一定数量的主成分特征来降维。也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。</span></span><br><span class="line">   <span class="comment"># 2）whiten ：判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为1.对于PCA降维本身来说，一般不需要白化。如果你PCA降维后有后续的数据处理动作，可以考虑白化。默认值是False，即不进行白化。</span></span><br><span class="line">   <span class="comment"># 3）svd_solver：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：&#123;‘auto’, ‘full’, ‘arpack’, ‘randomized’&#125;。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。 full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。</span></span><br><span class="line"> 　<span class="comment"># 除了这些输入参数外，有两个PCA类的成员值得关注。第一个是explained_variance_，它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。第二个是explained_variance_ratio_，它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。</span></span><br><span class="line">   <span class="comment"># fit_transform(X), 用X来训练PCA模型，同时返回降维后的数据。</span></span><br></pre></td></tr></table></figure>
<h5 id="sns-FacetGrid"><a href="#sns-FacetGrid" class="headerlink" title="sns.FacetGrid()"></a>sns.FacetGrid()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先sns.FacetGrid画出轮廓 然后用map填充内容</span></span><br><span class="line"><span class="comment"># plt.hist直方图 plt.scatter散点图 sns.barplot条形图</span></span><br><span class="line"><span class="comment"># 参数alpha，设置点的大小</span></span><br></pre></td></tr></table></figure>
<h5 id="pd-crosstab"><a href="#pd-crosstab" class="headerlink" title="pd.crosstab"></a>pd.crosstab</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉表是用于统计分组频率的特殊透视表</span></span><br><span class="line"><span class="comment"># 透视表：pd.pivot_table, 透视表就是将指定原有DataFrame的列分别作为行索引和列索引，然后对指定的列应用聚集函数(默认情况下式mean函数)。</span></span><br></pre></td></tr></table></figure>
<h5 id="df-drop-duplicates"><a href="#df-drop-duplicates" class="headerlink" title="df.drop_duplicates()"></a>df.drop_duplicates()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除特定列下面的重复行, 返回DataFrame格式的数据</span></span><br><span class="line">DataFrame.drop_duplicates(subset=<span class="literal">None</span>, keep=<span class="string">'first'</span>, inplace=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># subset : column label or sequence of labels, optional, 用来指定特定的列，默认所有列</span></span><br><span class="line"><span class="comment"># keep : &#123;‘first’, ‘last’, False&#125;, default ‘first’, 删除重复项并保留第一次出现的项</span></span><br><span class="line"><span class="comment"># inplace : boolean, default False, 是直接在原来数据上修改还是保留一个副本</span></span><br></pre></td></tr></table></figure>
<h5 id="sklearn-LabelEncoder"><a href="#sklearn-LabelEncoder" class="headerlink" title="sklearn.LabelEncoder()"></a>sklearn.LabelEncoder()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LabelEncoder可以将标签分配一个0—n_classes-1之间的编码, 将各种标签分配一个可数的连续编号</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">le = preprocessing.LabelEncoder()</span><br><span class="line">le.fit([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>])  <span class="comment"># ---&gt; LabelEncoder()</span></span><br><span class="line">le.classes_   <span class="comment"># --&gt; array([1, 2, 6])</span></span><br><span class="line">le.transform([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">6</span>]) <span class="comment"># --&gt; array([0, 0, 1, 2], dtype=int64)</span></span><br><span class="line">le.inverse_transform([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># Transform Integers Into Categories  --&gt; array([1, 1, 2, 6])</span></span><br></pre></td></tr></table></figure>
<h5 id="pd-get-dummies"><a href="#pd-get-dummies" class="headerlink" title="pd.get_dummies()"></a>pd.get_dummies()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one-hot的基本思想：将离散型特征的每一种取值都看成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。</span></span><br><span class="line"><span class="comment"># dummy encoding 直观的解释就是任意的将一个状态位去除。</span></span><br><span class="line"><span class="comment"># pandas提供对one-hot编码的函数是：pd.get_dummies()</span></span><br><span class="line">pandas.get_dummies(data, prefix=<span class="literal">None</span>, prefix_sep=<span class="string">'_'</span>, dummy_na=<span class="literal">False</span>, columns=<span class="literal">None</span>, sparse=<span class="literal">False</span>, drop_first=<span class="literal">False</span>, dtype=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 输入：array-like, Series, or DataFrame</span></span><br><span class="line"><span class="comment"># 输出：DataFrame</span></span><br><span class="line"><span class="comment"># 主要参数说明：</span></span><br><span class="line"><span class="comment"># data : array-like, Series, or DataFrame</span></span><br><span class="line"><span class="comment"># prefix : string, list of strings, or dict of strings, default None, 给输出的列添加前缀，如prefix="A",输出的列会显示类似</span></span><br><span class="line"><span class="comment"># prefix_sep : 设置前缀跟分类的分隔符sepration，默认是下划线"_"</span></span><br><span class="line"><span class="comment"># 直接在原始数据中操作，可以使用columns参数</span></span><br><span class="line"><span class="comment"># drop_first : bool, default False, 获得k中的k-1个类别值，去除第一个</span></span><br></pre></td></tr></table></figure>
<h5 id="round"><a href="#round" class="headerlink" title="round()"></a>round()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># round() 方法返回浮点数x的四舍五入值。</span></span><br></pre></td></tr></table></figure>
</0$为最大值），>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/GBDT/" rel="tag"># GBDT</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/31/今日头条app爬虫/" rel="next" title="今日头条App爬虫">
                <i class="fa fa-chevron-left"></i> 今日头条App爬虫
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/11/08/Python面试题/" rel="prev" title="Python面试题">
                Python面试题 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="木木">
            
              <p class="site-author-name" itemprop="name">木木</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT回归算法"><span class="nav-number">1.</span> <span class="nav-text">GBDT回归算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数学推导"><span class="nav-number">1.1.</span> <span class="nav-text">数学推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT算法的实现过程"><span class="nav-number">1.1.1.</span> <span class="nav-text">GBDT算法的实现过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT基本介绍"><span class="nav-number">1.2.</span> <span class="nav-text">GBDT基本介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DT：回归树"><span class="nav-number">1.2.1.</span> <span class="nav-text">DT：回归树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GB：梯度迭代"><span class="nav-number">1.2.2.</span> <span class="nav-text">GB：梯度迭代</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Shrinkage：缩减"><span class="nav-number">1.2.3.</span> <span class="nav-text">Shrinkage：缩减</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT适用范围"><span class="nav-number">1.2.4.</span> <span class="nav-text">GBDT适用范围</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#关于RankNet"><span class="nav-number">1.2.5.</span> <span class="nav-text">关于RankNet</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT在Sklearn中的实现"><span class="nav-number">1.3.</span> <span class="nav-text">GBDT在Sklearn中的实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT回归任务中常见的损失函数"><span class="nav-number">1.4.</span> <span class="nav-text">GBDT回归任务中常见的损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT的正则化：防止过拟合"><span class="nav-number">1.5.</span> <span class="nav-text">GBDT的正则化：防止过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有关GBDT常见面试问题"><span class="nav-number">1.6.</span> <span class="nav-text">有关GBDT常见面试问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT与AdaBoost的区别与联系？"><span class="nav-number">1.6.1.</span> <span class="nav-text">GBDT与AdaBoost的区别与联系？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT与随机森林（Random-Forest，RF）的区别与联系？"><span class="nav-number">1.6.2.</span> <span class="nav-text">GBDT与随机森林（Random Forest，RF）的区别与联系？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#我们知道残差-真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？"><span class="nav-number">1.6.3.</span> <span class="nav-text">我们知道残差=真实值-预测值，明明可以很方便的计算出来，为什么GBDT的残差要用负梯度来代替？为什么要引入麻烦的梯度？有什么用呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT训练过程"><span class="nav-number">1.6.4.</span> <span class="nav-text">GBDT训练过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT如何选择特征？"><span class="nav-number">1.6.5.</span> <span class="nav-text">GBDT如何选择特征？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT如何构建特征"><span class="nav-number">1.6.6.</span> <span class="nav-text">GBDT如何构建特征 ?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT-如何用于分类-？"><span class="nav-number">1.6.7.</span> <span class="nav-text">GBDT 如何用于分类 ？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#比较LR和GBDT，说说什么情景下GBDT不如LR"><span class="nav-number">1.6.8.</span> <span class="nav-text">比较LR和GBDT，说说什么情景下GBDT不如LR</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost"><span class="nav-number">2.</span> <span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数学推导-1"><span class="nav-number">2.1.</span> <span class="nav-text">数学推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数"><span class="nav-number">2.1.1.</span> <span class="nav-text">目标函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#树停止生长"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">树停止生长</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#稀疏感知算法"><span class="nav-number">2.2.</span> <span class="nav-text">稀疏感知算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法优缺点"><span class="nav-number">2.3.</span> <span class="nav-text">算法优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#优点"><span class="nav-number">2.3.1.</span> <span class="nav-text">优点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#缺点"><span class="nav-number">2.3.2.</span> <span class="nav-text">缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高频面试题"><span class="nav-number">2.4.</span> <span class="nav-text">高频面试题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB与GBDT、随机森林等模型相比，有什么优缺点"><span class="nav-number">2.4.1.</span> <span class="nav-text">XGB与GBDT、随机森林等模型相比，有什么优缺点?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB为什么可以并行训练？"><span class="nav-number">2.4.2.</span> <span class="nav-text">XGB为什么可以并行训练？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB用二阶泰勒展开的优势在哪？"><span class="nav-number">2.4.3.</span> <span class="nav-text">XGB用二阶泰勒展开的优势在哪？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB为了防止过拟合，进行了哪些设计？"><span class="nav-number">2.4.4.</span> <span class="nav-text">XGB为了防止过拟合，进行了哪些设计？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB如何处理缺失值？"><span class="nav-number">2.4.5.</span> <span class="nav-text">XGB如何处理缺失值？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么XGBoost相比某些模型对缺失值不敏感"><span class="nav-number">2.4.6.</span> <span class="nav-text">为什么XGBoost相比某些模型对缺失值不敏感</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB如何分裂一个结点？如何选择特征？"><span class="nav-number">2.4.7.</span> <span class="nav-text">XGB如何分裂一个结点？如何选择特征？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB中一颗树停止生长的条件有哪些？"><span class="nav-number">2.4.8.</span> <span class="nav-text">XGB中一颗树停止生长的条件有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB叶子结点的权重有什么含义？如何计算？"><span class="nav-number">2.4.9.</span> <span class="nav-text">XGB叶子结点的权重有什么含义？如何计算？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练一个XGB模型，经历了哪些过程？调参步骤是什么？"><span class="nav-number">2.4.10.</span> <span class="nav-text">训练一个XGB模型，经历了哪些过程？调参步骤是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGB如何给特征评分？"><span class="nav-number">2.4.11.</span> <span class="nav-text">XGB如何给特征评分？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost如何处理不平衡数据"><span class="nav-number">2.4.12.</span> <span class="nav-text">XGBoost如何处理不平衡数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost中如何对树进行剪枝"><span class="nav-number">2.4.13.</span> <span class="nav-text">XGBoost中如何对树进行剪枝</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LightGBM"><span class="nav-number">3.</span> <span class="nav-text">LightGBM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM在相比XGBoost哪些地方进行了优化？"><span class="nav-number">3.1.</span> <span class="nav-text">LightGBM在相比XGBoost哪些地方进行了优化？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数学原理"><span class="nav-number">3.2.</span> <span class="nav-text">数学原理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#单边梯度抽样算法"><span class="nav-number">3.2.0.1.</span> <span class="nav-text">单边梯度抽样算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#直方图算法"><span class="nav-number">3.2.0.2.</span> <span class="nav-text">直方图算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#直方图算法-1"><span class="nav-number">3.2.0.2.1.</span> <span class="nav-text">直方图算法</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#直方图加速"><span class="nav-number">3.2.0.2.2.</span> <span class="nav-text">直方图加速</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#稀疏特征优化"><span class="nav-number">3.2.0.2.3.</span> <span class="nav-text">稀疏特征优化</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#带深度限制的-Leaf-wise-算法"><span class="nav-number">3.2.0.3.</span> <span class="nav-text">带深度限制的 Leaf-wise 算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#类别特征最优分割"><span class="nav-number">3.2.0.4.</span> <span class="nav-text">类别特征最优分割</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost和LightGBM的区别"><span class="nav-number">3.3.</span> <span class="nav-text">XGBoost和LightGBM的区别</span></a></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#四、BigQuery-Geotab-Intersection-Congestion"><span class="nav-number">4.</span> <span class="nav-text">四、BigQuery-Geotab Intersection Congestion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-基本介绍"><span class="nav-number">4.1.</span> <span class="nav-text">1. 基本介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-概要"><span class="nav-number">4.1.1.</span> <span class="nav-text">1.1 概要</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-评估"><span class="nav-number">4.1.2.</span> <span class="nav-text">1.2 评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-资源"><span class="nav-number">4.1.3.</span> <span class="nav-text">1.3 资源</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-数据描述"><span class="nav-number">4.1.4.</span> <span class="nav-text">1.4 数据描述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-实现"><span class="nav-number">4.2.</span> <span class="nav-text">2. 实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-导入库"><span class="nav-number">4.2.1.</span> <span class="nav-text">2.1 导入库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-部分函数解读"><span class="nav-number">4.2.2.</span> <span class="nav-text">2.2 部分函数解读</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#pd-DataFrame"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">pd.DataFrame()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#df-reset-index"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">df.reset_index()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#df-isnull"><span class="nav-number">4.2.2.3.</span> <span class="nav-text">df.isnull()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#df-nunique"><span class="nav-number">4.2.2.4.</span> <span class="nav-text">df.nunique()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#df-value-counts"><span class="nav-number">4.2.2.5.</span> <span class="nav-text">df.value_counts()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#df-groupby"><span class="nav-number">4.2.2.6.</span> <span class="nav-text">df.groupby()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sns-heatmap"><span class="nav-number">4.2.2.7.</span> <span class="nav-text">sns.heatmap()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pd-concat"><span class="nav-number">4.2.2.8.</span> <span class="nav-text">pd.concat()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sklearn-decomposition-PCA"><span class="nav-number">4.2.2.9.</span> <span class="nav-text">sklearn.decomposition.PCA</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sns-FacetGrid"><span class="nav-number">4.2.2.10.</span> <span class="nav-text">sns.FacetGrid()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pd-crosstab"><span class="nav-number">4.2.2.11.</span> <span class="nav-text">pd.crosstab</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#df-drop-duplicates"><span class="nav-number">4.2.2.12.</span> <span class="nav-text">df.drop_duplicates()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sklearn-LabelEncoder"><span class="nav-number">4.2.2.13.</span> <span class="nav-text">sklearn.LabelEncoder()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pd-get-dummies"><span class="nav-number">4.2.2.14.</span> <span class="nav-text">pd.get_dummies()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#round"><span class="nav-number">4.2.2.15.</span> <span class="nav-text">round()</span></a></li></ol></li></ol></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木木</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
